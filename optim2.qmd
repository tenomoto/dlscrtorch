# L-BFGSを用いた函数の最適化

`torch`のモジュールと最適化器になじんだところで、別々に取り組んだ二つの課題、函数最小化とニューラルネットワークの訓練に戻る。
再び、最小化から始め、ネットワークは次章で扱う。

ローゼンブロック函数を最小化したときに何をしたか思い返すと、最も重要なことは、

1. 最適化すべきバラメタを格納するテンソルを定義する。つまり、 $\mathbf{x}$ が最小をとるところを見つける。
2. 反復してパラメタを更新するため、現在の勾配の一定の割合を差し引く。

方法としては、これは簡単だが、問題が残っている。
どのくらいの割合を差し引くげきか。
まさにこれが最適化器が有用な点である。

## L-BFGS見参

これまで、深層学習でよく使われる最適化器、確率的勾配降下法（SGD）やモーメンタム付SGDに加えて、適応学習率を使う種類、RMSPropや、Adadelta、Adagrad、Adamのような古典的な手法について議論した。
これらには一つの共通点がある。
それは、勾配、つまり一階微分のベクトルだけを使うことだ。
従って、これらは一次アルゴリズムである。
しかしながら、これらの手法はヘシアン、二階微分の行列から得られる有用な情報が欠けていることを意味している。

### 変化する傾き

一階微分からは地形の *勾配* が得られる。
上がっているか、下がっているか。それはどれくらいか。
一歩先に進めると、二階微分は傾きがどれくらい変化するかを表す。

なぜこれが重要なのか。

現在$\mathbf{x}_n$にいて、適切な降下方向を決めたとする。
事前に定めた学習率で決めた幅の一歩を進んで、$\mathbf{x}_{n+1}$に到達して完了する。
分からないのは、傾きが到着するまでにどれくらい変化したかだ。
途中でかなり平らになったからもしれない。
その場合、行き過ぎて遠いところに行ってしまい、途中で（傾きが再び上りに転じることも含めて）いろいろなことが起きていたかもしれない。

これを単変数の関数で説明できる。
放物線、例えば

$$
y = 10x^2
$$
を考える。
その微分は$\mathrm{d}y/\mathrm{d}x= 20 x$だ。
現在の$x$が例えば3で、学習率が$0.1$であれば、20 * 3 * 0.1 = 6を引いて、-3に至る。

でも2で減速して、そこでの傾きを調べると、緩やかになっているので、20 * 2 * 0.1 = 4を差し引くことになる。


運を天に任せ、「目を閉じて飛び込む」という方法がうまくいくのは、問題となっている函数に対する適切な学習率を用いた場合に限る。
（選んだ学習率だと、別の放物線$y=5x^2$の場合が該当する。）
しかし、最初から判断に二階微分を含めておくほうが賢明ではないだろうか。

この形を実践するアルゴリズムがニュートン法の一群である。
最初に最も「純粋な」種類を見る。
これは、原理を説明するには最適だが、実際に使うのが容易であることは稀である。

### 厳密ニュートン法

高次元で厳密ニュートン法は勾配にヘシアン逆行列をかけ、降下方向を座標毎に伸縮する。
我々の例は独立変数は一つだけなので、一階微分を二階微分で割るということを意味する。

勾配の伸縮をしたところで、どの位の割合を差し引くべきか。
原型では、厳密ニュートン法は学習率を使用しないので、慣れ親しんだ試行錯誤からは解放される。
例えば、我々の例では二階微分は20なので、(20 * 3) / 20 = 3を引く。
確かに、最小値の場所である0に一歩でたどり着く。

この結果が素晴らしいのに、なぜいつも使わないのか。
一つ理由は、示した例のような二次函数には完璧に働くが、通常は何らかの「調整」、例えば学習率を使うことなども必要となることだ。


しかし、主要な理由は別にある。
もっと現実的な応用で、機械学習と深層学習の分野では、ヘシアンの逆行列を毎回計算するのが高くつくからだ。
（そもそも可能ではないかもしれない。）
そこで、準ニュートン法として知られる近似が使われる。

## ニュートンの近似: BFGSとL-BFGS

ニュートン法の近似のうち、最もよく使われているのは、Broyden-Goldfarb-Fletcher-Shannoアルゴリズム、 *BFGS* である。
ヘシアンの厳密な逆行列を連続して計算する代わりに、反復更新された逆行列の近似を保持する。
BFGSはメモリに優しい派生型、 メモリ制約BFGS（ *L-BFGS* ）で実装されることが多い。
これが `torch` 最適化器の一部として提供されている。

試してみる前に、もう一つの概念を説明しておく。

### 直線探索

厳密な形式同様に、近似ニュートン法は学習率なしで使うことができる。
その場合、降下方向を計算して伸縮した勾配にそのまま従う。
問題となる函数次第で、これはうまく行ったり行かなかったりする。
うまく行かない場合は、二つの対応がある。
一つは、短い幅をとる、つまり学習率を導入する。
もう一つは直線探索だ。

直線探索を使うと、どこまで降下方向に沿うのかの評価に時間にをかける。
これを行う二つの主要な方法がある。

最初は、厳密な直線探索で、もう一つの最適化問題問題を伴う。
現在の位置で、降下方向を計算し、これを学習率だけに依存する *第二の* 函数として書く。
そしてこの函数を微分して、その最小を見つける。
かいは、歩幅を最適化する学習率となる。

代替の方法は、近似探索だ。
もう驚かないだろうが、厳密ニュートン法よりも近似ニュートン法が現実的に実行可能であるように、近似探索は厳密直線探索よりも実行がしやすい。

直線探索では、最適解の近似は以下の折り紙付きの経験則に基づいている。
基本的に、十分であるものを探す。
確立した経験則の最たるものが *強ウルフ* 条件であり、これが `torch` の`optim_lbfgs()` に実装されている。
次の節では、 `optim_lbfgs()`の使い方を学び、ローゼンブロック函数を最適化で、直線探索ありとなしの両方を使う。

## `optim_lbfgs()` を使ったローゼンブロック函数の最小化

ローゼンブロック函数を再掲する。

```{r}
library(torch)

a <- 1
b <- 5

rosenbrock <- function(x) {
  x1 <- x[1]
  x2 <- x[2]
  (a - x1)^2 + b * (x2 - x1^2)^2
}
```

自作で最小化を試みた時は、次の手順で行った。
最初だけ、まずパラメタテンソルを定義し、現在の $\mathbf{x}$ を格納した。

```r
x <- torch_tensor(c(-1, 1), requires_grad = TRUE)
```

そして、反復して次の演算を実行した。

1. 現在の $\mathbf{x}$ での函数値を計算する。
2. 考えている場所におけるその値の勾配を計算する。
3. 現在の $\mathbf{x}$ から勾配の一定の割合を差し引く。

どのようにこの青写真が変わるのか。

最初の手順は変わらない。

```r
value <- rosenbrock(x)
```

二つ目の手順も変わらず、直接出力テンソルに対して`backward()`を呼ぶ。

```r
value$backward()
```

その理由は最適化器は勾配を *計算* せず、ひとたび計算されたら勾配をどうするか決めている。

変わるのは、三番目の手順で、最も面倒でもあった。

更新を適用するのは最適化器である。
それを可能にするには、前提がある。
ループを始める前に、最適化器はどのパラメタに対して作用するか決められている必要がある。
実は、これはとても重要であり、そのパラメタを渡すことなく、最適化器を作ることすらできない。

```r
opt <- otpim_lbfgs(x)
```

ループでは最適化オブジェクトに対して `step()` メソッドを呼んでパラメタを最適化する。
自作での手続のうち新しいやり方に引き継がなければならないことが一つだけある。
依然として各反復で勾配を0にしなければならない。
今回は、パラメタ `x` に対してではなく、最適化オブジェクトそのものに対してである。
基本的に、各反復して次の動作を実行することになる。

```r
value <- rosenbrock(x)

opt$zero_grad()
value$backward()

opt$step()
```

なぜ「基本的に」なのか。
実は、これは `optimizer_lbdgs()`  *以外* に書かなければならないものだ。

`optim_lbfgs()` については、`step()` は無名函数、クロージャに渡す必要がある。
前の勾配を0にするもの、函数呼び出し、勾配計算が全てクロージャの中で行われる。

```r
calc_loss <- function() {
  optimzer$zero_grad()
  value <- rosenbrokc(x_star)
  value$backward()
  value
}
```

これらの操作を実行したら、クロージャは函数値を返す。
`step()` でどのように呼ぶか示す。

```r
for (i in 1:num_iterations) {
  optimizer$step(calc_loss)
}
```

組み上がったので、少しログ出力を加えて、直線探索ありとなしで何が起きるか比較してみよう。

### optim_lbfgs()の既定の動作

基準として、まず直線探索なしで走らせる。
反復2回で十分だ。
以下の出力で、各反復でクロージャは何回か評価される。
これがまずクロージャを書いた技術的な理由だ。

```{r}
num_iterations <- 2

x <- torch_tensor(c(-1,1), requires_grad = TRUE)

optimizer <- optim_lbfgs(x)

calc_loss <- function() {
  optimizer$zero_grad()
  
  value <- rosenbrock(x)
  cat("Value is: ", as.numeric(value), "\n")
  value
}

for (i in 1:num_iterations) {
  cat("\nIteration: ", i, "\n")
  optimizer$step(calc_loss)
}
```

最小が見つかったか確認するするには、`x`を印字してみる。

```{r}
x
```

### 直線探索付`optim_lbfgs()`

```{r}
num_iterations <- 2

x <- torch_tensor(c(-1, 1), requires_grad = TRUE)

optimizer <- optim_lbfgs(x, line_search_fn = "strong_wolfe")

calc_loss <- function() {
  optimizer$zero_grad()
  
  value <- rosenbrock(x)
  cat("Value is: ", as.numeric(value), "\n")

  value$backward()
  value
}

for (i in 1:num_iterations) {
  cat("\nIteration: ", i, "\n")
  optimizer$step(calc_loss)
}
```

直線探索を使うと、一回の反復で十分に最小値に到達する。
順に損失を確認すると、アルゴリズムは函数を調べる度にほぼ毎回損失を減少させるが、使わない場合はそうではないことが分かる。