# モジュール

前の章では、ニューラルネットワークを構築して回帰の問題に適用した。
演算には、線型と非線型の二種類があった。

非線型に分類されるReLU活性化は、簡単な函数呼び出し`nnf_relu()`で表されていた。
活性化函数は、函数であり、与えられた入力$\mathbf{x}$に対して出力$\mathbf{y}$を呼び出す毎に返す。
つまり、決定論的であるが、線型部分は異なる。

回帰ネットワークの線型部分は、重みを表す行列との積とバイアスを表すベクトルの和で実装されていた。
このような操作では、結果は当然関係するテンソルに格納されている実際の値に依存する。
言い換えれば、演算は状態依存である。

状態が関係する場合は、それをオブジェクトにカプセル化して、ユーザが直接管理する必要がないようにするとよい。
これが`torch`の*モジュール*が行なっていることだ。

*モジュール*という用語に注意してほしい。
`torch`では、モジュールの複雑さの程度はさまざまである。
この後すぐに導入する`nn_linear()`のような基本的な層から、多層からなる完成された*モデル*までが含まれる。
コード上では、「層」と「モデル」との違いはない。
そのため、「モジュール」だけが使われることもある。
本書では、一般的な層とモデルの使い方に従い、概念との対応を重視する。

*なぜ*モジュールかという話に戻る。
カプセル化に加えて、層オブジェクトが提供されているのには、別の理由がある。
よく用いられる層が全部`nn_linear()`のように軽量ではない。
次の節の最後で、簡単に他のモジュールについて述べ、本書の後の章で詳しく説明する。

## 組込の`nn_moudle()`

`torch`では、線型層は`nn_linear()`で作る。
`nn_linear()`は、`in_features`と`out_features`の（最低）二つの引数が必要である。
入力データに観測が50あり、それぞれ五つの特徴量がある場合、大きさは50 x 5となる。
潜在層には、16のユニットを置く。
この場合、`in_features`は5、`out_features`は16である。
（自分で重み行列を作る場合、同じ5と16が行と列の数である。）

```{r}
library(torch)
l <- nn_linear(in_features = 5, out_features = 16)
```

一度作られれば、モジュールのパラメタの情報は簡単に得られる。

```{r}
l
```

カプセル化されていても、重みとバイアステンソルを確認することができる。

```{r}
l$weight
```

```{r}
l$bias
```

この時点で、ちょっと立ち止まってほしい。
`torch`が返した重み行列の大きさは16 x 5で、白紙からコードを書いたときの5 x 16ではない。
これは基礎となるC++実装`libtorch`に由来する。
性能のため、`libtorch`の線型モジュールは重みとバイアスを転置で格納している。
Rからは、この点を指摘しておくことくらいしかできない。
混乱が軽減されることを望む。

続けよう。
このモジュールにデータを適用するには函数のように「呼び出す」だけだ。

```{r}
x <- torch_randn(50, 5)
output <- l(x)
output$size()
```

これが順伝播だ。
勾配計算はどうするのか。
以前は、勾配計算の「入力」として必要なテンソルを作るとき、`torch`に明示的に`requires_grad = TRUE`を渡す必要があった。
組込`nn_module()`ではその必要はない。
すぐに`output`が`backward()`でどのような計算をすべきか調べることができる。

```{r}
output$grad_fn
```

確認のため、`output`に基づいて、適当な損失を計算して、`backward()`を読んでみよう。
線型モジュールの`weight`テンソルには埋められた`grad`フィールドがあることが分かる。

```{r}
loss <- output$mean()
loss$backward()
l$weight$grad
```

つまり、`nn_module`を使うと、`torch`は自動的に勾配計算が必要だとみなされる。

`nn_linear`は簡単に見えるが、ほとんど全てのモデル構成において必須の要素だ。

* `nn_conv1d()`、`nn_conv2d()`、および`nn_conv3d()`、いわゆる*畳み込み*層は入力要素に対するフィルタを異なる次元で適用する。
* `nn_lstm()`と`nn_gru()`は状態を引き継ぐ再帰層。
* `nn_embedding()`は高次元の質的データにつ分かれる。
* その他多数

## モデルの構築

組込の`nn_module()`は普通の呼び方では層だが、どのようにモデルに組み上げるのか。
「工場函数」`nn_module()`を使って、モデルを任意の複雑さで定義できる。
でも、必ずしもそうする必要はない。

### 層の列としてのモデル: `nn_sequential()`

モデルが単に層を伝播するだけであれば、モデルを作るときに`nn_sequential()`が使える。
線型層だけからなるモデルは多層パーセプトロン（MPL: Multi-Layer Perceptron）として知られている。

```{r}
mlp <- nn_sequential(
  nn_linear(10, 32),
  nn_relu(),
  nn_linear(32, 64),
  nn_relu(),
  nn_linear(64, 1)
)
```

関係する層を詳しく見てみよう。
ReLU活性化を実装した函数をであった。
（`nnf_`の`f`は汎函数であることを示す。）
以下は`nn_relu`は`nn_linear()`と同様でモジュール、つまりオブジェクトで、引数は全てモジュールでなければならない。

組込モジュール同様、このモデルをデータに適用するには呼び出せばよい。

```{r}
mlp(torch_randn(5, 10))
```

一回の呼び出しによりネットワークを通じた順伝播が始動した。
同様に、`backward()`を呼び出せ全ての層を通じて逆伝播される。

### 読字処理のモデル

既に暗示されているように、ここが`nn_module`の使いどころだ。


`nn_module|()`は独自に作成されたR6オブジェクトに対するコンストラクタを作る。
以下、`my_linear()`はそのようなコンストラクタだ。
呼び出させれると、組込の`nn_linear()`に似た線型モジュールを返す。

コンストラクタの定義の中で、二つのメソッドが実装されなければならない。`initialize()`と`forward()`である。
`initialize()`はモジュールのオブジェクトフィールドを作る。
すなわちそれが持つオブジェクトまたは値でどのメソッドの中からも見える。
`forward()`はモジュールが入力があったときの動作を定義する。

```{r}
my_linear <- nn_module(
  initialize = function(in_features, out_features) {
    self$w <- nn_parameter(torch_randn(
      in_features, out_features
    ))
    self$b <- nn_parameter(torch_zeros(out_features))
  },
  forward = function(input) {
    input$mm(self$w) + self$b
  }
)
```

`nn_parameter()`の使い方を見てほしい。
`nn_parameter()`は渡されたテンソルかモジュールの *パラメタ* として登録されていることを確認する、つまり逆伝播されるのが既定だ。

新たに定義されたモジュールのインスタンスを作るには、コンストラクタを呼び出す。

```{r}
l <- my_linear(7, 1)
l
```

この例では、モジュールを定義するために必要な独自のロジックはないが、ここではどんな利用形態にも適用できる雛型である。
後に、より複雑な`initialize()`と`forward()`を検討したが、モジュールに定義される追加のメソッドを示す。
基本的な仕組みは一緒だ。

ここ手では、前の章でモジュールを使ったニューラルネットワークを書き換えることができると感じるかもしれない。
自由に取り組んでもよいし、最適化手法と組込損失函数を学ぶ次の章まで待ってもよい。
それが済んだら、函数最小化と回帰ネットワークの二つの例に戻ることができる。
そして、`torch`により自作した余計なものを取り除くことができる。