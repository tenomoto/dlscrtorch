# モジュール

前の章では、ニューラルネットワークを構築して回帰の問題に適用した。
演算には、線型と非線型の二種類があった。

非線型に分類されるReLU活性化は、簡単な函数呼び出し`nnf_relu()`で表されていた。
活性化函数は、函数であり、与えられた入力$\mathbf{x}$に対して出力$\mathbf{y}$を呼び出す毎に返す。
つまり、決定論的である。
でも、線型部分は異なる。

回帰ネットワークの線型部分は、重みを表す行列との積とバイアスを表すベクトルの和で実装されていた。
このような操作では、結果は当然関係するテンソルに格納されている実際の値に依存する。
言い換えれば、演算は状態依存である。

状態が関係する場合は、それをオブジェクトにカプセル化して、ユーザが直接管理する必要がないようにするとよい。
これが`torch`の*モジュール*が行なっていることだ。

*モジュール*という用語に注意してほしい。
`torch`では、モジュールの複雑さの程度はさまざまである。
この後すぐに導入する`nn_linear()`のような基本的な層から、多層からなる完成された*モデル*までが含まれる。
コード上では、「層」と「モデル」との違いはない。
そのため、「モジュール」だけが使われることもある。
本書では、一般的な層とモデルの使い方に従い、概念との対応を重視する。

*なぜ*モジュールかという話に戻る。
カプセル化に加えて、層オブジェクトが提供されているのには、別の理由がある。
よく用いられる層が全部`nn_linear()`のように軽量ではない。
次の節の最後で、簡単に他のモジュールについて述べ、本書の後の章で詳しく説明する。

## 組込の`nn_moudle()`

`torch`では、線型層は`nn_linear()`で作る。
`nn_linear()`は、`in_features`と`out_features`の（最低）二つの引数が必要である。
入力データに観測が50あり、それぞれ五つの特徴量がある場合、大きさは50 x 5となる。
潜在層には、16のユニットを置く。
この場合、`in_features`は5、`out_features`は16である。
（自分で重み行列を作る場合、同じ5と16が行と列の数である。）

```{r}
library(torch)
l <- nn_linear(in_features = 5, out_features = 16)
```

一度作られれば、モジュールのパラメタの情報は簡単に得られる。

```{r}
l
```

カプセル化されていても、重みとバイアステンソルを確認することができる。

```{r}
l$weight
```

```{r}
l$bias
```

この時点で、ちょっと立ち止まってほしい。
`torch`が返した重み行列の大きさは16 x 5で、白紙からコードを書いたときの5 x 16ではない。
これは基礎となるC++実装`libtorch`に由来する。
性能のため、`libtorch`の線型モジュールは重みとバイアスを転置で格納している。
Rからは、この点を指摘しておくことくらいしかできない。
混乱が軽減されることを望む。

続けよう。
このモジュールにデータを適用するには函数のように「呼び出す」だけだ。

```{r}
x <- torch_randn(50, 5)
output <- l(x)
output$size()
```

これが順伝播だ。
勾配計算はどうするのか。
以前は、勾配計算の「入力」として必要なテンソルを作るとき、`torch`に明示的に`requires_grad = TRUE`を渡す必要があった。
組込`nn_module()`ではその必要はない。
すぐに`output`が`backward()`でどのような計算をすべきか調べることができる。

```{r}
output$grad_fn
```

確認のため、`output`に基づいて、適当な損失を計算して、`backward()`を読んでみよう。
線型モジュールの`weight`テンソルには埋められた`grad`フィールドがあることが分かる。

```{r}
loss <- output$mean()
loss$backward()
l$weight$grad
```

つま、`nn_module`を使うと、`torch`は自動的に勾配計算が必要だとみなされる。

`nn_linear`は簡単に見えるが、ほとんど全てのモデル構成において必須の要素だ。

* `nn_conv1d()`、`nn_conv2d()`、および`nn_conv3d()`、いわゆる*畳み込み*層は入力要素に対するフィルタを異なる次元で適用する。
* `nn_lstm()`と`nn_gru()`は状態を引き継ぐ再帰層。
* `nn_embedding()`は高次元の質的データにつ分かれる。
* その他多数