# *autograd*を使った函数の最小化

前の二つの章ではテンソルと自動微分について学んだ。
これから二つの章では`torch`の仕組みについての勉強は休んで、その代わりすでに学んだことで何ができるか試してよう。
テンソルだけを使い、自動微分だけの力を借りるだけで、既に二つのことができる。

* 函数を最小化する（つまり、数値最適化を行う）、そして
* ニューラルネットワークを構築して訓練する。

この章では、最小化からはじめ、ネットワークは次章に回す。

## 最適化の古典

最適化研究において、*ローゼンブロック函数* は古典である。
この函数は二つの変数をとり、`(1, 1)`で最小となる。
等値線を眺めると、最小は伸びた細い谷の中にあることが分かる。

![ローゼンブロック函数](images/optim-1-rosenbrock.png){#fig-optim1-rosenbrock}

函数の定義は次の通りだ。
`a`と`b`は自由に定めてよいパラメタだが、よく使われる値を用いる。

```{r}
a <- 1
b <- 5

rosenbrock <- function(x) {
  x1 <- x[1]
  x2 <- x[2]
  (a - x1)^2 + b * (x2 - x1^2)^2
}
```

## 最小化を白紙から

シナリオは次の通り。
与えられた点`(x1, x2)`から出発し、ローゼンブロック函数が最小になる場所を見つける。

前の章で説明した方法に従い、現在の位置における函数の勾配を計算し、それを使って逆方向にに進む。
どれくらい遠くまで行けばよいかは分からない。
大きく進みすぎると、簡単に行き過ぎてしまう。
（等値線図を再度確認すると最小値のの西または東の急な崖に立っていると、これがすぐに生じることが分かる。）

つまり、最良の方法は反復して進むことで、妥当な幅を取り、毎回勾配を再評価することだ。

まとめると、最適化の手順は次のようになる。

```r
library(torch)

# これはまだ正しい手順ではない!

for (i in 1:num_iterations) {
  
  # 函数を呼び現在のパラメタ値を渡す。
  value <- rosenbrock(x)
  
  # パラメタについての勾配を計算する。
  value$backward()
  
  # 手動でパラメタを更新し、勾配に比例した一部を引く。
  # ここはまだ正しくない。
  x$sub_(lr * x$grad)
}
```

書かれている通り、コード片は考えを示したもので、（まだ）正しくない。
また、いくつかの必要なものが欠けている。
テンソル`x`も変数`lr`や`num_iterations`も定義されていない。
まず、これらを準備しよう。
学習率`lr`は毎回引く勾配に比例した一部で、`num_iterations`は反復する回数である。
これらは実験パラメタである。

```r
lr <- 0.01

num_iterations <- 1000
```

`x`は最適化するパラメタ、つまり函数の入力であり、最適化の最後に可能な限り最小の函数値に近い値を与える位置であることが望まれる。
このテンソルについて函数の微分を計算するので、`requires_grad = TRUE`をつけて作る必要がある。

```r
x <- torch_tensor(c(-1, 1), requires_grad = TRUE)
```

初期位置`(-1, 1)`は任意に選択した。
さて、残っているのは最適化ループを少し修正することだ。
*autograd* が`x`について有効化されていると、`torch`はこのテンソルに対して行われるすべての演算を記録する。
そのため`backward()`を呼ぶたびに、すべての必要な微分を計算しようとすることになる。
しかし、勾配の一部を引くときは、微分を計算する必要はない。
`torch`にこれを記録しないように指示するために、`with_no_grad()`を囲む。

ここで説明しておかなければならないことがある。
既定で`torch`は`grad`フィールドに格納された勾配を蓄積する。
新しい計算をするたびに`grad$zero_()`を使ってゼロに消去する必要がある。

これらを考慮すると、パラメタの更新は次のように書ける。

```r
with_no_grad({
  x$sub_(lr * x$grad)
  x$grad$zer_()
})
```

完成したコードは次のようになる。
ログをとる文を追加して何が起きているか分かるようにしてある。

```{r}
library(torch)

num_iterations <- 1000

lr <- 0.01
x <- torch_tensor(c(-1, 1), requires_grad = TRUE)

for (i in 1:num_iterations) {
  if (i %% 100 == 0) cat("Iteration: ", i, "\n")
  
  value <- rosenbrock(x)
  if (i %% 100 == 0) {
    cat("Value is : ", as.numeric(value), "\n")
  }
  
  value$backward()
  if (i %% 100 == 0 ) {
    cat("Gradient is : ", as.matrix(x$grad), "\n")
  }
  
  with_no_grad({
    x$sub_(lr * x$grad)
    x$grad$zero_() 
  })
}
```

1000回の反復後、函数値は0.0001よりも小さくなった。
対応する`(x1,x2)`の位置はどこになったか。

```{r}
x
```

これは真の最小`(1,1)`にかなり近い。
気が向いたら、学習率がどのような違いを生じるか試してみよう。
例えば、0.001と0.1をそれぞれ使ってみるとよい。

次の章では、白紙からニューラルネットワークを構築する。
そこで最小化するのは *損失函数* 、つまり回帰問題から現れる平均二乗誤差である。
