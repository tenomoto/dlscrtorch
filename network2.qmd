# ニューラルネットワークのモジュール化

少し前の章で構築したネットワークを思い出そう。
その目的は回帰だったが、その手法は線型ではなかった。
その代わり、活性化函数（ReLU、rectified linear unit）により非線型性を導入し、単一の隠れ層と出力層との間に配置した。
「層」の元の実装では、単なるテンソルで重みとバイアスを表していた。
これらが *モジュール* に置き換えると聞いても驚かないだろう。

どのように訓練の過程は変わるだろうか。
概念的には、四つの段階に分けることができる。
順伝播、損失の計算、勾配の逆伝播、そして最後に重みの更新である。
新しい道具がどこに使われるかを考えてみよう。

* 順伝播では、テンソルに対して函数を呼ぶ代わりに、モデルを呼ぶ。
* 損失計算では、`torch` の `nnf_mse_loss()` を利用する。
* 勾配の逆伝播は唯一変わらない。
* 重みの更新は、最適化器が担当する。

これらの変更を加えたら、コードはよりモジュール化され、より読みやすくなるだろう。

## データ

準備として、データを前のように生成する。

```{r}
library(torch)

# 入力次元（入力する特徴量の数）
d_in <- 3
# 訓練集合に含まれる観測の数
n <- 100

x <- torch_randn(n, d_in)
coefs <- c(0.2, -1.3, -0.5)
y <- x$matmul(coefs)$unsqueeze(2) + torch_randn(n, 1)
```

## ネットワーク

二つの線型層がReLU活性化函数により接続するには、`sequential`モジュールを使うのが最も簡単である。
これは、モジュールを導入したときに見たものによく似ている。

```{r}
# 隠れ層の次元
d_hidden <- 32
# 出力層の次元（予測される特徴量の数）
d_out <- 1

net <- nn_sequential(
  nn_linear(d_in, d_hidden),
  nn_relu(),
  nn_linear(d_hidden, d_out)
)
```

## 訓練

更新された訓練の過程を示す。
よく選ばれている、Adam最適化器を使う。

```{r}
opt <- optim_adam(net$parameters)

### 訓練ループ --------------------------------------

for (t in 1:200) {
  
  ### -------- 順伝播 --------
  y_pred <- net(x)
  
  ### -------- 損失計算 --------
  loss <- nnf_mse_loss(y_pred, y)
  if (t %% 10 == 0)
    cat("エポック: ", t, " 損失: ", loss$item(), "\n")
  
  ### -------- 逆伝播 --------
  opt$zero_grad()
  loss$backward()
  
  ### -------- 重みの更新 --------
  opt$step()
}
```

コードが短くなり、効率的になっただけでなく、変更により性能も大きく向上した。