---
title: "テンソル"
format: html
---

## テンソルとは何か

`torch`の`tensor`は、Rの`array`に同様に任意の次元を取れる。
Rの`array`とは異なり、高速かつ大規模に計算を実行するために、GPUに移すことができる（おまけに、自動微分ができるので、大変有用だ）。

`tensor`はR6オブジェクトに類似していて、`$`によりフィールドやメソッドを利用できる。

```{r}
library(torch)

t1 <- torch_tensor(1)
t1
```

これは単一の値1だけを格納したテンソルだ。
CPUに「生息」しており、その型は`Float`。
次に波括弧の中の1`{1}`に着目する。
これはテンソルの値を改めて示したものではない。
これはテンソルの形状、つまりそれが生息する空間と次元の長さである。
Base Rと同様にベクトルは単一の要素だけでもよい
（base Rは`1`と`c(1)`を区別しないことを思い出してほしい）。

前述の`$`記法を使って、一つ一つ関連するフィールドを参照することで、個別に以上の属性を確認できる。

```{r}
t1$dtype
```

```{r}
t1$device
```

```{r}
t1$shape
```

テンソルの `$to()` を使うと、メソッドいくつかの属性は直接変更できる。

```{r}
t2 <- t1$to(dtype = torch_int())
t2$dtype
```

```{r}
# GPUがある場合
#t2 <- t1$to(device = "GPU")
# Apple Siliconの場合
t2 <- t1$to(device = "mps")
t2$device
```

形状の変更はどのようにするのか。
これは別途扱うに値する話題だが、手始めにいじってみることにする。
値の変更なしに、この1次元の「ベクトルテンソル」を2次元の「行列テンソル」にできる。

```{r}
t3 <- t1$view(c(1, 1))
t3$shape
```

概念的には、Rで1要素のベクトルや行列を作るのに似ている。
```{r}
c(1)
matrix(1)
```

テンソルがどのようなものか分かったところで、いくつかのテンソルを作る方法について考えてみる。


## テンソルの作成

既に見たテンソルを作る一つの方法は`torch_tensor()`を呼び出し、Rの値を渡すというものだった。
この方法は多次元オブジェクトに適用でき、以下にいくつかの例を示す。

しかし、多くの異なる値を渡す必要があるときは効率が悪くなる。
ありがたいことに、値が全て同一であるべき場合や、明示的なパターンに従うときに適用できる別の方法がある。
この節ではこの技についても説明する。

### 値からテンソル

前の例では単一要素のベクトルを`torch_tensor()`に渡したが、より長いベクトルを同様に渡すことができる。

```{r}
torch_tensor(1:5)
```

同様に規定のデバイスはCPUだが、最初からGPU/MPSに配置するテンソルを作成することもできる。

```{r}
#torch_tensor(1:5, device = "cuda")
torch_tensor(1:5, device = "mps")
```

これまで作ってきたのはベクトル。
行列、つまり2次元テンソルはどうやって作るのか。

Rの行列を同様に渡せばよい。

```{r}
torch_tensor(matrix(1:9, ncol = 9))
```

結果を見てほしい。
1から9までの数字は、列ごとに表示されている。
これは意図通りかもしれないし、そうではないかもしれない。
意図と異なる場合は`matrix()`に`byrow = TRUE`を渡せばよい。

```{r}
torch_tensor(matrix(1:9, ncol = 3, byrow = TRUE))
```

高次元のデータはどうするか。
同様の方針に従って、配列を渡すことができる。

```{r}
torch_tensor(array(1:24, dim = c(4, 3, 2)))
```

この場合でも、結果はRの埋め方に沿ったものとなる。
これが求めるものではないなら、テンソルを構築するプログラムを書いた方が簡単かもしれない。

慌てる前に、その必要が非常に稀であることを考えてみてほしい。
実際は、Rのデータセットからテンソルを作ることがほとんどだ。
「データセットからテンソル」の最後の小節で詳しく確認する。
その前に、少し時間をとって最後の出力を少し吟味しよう。

![4x3x2テンソル](images/tensors-dimensions.png) {#fig-tensor-432}
私たテンソルは以下のように印字される。
```{r}
array(1:24, dim = c(4, 3, 2))
```

上のテンソルの印字と比較しよう。
`Array`と`tensor`は異なる方向にオブジェクトを切っている。
テンソルは値を`3x2`の上向きと奥に向かう広がる長方形に切り、4つの$x$のそれぞれの値に対して一つ示している。
一方、配列は`z`の値で分割し、二つの奥向きと右向きに進む大きな`4x3`の部分を示す。

言い換えれば、テンソルは左/「外側」から、配列は右/「内側」から思考を始めているとも言えるだろう。

### 指定からテンソル

`torch`の大口生成函数が便利な状況は、おおまかに二つある。
一つは、テンソルの個々の値は気にせず、分布のみに興味がある場合だ。
もう一つは、ある一定のパターンに従う場合だ。


要素の値の代わりに、大口生成函数を使うときは、取るべき形状を指定する。
例えば、3x3のテンソルを生成し、標準正規分部の値で埋める場合は次のようにする。

```{r}
torch_randn(3, 3)
```

次に示すのは、0と1の間の一様分布に対する同様なもの。

```{r}
torch_rand(3, 3)
```

全て1や0からなるテンソルが必要となることがよくある。

```{r}
torch_zeros(2, 5)
```

```{r}
torch_ones(2, 2)
```

他にも多くの大口生成函数がある。
最後に線型代数で一般的ないくつかの行列を作る方法を見ておく。
これは単位行列。

```{r}
torch_eye(n = 5)
```

そしてこれは対角行列。

```{r}
torch_diag(c(1, 2, 3))
```

### データセットからテンソル

さて、Rのデータセットからテンソルを作る方法を見ていこう。
データセットよっては、この過程は「自動」であったり、考慮や操作が必要になったりする。

まず、base Rについてくる`JohnsonJohnson`を試してみる。
これは、Johnson & Johnsonの一株あたりの四半期利益の時系列である。

```{r}
JohnsonJohnson
```

`torch_tensor()`に渡すだけで、魔法のようにほしいものが手に入るだろうか。

```{r}
torch_tensor(JohnsonJohnson)
```

うまくいっているようだ。
値は希望通り四半期ごとに並んでいる。

魔法？いや、そうではない。
`torch`ができるのは与えられたものに対して動作することだ。
ここでは、与えられたのは実は四半期順に並んだ`double`のベクトル。
データは`ts`クラスなので、その通りに印字されただけだ。

```{r}
unclass(JohnsonJohnson)
```

これはうまくいった。
別なものを試そう。

```{r}
dim(Orange)
```

```{r}
head(Orange)
```

```{r}
#| error: true
torch_tensor(Orange)
```

どの型が処理されないのか。
「元凶」は順序付き因子の列`Tree`に違いないのは明らかだ。
先に`torch`が因子を扱えるか確認する。

```{r}
f <- factor(c("a", "b", "c"), ordered = TRUE)
torch_tensor(f)
```

これは問題なく動作した。
他に何がありうるか。
ここでの問題は含まれている構造`data.structure`である。
`as.matrix()`を先に作用させる必要がある。
でも、因子が存在するので、全て文字列の配列になってしまい、希望通りにならない。
したがって、基礎となるレベル（整数）を抽出してから、`data.frame`から行列に変換する。

```{r}
 orange_ <- Orange |>
  transform(Tree = as.numeric(Tree)) |>
  as.matrix()

torch_tensor(orange_) |> print(n = 7)
```

同じことを別の`data.frame`、`modeldata`の`okc`でしてみよう。

::: {.callout-caution}
`okc`は`modeldata`の0.1.1で廃止となり、0.1.2以降は削除された。
:::

```{r}
load("data/okc.RData")

head(okc)
```
```{r}
dim(okc)
```

二つある整数の列は問題なく、一つある因子の列の扱い方は学んだ。
`character`と`date`の列はどうだろう。
個別に`date`の列からテンソルを作ってみる。

```{r}
print(torch_tensor(okc$date), n = 7)
```

これはエラーを投げなかったが、何を意味するのか。
こられはRの`Date`に格納されている実際の値、つまり1970年1月1日からの日数である。
すなわち、技術的には動作する変換だ。
結果が実際に意味をなすかは、どのようにそれを使うつもりかという問題だ。
言い換えれば、おそらく計算に使う前に、これらのデータを追加の処理する必要がある。
どのようにするかは文脈次第。

次に`location`を見る。
これは、`character`型の列のうちの一つだ。
そのまま`torch`に渡すとどうなるか。

```{r}
#| error: true
torch_tensor(okc$location)
```

実際`torch`には文字列を格納するテンソルはない。
これらをnumeric型に本管する何らかの方法を適用する必要がある。
この例のような場合、個々の観測が単一の実体（例えば文やパラグラフではなく）を含む場合、最も簡単な方法はRで`factor`に変換し、`numeric`、そして`tensor`にすることだ。

```{r}
okc$location |>
  factor() |>
  as.numeric() |>
  torch_tensor() |>
  print(n = 7)
```

確かに、技術的にはこれはうまく動作する。
しかしながら、情報が失われる。
例えば、最初と3番目の場所はそれぞれ"south san francisco"と"san francisco"だ。
一度因子に変換されると、これらは意味の上で"san francisco"や他の場所と同じ距離になる。
繰り返しになるが、これが重要かはデータの詳細と目的次第だ。
これが重要なら、例えば、観測をある基準でまとめたり、緯度/経度に変換したりすることを含めさまざまな対応がありうる。
これらの考慮は全く`torch`に特有ではないが、ここで述べたのは`torch`の「データ統合フロー」に影響するからだ

最後に実際のデータ科学の世界に挑むには、`NA`を無視するわけにはいかない。
確認しよう。

```{r}
torch_tensor(c(1, NA, 3))
```

Rの`NA`は`NaN`に変換された。
これを扱えるだろうか。
いくつかの`torch`のかんすうでは可能だ。
例えば、`torch_nanquantil()`は単に`NaN`を無視する。

```{r}
torch_nanquantile(torch_tensor(c(1, NA, 3)), q = 0.5)
```

ただし、ニューラルネットワークを訓練するなら、欠損値を意味のあるように置き換える方法を考える必要があるが、この話題は後回しにする。

## テンソルに対する操作

テンソルに対する数学的操作は全て可能だ。和、差、積など。
これらの操作は（`torch_`で始まる）函数や（`$`記法で呼ぶ）オブジェクトに対するメソッドとして利用可能だ。
次の二つは同じだ。

```{r}
t1 <- torch_tensor(c(1, 2))
t2 <- torch_tensor(c(3, 4))

torch_add(t1, t2)
```

```{r}
t1$add(t2)
```

どちらも新しいオブジェクトが生成され、`t1`も`t2`も変更されない。
オブジェクトをその場で変更する別のメソッドもある。

```{r}
t1$add_(t2)
```

```{r}
t1
```


実は、同じパターンは他の演算にも適用される。
アンダスコアが後についているのを見たら、オブジェクトはその場で変号とされる。

当然、科学計算の場面では行列演算は特に重要だ。
二つの一次元構造、つまりベクトルの内積から始める。

```{r}
t1 <- torch_tensor(1:3)
t2 <- torch_tensor(4:6)
t1$dot(t2)
```

これは動かないはずだと考えただろうか。
テンソルの一つを転置（`torch_t()`）する必要があっただろうか。
これも動作する。

```{r}
t1$t()$dot(t2)
```

最初の呼び出しも動いたのは、`torch`が行ベクトルと列ベクトルを区別しないからだ。
結果として、`torch_matmul()`を使ってベクトルを行列にかけるときも、ベクトルの向きを心配する必要はない。

```{r}
t3 <- torch_tensor(matrix(1:12, ncol = 3, byrow = TRUE))
t3$matmul(t1)
```

同じ函数`torch_matmul()`は二つの行列をかけるときにも使う。
これが`torch_multiply()`が行う、引数のスカラとどのように異なるかよく見てほしい。

```{r}
torch_multiply(t1, t2)
```

テンソル演算は他にも多数あり、勉強の途中、いくつかに出会うことになるか、特に述べておく必要な集まりが一つある。

### 集計

R行列に対して和を計算する場合、それは次の三つのうちの一つを意味する。
総和、行の和、もしくは列の和。
これら三つを見てみよう（訳あって`apply()`を使う）。

```{r}
m <- outer(1:3, 1:6)

sum(m)
apply(m, 1, sum)
apply(m, 2, sum)
```

それでは`torch`で同じことをする。
総和から始める。

```{r}
t <- torch_outer(torch_tensor(1:3), torch_tensor(1:6))
t$sum()
```

行と列の和は面白くなる。
`dim`引数は`torch`にどの次元の和をとるか伝える。
`dim = 1`を渡すと次のようになる。

```{r}
t$sum(dim = 1)
```

予想外にも列の和になった。
結論を導く前に、`dim = 2`だとどうなるか。

```{r}
t$sum(dim = 2)
```

今度は行の和である。
`torch`の次元の順序を誤解したのだろうか。そうではない。
`torch`では、二つの次元があれば行が第一で列が第二である
（すぐに示すように、添え字はRで一般的なのと同じで1から始まる）。

むしろ、概念の違いは集計にある。
Rにおける集計は、頭の中にあるものをよく特徴づけている。
行（次元1）ごとに集計して行のまとめを得て、列（次元2）ごとに集計した列のまとめを得る。
`torch`では考え方が異なる。
列（次元2）を圧縮して行のまとめを計算し、行（次元1）で列のまとめを得る。

同じ考え方がより高い次元に対しても適用される。
例えば、4人の時系列データを記録しているとする。
二つの特徴量を3回計測する。
再帰型ニューラルネットワーク（詳しくは後ほど）を訓練する場合、測定を次のように並べる。

* 次元1: 個人に亙る。
* 次元2: 時刻に亙る。
* 次元3: 特徴に亙る。

テンソルは次のようになる。

```{r}
t <- torch_randn(4, 3, 2)
t
```

二つの特徴量についての平均は、対象と時刻に独立で、次元1と2を圧縮する。

```{r}
t$mean(dim = c(1, 2))
```

一方、特徴量について平均を求めるが、各個人に対するものは次のように計算する。

```{r}
t$mean(dim = 2)
```

ここで圧縮されたは時刻である。



