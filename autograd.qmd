# 自動微分

前の章では、テンソルをどのように扱うかを学び、それに対して行うことができる数学的な演算の例を示した。
そのような演算が多数あったとしても、それらが主要な`torch`の全てだったら、この本は読む必要がない。
`torch`のようなフレームワークの人気は、それらを使ってできること、一般的には深層学習、機械学習、最適化、大規模科学計算にある。
これらの応用分野の多くは、なんらかの損失函数を最小化と関係している。
これは、さらに函数の *微分* の計算を必要とする。
ここで、利用者として、個々の微分の函数形を自分で指定しなくてはならないということを想像してみよう。
特にニューラルネットワークでは、すぐに面倒になるだろう。

実は、`torch`は微分の函数表現を作ったり、保存したりしない。
代わりに、 *自動微分* と呼ばれるものを実装している。
自動微分では、より具体的には逆モード形では、微分はテンソル演算のグラフを逆向き走査で計算され、結合される。
この後すぐに例を示すが、その前に一歩引いてなぜ微分を計算する必要があるのか、簡単に議論しておこう。

## なぜ微分を計算するのか

教師あり機械学習では、訓練集合が使えて、予測したい変数は既知である。
これが目的変数で真値である。
今予測アルゴリズムを開発し、これを入力変数、予測変数に基づいて訓練する。
この訓練あるいは学習過程は、アルゴリズムの予測と真値とを比べ、
現在の予測がどれくらいよいか悪いか捉える数値が出てくるような比較に基づいている。
この数値を与えるのは、 *損失函数* の仕事だ。

一度現在の損失が分かったら、アルゴリズムはパラメタ、つまりニューラルネットワークの重みを調整して、もっとよい予測にする。
アルゴリズムはどの方向に調整するか知る必要がある。
この情報は、 *勾配* つまり微分のベクトルから得られる。

例として次のような損失函数を想像してみる [@fig-autograd-paraboloid]。

![仮想的な損失函数（放物面）。](images/autograd-paraboloid.png){#fig-autograd-paraboloid}

これは二変数の二次函数 `f(x_1, x_2) = 0.2x_1^2 + 0.2x_2^2 - 5` である。
最小値は`(0,0)`で、この点を求める。
白い点で示した点に立ち、風景を眺めれば、坂を速く降る方法は明確に分かる（坂を下るのを恐れないとする）。
でも、最良の方向を計算で見つけるには、勾配を計算する。

$x_1$の方向を取り上げる。
$x_1$に関する函数の微分は、函数値が$x_1$とともにどのように変化するかを示す。
計算すると$\frac{\partial f}{\partial x_1} = 0.4x_1$となる。
これは$x_1$が増えると損失が増えることと、それがどの程度かを示している。
でも損失を減らす必要があるので、逆方向に進む必要がある。

同じことが$x_2$軸に対しても成り立つ。
微分を計算すると、$\frac{\partial f}{\partial x_2} = 0.4x_2$を得る。
再び、微分が示す向きと逆方向を選ぶ。
全体では、降下方向は
$$
\begin{bmatrix}
-0.4x_1\\
-0.4x_2
\end{bmatrix}
$$
である。

この方法は記述的に最急降下と呼ばれている。
一般的に *勾配降下* と呼ばれ、機械学習で最も基本的な最適化アルゴリズムである。
おそらく直感に反して、最も効率の良い方法ではない。
さらに別の問いがある。
出発点で計算されたこの方向は降下中にずっと最適なのか。
代わりに、定期的に方向を計算し直した方が良いのかもしれない。
このような質問は後の章で検討する。
