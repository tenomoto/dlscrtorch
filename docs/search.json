[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R torchによる深層学習と科学計算",
    "section": "",
    "text": "ようこそ\n\nこれはオンライン版 Deep Learning and Scientific Computing with R torch Sigrid Keydana 著の部分的な和訳「R torchによる深層学習と科学計算」である。 GitHubリポジトリは 原典、和訳 にある。 原典の紙媒体 CRC Press は出版社または Amazonのような販売店から購入できる。\nこのオンラインの著作は Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Licenseでライセンスされている。\n\nはじめに\nこの本は torch 、 PyTorchのRインターフェースについて書かれている。 PyTorchは執筆時点で、主要な機械学習と科学計算フレームワークの一つで、産業や研究の分野で広く使われている。 torch を使うと、Rから直接その豊富な機能を使うことができる。 Pythonをインストールすることも、学ぶことすら必要ない。 まだ 若い プロジェクトだが、 torch は既に活発なユーザと開発者のコミュニティがある。 開発者はフレームワーク本体だけでなく、その上に自前のパッケージを構築している。\n本書では、三つの部に対応して、三つの目的を達成することを目指している。\n最初の目的は torch の基礎について丁寧に紹介することである。 基礎とは、それなしには何も動作しない基礎的な構造を指す。 先々では、できるだけ高度な文法構造を使うことになるが、基礎的な機能の動作や、概念について知ることは重要である。 さらに、実用的な観点からは、 必要以上の試行錯誤をしなくて済むようにtorch についてある程度慣れることは重要である。\n第二部では、説明した基礎の上に、画像認識、時系列データ、表形式データ、音声分類など、様々な深層学習への応用を探求する。 ここでも、概念的な説明に重点を置いている。 さらに、各章では、自分のアプリケーションに使える「テンプレート」としてのアプローチを紹介している。 適切な場合には、稀ではない「ビッグデータ、巨大モデル、大量計算」の方法とは一線を画して、専門知識の組み込みの重要性についても指摘している。\n第三部は、torch でできるディープラーニング以外のことを紹介している。 行列計算（例えば、線型回帰問題の解法）、離散フーリエ変換、ウェーブレット解析などである。 ここで、本書の他のどの部分よりも、概念的な記述が非常に重要であると考えている。 その理由について説明する。\n一つには、読者の教育的背景が様々であると想定しているからである。 Rが自然科学や応用数学に近い分野で教えられ、使われるようになっているため、例えば、離散フーリエ変換がどのように動作するかの概念的な説明には、あまり恩恵を受けられないと感じる人もいるかもしれない。 一方で、これらのことが未知の領域である可能性がある人もいるだろう。 例えば、文学、文化研究、言語学など、伝統的に経験主義的でない背景を持つ人々である。 もちろん、もし読者が後者に属している場合、本書の説明が概念に焦点を合わせているにもかかわらず、それでも非常に（または非常に）数学的であると感じるかもしれない。 その場合でも、安心してほしい。 確かに、（他の多くの有意義なことを理解するのと同様に）数学的な概念を理解するには長い道のりが必要だが、人生をかけてゆっくり進めばよい。\n二つ目に、深層学習が過去10年間のパラダイムであったにもかかわらず、最近の動向は、数学的または専門分野に根差した基盤への関心が再び高まっていることを示している（例えば、Bronstein et al. (2021)　にある体系的に説明やBeyond alchemy: A first look at geometric deep learning （錬金術を超えて–幾何学的深層学習ことはじめ） が概念を紹介している幾何学的ディープラーニングアプローチ参照）。 将来、深層学習技術と専門知識を統合する「ハイブリッド」アプローチがますます増えると予想される。 フーリエ変換は消えない。\n最後にこの話題に関して重要なことは、全ての章には torch のコードを付けたということを言及しておきたい。 例えば、フーリエ変換の場合、専用の機能を使った公式の方法だけでなく、アルゴリズムを自分でコーディングする様々な方法も習得できる。 自作のコードは、驚くほど少ない行数で、非常に印象的なパフォーマンスで実装できる。\n簡単に言えば、これが本書から得られるものである。 最後に、一つだけ言及しておきたいことがある。 述べようとしたが、技術的な内容のため、本編ではあまり触れることができなかったことだ。 私たちの社会では、機械/ディープラーニング（「AI」）の採用が増えるにつれ、政府や民間組織による悪用の機会も増えている。 しばしば、悪意がなくても、結果は壊滅的になることがある。 特に、少数派や既に不利な立場にあるグループにとっては、そのような結果は致命的である。 そのため、現在の政治体制では不可避だが、利益を追求することが、最悪の場合疑問の余地のある機能（監視、すべての量的化）を持つ社会を生み出し、最も不公平、差別、深刻な害をもたらす可能性が高い。 ここで、この問題に注意を向けることをお願いすべく、ブログ記事 Starting to think about AI Fairness （AI公平性についての考察の試み） を参照して、社会生活や自分の仕事や応用において、この問題に積極的に注意を払っていただきたい。\n最後に、感謝の言葉で締めくくりたい。 感謝すべき人々は多すぎるので、誰かを落としてしまうかもしれないが、それでも短く書くことにする。 出版社であるCRC Press（最初に、David GrubbsとCurtis Hill）には、執筆と編集のすべての段階で非常に快適なやり取りをしていただき、感謝している。 そして、以下の人々に対し、この本に関連する支援とそれぞれの役割に対する特別な感謝を表明する。 Daniel Falbel、torch の作成者兼メンテナー、この本を詳細にレビューし、多くの技術的問題を助けてくれた。 Tracy Teal、私の上司で、すべての可能な方法で支援し、励ましてくれた。 Posit（旧RStudio）は、雇用主であり、このような仕事で生計を立てることを許してくれる。\nSigrid Keydana\n\n\n\n\nBronstein, M. M., J. Bruna, T. Cohen, and P. Velickovic, 2021: Geometric deep learning: Grids, groups, graphs, geodesics, and gauges. CoRR, abs/2104.13478.",
    "crumbs": [
      "はじめに"
    ]
  },
  {
    "objectID": "overview.html",
    "href": "overview.html",
    "title": "1  概要",
    "section": "",
    "text": "本書は三部からなる。 第二部と第三部は、それぞれ深層学習の様々な応用例と基本的な科学計算技術を調査する。 その前に、第一部ではtorchの基本的な構成要素である、テンソルや自動微分、最適化法、モジュールについて学ぶ。 この部分を「torchの基礎」、よくある雛型に従って「torchを始める」にすることもできたが、これらが誤った印象を与えると考えた。 これらは確かに基礎ではあるが、基盤としての基礎である。 これらの章を勉強すれば、torchがどのように動いているかしっかりとした概念が身につき、後の節に現れるより複雑な例をいじるのに支障がない程度までコードに馴染むことができる。 言い換えれば、ある程度torchに 堪能 になれる。\nまた、ニューラルネットワークを一度や二度、白紙から書くことになる。 最初は、単なるテンソルそのものとそれが備える機能を使う 次は、ニューラルネットワークの学習に不可欠な機能をオブジェクト指向でカプセル化した、torchの専用の構造を利用する。 結果として、第二部に進む準備が周到に整い、そこで深層学習を様々な問題や分野に適用する。",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>概要</span>"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "2  torchとその入手方法",
    "section": "",
    "text": "2.1 torchの世界\ntorchはPyTorchをRに移植したものである。 PyTorchは、（執筆時点で）産業や研究で最もよく使われている二つの深層学習フレームワークのうちの一つである。 その設計から、様々な科学計算の問題（その一部を本書の最終部で扱う）で有用なすばらしい道具でもある。 torchはRとC++（少しのCを含む）だけで書かれており、これを使うためにPythonをインストールする必要はない。\nPython（PyTorch）の側では、エコシステムは同心円状になっている。 中にPyTorch自体、これなしでは何も動作しない中心ライブラリがある。 これを取り囲むのは、フレームワークライブラリと呼ばれる内側の円があり、特別なデータの種類に特化しているか、運用のような仕事の流れの問題を中心据えている。 さらに、より広範なアドオンや特化したコード、ライブラリのエコシステムがあり、それはPyTorchを構成要素やツールとして使っている。\nRの側では同一の「心臓」を用いている。 全てはtorchのコアに依存している。 Rでも類似のライブラリがあるが、カテゴリの「円」は互いに明確に定められておらず、境界ははっきりとしていない。 活発な開発者のコミュニティがあり、出自や目的も様々だか、さらにtorchを開発して拡張するために共同で活動し、より多くの人々がそれぞれの目的の達成を手助けしている。 エコシステムは急速に成長しており、個々のパッケージに言及することは控えるが、torchのウェブサイトを訪れれば、その一部が掲示されている。\n三つのパッケージについては、名前をあげ、本書で利用する。 それらはtorchvision、torchaudioとluzである。 最初の二つは用途を特定した変換、深層学習モデル、データセット、それぞれ画像（動画を含む）及び音声データのユーティリティを集めたものである。 三番目はtorchの高水準で直感的、使いやすいインターフェースで、ニューラルネットワークの定義、訓練、評価がわずか数行でできる。 torch自体のように三つのパッケージはCRANからインストールできる。",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>`torch`とその入手方法</span>"
    ]
  },
  {
    "objectID": "about.html#torchのインストールと実行",
    "href": "about.html#torchのインストールと実行",
    "title": "2  torchとその入手方法",
    "section": "2.2 torchのインストールと実行",
    "text": "2.2 torchのインストールと実行\ntorchはWindows、macOSとLinuxで利用できる。 対応するGPUと必要なNVIDAのソフトウェアがインストールされていれば、訓練されたモデルの種類次第で、かなりの高速化の恩恵を得られる。 本書の全ての例は、CPUで実行できるものを選び、読者が辛抱強く待つ必要がないようにした。\n適合性の問題は一時的なものであるため、本書ではここに記さない。 同様に具体的なインストールの手順を羅列することも控える。 いつでも最新情報はvignetteから得られる。 問題や質問があれば、torchのGitHubリポジトリに遠慮なくissueを立ててほしい。",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>`torch`とその入手方法</span>"
    ]
  },
  {
    "objectID": "tensors.html",
    "href": "tensors.html",
    "title": "3  テンソル",
    "section": "",
    "text": "3.1 テンソルとは何か\ntorchで何か役に立つことをするには、テンソルについて知る必要がある。 数学や物理の意味のテンソルではない。 TensorFlowや (Py-)Torchのような深層学習フレームワークでは、 テンソル は「単なる」多次元配列で、CPUだけでなく、GPUやTPUのような専用の装置上での高速計算に最適化されたものだ。\n実際、torchのtensorは、Rのarrayに同様に任意の次元を取れる。 Rのarrayとは異なり、高速かつ大規模に計算を実行するために、GPUに移すことができる（おまけに、自動微分ができるので、大変有用だ）。\ntensorはR6オブジェクトに類似していて、$によりフィールドやメソッドを利用できる。\nlibrary(torch)\n\nt1 &lt;- torch_tensor(1)\nt1\n\ntorch_tensor\n 1\n[ CPUFloatType{1} ]\nこれは単一の値1だけを格納したテンソルだ。 CPUに「生息」しており、その型はFloat。 次に波括弧の中の1{1}に着目する。 これはテンソルの値を改めて示したものではない。 これはテンソルの形状、つまりそれが生息する空間と次元の長さである。 Base Rと同様にベクトルは単一の要素だけでもよい （base Rは1とc(1)を区別しないことを思い出してほしい）。\n前述の$記法を使って、一つ一つ関連するフィールドを参照することで、個別に以上の属性を確認できる。\nt1$dtype\n\ntorch_Float\nt1$device\n\ntorch_device(type='cpu')\nt1$shape\n\n[1] 1\nテンソルの $to() を使うと、メソッドいくつかの属性は直接変更できる。\nt2 &lt;- t1$to(dtype = torch_int())\nt2$dtype\n\ntorch_Int\n# GPUがある場合\n#t2 &lt;- t1$to(device = \"GPU\")\n# Apple Siliconの場合\nt2 &lt;- t1$to(device = \"mps\")\nt2$device\n\ntorch_device(type='mps', index=0)\n形状の変更はどのようにするのか。 これは別途扱うに値する話題だが、手始めにいじってみることにする。 値の変更なしに、この1次元の「ベクトルテンソル」を2次元の「行列テンソル」にできる。\nt3 &lt;- t1$view(c(1, 1))\nt3$shape\n\n[1] 1 1\n概念的には、Rで1要素のベクトルや行列を作るのに似ている。\nc(1)\n\n[1] 1\n\nmatrix(1)\n\n     [,1]\n[1,]    1\nテンソルがどのようなものか分かったところで、いくつかのテンソルを作る方法について考えてみる。",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>テンソル</span>"
    ]
  },
  {
    "objectID": "tensors.html#テンソルの作成",
    "href": "tensors.html#テンソルの作成",
    "title": "3  テンソル",
    "section": "3.2 テンソルの作成",
    "text": "3.2 テンソルの作成\n既に見たテンソルを作る一つの方法はtorch_tensor()を呼び出し、Rの値を渡すというものだった。 この方法は多次元オブジェクトに適用でき、以下にいくつかの例を示す。\nしかし、多くの異なる値を渡す必要があるときは効率が悪くなる。 ありがたいことに、値が全て同一であるべき場合や、明示的なパターンに従うときに適用できる別の方法がある。 この節ではこの技についても説明する。\n\n3.2.1 値からテンソル\n前の例では単一要素のベクトルをtorch_tensor()に渡したが、より長いベクトルを同様に渡すことができる。\n\ntorch_tensor(1:5)\n\ntorch_tensor\n 1\n 2\n 3\n 4\n 5\n[ CPULongType{5} ]\n\n\n同様に規定のデバイスはCPUだが、最初からGPU/MPSに配置するテンソルを作成することもできる。\n\n#torch_tensor(1:5, device = \"cuda\")\ntorch_tensor(1:5, device = \"mps\")\n\ntorch_tensor\n 1\n 2\n 3\n 4\n 5\n[ MPSLongType{5} ]\n\n\nこれまで作ってきたのはベクトル。 行列、つまり2次元テンソルはどうやって作るのか。\nRの行列を同様に渡せばよい。\n\ntorch_tensor(matrix(1:9, ncol = 9))\n\ntorch_tensor\n 1  2  3  4  5  6  7  8  9\n[ CPULongType{1,9} ]\n\n\n結果を見てほしい。 1から9までの数字は、列ごとに表示されている。 これは意図通りかもしれないし、そうではないかもしれない。 意図と異なる場合はmatrix()にbyrow = TRUEを渡せばよい。\n\ntorch_tensor(matrix(1:9, ncol = 3, byrow = TRUE))\n\ntorch_tensor\n 1  2  3\n 4  5  6\n 7  8  9\n[ CPULongType{3,3} ]\n\n\n高次元のデータはどうするか。 同様の方針に従って、配列を渡すことができる。\n\ntorch_tensor(array(1:24, dim = c(4, 3, 2)))\n\ntorch_tensor\n(1,.,.) = \n   1  13\n   5  17\n   9  21\n\n(2,.,.) = \n   2  14\n   6  18\n  10  22\n\n(3,.,.) = \n   3  15\n   7  19\n  11  23\n\n(4,.,.) = \n   4  16\n   8  20\n  12  24\n[ CPULongType{4,3,2} ]\n\n\nこの場合でも、結果はRの埋め方に沿ったものとなる。 これが求めるものではないなら、テンソルを構築するプログラムを書いた方が簡単かもしれない。\n慌てる前に、その必要が非常に稀であることを考えてみてほしい。 実際は、Rのデータセットからテンソルを作ることがほとんどだ。 「データセットからテンソル」の最後の小節で詳しく確認する。 その前に、少し時間をとって最後の出力を少し吟味しよう。\n 渡したテンソルは以下のように印字される。\n\narray(1:24, dim = c(4, 3, 2))\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n[3,]    3    7   11\n[4,]    4    8   12\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]   13   17   21\n[2,]   14   18   22\n[3,]   15   19   23\n[4,]   16   20   24\n\n\n上のテンソルの印字と比較しよう。 Arrayとtensorは異なる方向にオブジェクトを切っている。 テンソルは値を3x2の上向きと奥に向かう広がる長方形に切り、4つの\\(x\\)のそれぞれの値に対して一つ示している。 一方、配列はzの値で分割し、二つの奥向きと右向きに進む大きな4x3の部分を示す。\n言い換えれば、テンソルは左/「外側」から、配列は右/「内側」から思考を始めているとも言えるだろう。\n\n\n3.2.2 指定からテンソル\ntorchの大口生成函数が便利な状況は、おおまかに二つある。 一つは、テンソルの個々の値は気にせず、分布のみに興味がある場合だ。 もう一つは、ある一定のパターンに従う場合だ。\n要素の値の代わりに、大口生成函数を使うときは、取るべき形状を指定する。 例えば、3x3のテンソルを生成し、標準正規分部の値で埋める場合は次のようにする。\n\ntorch_randn(3, 3)\n\ntorch_tensor\n-0.5821 -0.0801  0.4844\n-1.1522  1.8918 -1.2370\n 1.2396  1.0422 -1.1544\n[ CPUFloatType{3,3} ]\n\n\n次に示すのは、0と1の間の一様分布に対する同様なもの。\n\ntorch_rand(3, 3)\n\ntorch_tensor\n 0.2531  0.2757  0.0178\n 0.3006  0.9228  0.0289\n 0.4241  0.2712  0.3990\n[ CPUFloatType{3,3} ]\n\n\n全て1や0からなるテンソルが必要となることがよくある。\n\ntorch_zeros(2, 5)\n\ntorch_tensor\n 0  0  0  0  0\n 0  0  0  0  0\n[ CPUFloatType{2,5} ]\n\n\n\ntorch_ones(2, 2)\n\ntorch_tensor\n 1  1\n 1  1\n[ CPUFloatType{2,2} ]\n\n\n他にも多くの大口生成函数がある。 最後に線型代数で一般的ないくつかの行列を作る方法を見ておく。 これは単位行列。\n\ntorch_eye(n = 5)\n\ntorch_tensor\n 1  0  0  0  0\n 0  1  0  0  0\n 0  0  1  0  0\n 0  0  0  1  0\n 0  0  0  0  1\n[ CPUFloatType{5,5} ]\n\n\nそしてこれは対角行列。\n\ntorch_diag(c(1, 2, 3))\n\ntorch_tensor\n 1  0  0\n 0  2  0\n 0  0  3\n[ CPUFloatType{3,3} ]\n\n\n\n\n3.2.3 データセットからテンソル\nさて、Rのデータセットからテンソルを作る方法を見ていこう。 データセットよっては、この過程は「自動」であったり、考慮や操作が必要になったりする。\nまず、base RについてくるJohnsonJohnsonを試してみる。 これは、Johnson & Johnsonの一株あたりの四半期利益の時系列である。\n\nJohnsonJohnson\n\n      Qtr1  Qtr2  Qtr3  Qtr4\n1960  0.71  0.63  0.85  0.44\n1961  0.61  0.69  0.92  0.55\n1962  0.72  0.77  0.92  0.60\n1963  0.83  0.80  1.00  0.77\n1964  0.92  1.00  1.24  1.00\n1965  1.16  1.30  1.45  1.25\n1966  1.26  1.38  1.86  1.56\n1967  1.53  1.59  1.83  1.86\n1968  1.53  2.07  2.34  2.25\n1969  2.16  2.43  2.70  2.25\n1970  2.79  3.42  3.69  3.60\n1971  3.60  4.32  4.32  4.05\n1972  4.86  5.04  5.04  4.41\n1973  5.58  5.85  6.57  5.31\n1974  6.03  6.39  6.93  5.85\n1975  6.93  7.74  7.83  6.12\n1976  7.74  8.91  8.28  6.84\n1977  9.54 10.26  9.54  8.73\n1978 11.88 12.06 12.15  8.91\n1979 14.04 12.96 14.85  9.99\n1980 16.20 14.67 16.02 11.61\n\n\ntorch_tensor()に渡すだけで、魔法のようにほしいものが手に入るだろうか。\n\ntorch_tensor(JohnsonJohnson)\n\ntorch_tensor\n  0.7100\n  0.6300\n  0.8500\n  0.4400\n  0.6100\n  0.6900\n  0.9200\n  0.5500\n  0.7200\n  0.7700\n  0.9200\n  0.6000\n  0.8300\n  0.8000\n  1.0000\n  0.7700\n  0.9200\n  1.0000\n  1.2400\n  1.0000\n  1.1600\n  1.3000\n  1.4500\n  1.2500\n  1.2600\n  1.3800\n  1.8600\n  1.5600\n  1.5300\n  1.5900\n... [the output was truncated (use n=-1 to disable)]\n[ CPUFloatType{84} ]\n\n\nうまくいっているようだ。 値は希望通り四半期ごとに並んでいる。\n魔法？いや、そうではない。 torchができるのは与えられたものに対して動作することだ。 ここでは、与えられたのは実は四半期順に並んだdoubleのベクトル。 データはtsクラスなので、その通りに印字されただけだ。\n\nunclass(JohnsonJohnson)\n\n [1]  0.71  0.63  0.85  0.44  0.61  0.69  0.92  0.55  0.72  0.77  0.92  0.60\n[13]  0.83  0.80  1.00  0.77  0.92  1.00  1.24  1.00  1.16  1.30  1.45  1.25\n[25]  1.26  1.38  1.86  1.56  1.53  1.59  1.83  1.86  1.53  2.07  2.34  2.25\n[37]  2.16  2.43  2.70  2.25  2.79  3.42  3.69  3.60  3.60  4.32  4.32  4.05\n[49]  4.86  5.04  5.04  4.41  5.58  5.85  6.57  5.31  6.03  6.39  6.93  5.85\n[61]  6.93  7.74  7.83  6.12  7.74  8.91  8.28  6.84  9.54 10.26  9.54  8.73\n[73] 11.88 12.06 12.15  8.91 14.04 12.96 14.85  9.99 16.20 14.67 16.02 11.61\nattr(,\"tsp\")\n[1] 1960.00 1980.75    4.00\n\n\nこれはうまくいった。 別なものを試そう。\n\ndim(Orange)\n\n[1] 35  3\n\n\n\nhead(Orange)\n\n  Tree  age circumference\n1    1  118            30\n2    1  484            58\n3    1  664            87\n4    1 1004           115\n5    1 1231           120\n6    1 1372           142\n\n\n\ntorch_tensor(Orange)\n\nError in torch_tensor_cpp(data, dtype, device, requires_grad, pin_memory): R type not handled\n\n\nどの型が処理されないのか。 「元凶」は順序付き因子の列Treeに違いないのは明らかだ。 先にtorchが因子を扱えるか確認する。\n\nf &lt;- factor(c(\"a\", \"b\", \"c\"), ordered = TRUE)\ntorch_tensor(f)\n\ntorch_tensor\n 1\n 2\n 3\n[ CPULongType{3} ]\n\n\nこれは問題なく動作した。 他に何がありうるか。 ここでの問題は含まれている構造data.structureである。 as.matrix()を先に作用させる必要がある。 でも、因子が存在するので、全て文字列の配列になってしまい、希望通りにならない。 したがって、基礎となるレベル（整数）を抽出してから、data.frameから行列に変換する。\n\n orange_ &lt;- Orange |&gt;\n  transform(Tree = as.numeric(Tree)) |&gt;\n  as.matrix()\n\ntorch_tensor(orange_) |&gt; print(n = 7)\n\ntorch_tensor\n    2   118    30\n    2   484    58\n    2   664    87\n    2  1004   115\n    2  1231   120\n    2  1372   142\n    2  1582   145\n... [the output was truncated (use n=-1 to disable)]\n[ CPUFloatType{35,3} ]\n\n\n同じことを別のdata.frame、modeldataのokcでしてみよう。\n\n\n\n\n\n\nCaution\n\n\n\nokcはmodeldataの0.1.1で廃止となり、0.1.2以降は削除された。\n\n\n\nload(\"data/okc.RData\")\n\nhead(okc)\n\n  age              diet height            location       date Class\n1  22 strictly anything     75 south san francisco 2012-06-28 other\n2  35      mostly other     70             oakland 2012-06-29 other\n3  38          anything     68       san francisco 2012-06-27 other\n4  23        vegetarian     71            berkeley 2012-06-28 other\n5  29              &lt;NA&gt;     66       san francisco 2012-06-27 other\n6  29   mostly anything     67       san francisco 2012-06-29  stem\n\n\n\ndim(okc)\n\n[1] 59855     6\n\n\n二つある整数の列は問題なく、一つある因子の列の扱い方は学んだ。 characterとdateの列はどうだろう。 個別にdateの列からテンソルを作ってみる。\n\nprint(torch_tensor(okc$date), n = 7)\n\ntorch_tensor\n 15519\n 15520\n 15518\n 15519\n 15518\n 15520\n 15516\n... [the output was truncated (use n=-1 to disable)]\n[ CPUFloatType{59855} ]\n\n\nこれはエラーを投げなかったが、何を意味するのか。 これらはRのDateに格納されている実際の値、つまり1970年1月1日からの日数である。 すなわち、技術的には動作する変換だ。 結果が実際に意味をなすかは、どのようにそれを使うつもりかという問題だ。 言い換えれば、おそらく計算に使う前に、これらのデータを追加の処理する必要がある。 どのようにするかは文脈次第。\n次にlocationを見る。 これは、character型の列のうちの一つだ。 そのままtorchに渡すとどうなるか。\n\ntorch_tensor(okc$location)\n\nError in torch_tensor_cpp(data, dtype, device, requires_grad, pin_memory): R type not handled\n\n\n実際torchには文字列を格納するテンソルはない。 これらをnumeric型に変換する何らかの方法を適用する必要がある。 この例のような場合、個々の観測が単一の実体（例えば文やパラグラフではなく）を含む場合、最も簡単な方法はRでfactorに変換し、numeric、そしてtensorにすることだ。\n\nokc$location |&gt;\n  factor() |&gt;\n  as.numeric() |&gt;\n  torch_tensor() |&gt;\n  print(n = 7)\n\ntorch_tensor\n 120\n  74\n 102\n  10\n 102\n 102\n 102\n... [the output was truncated (use n=-1 to disable)]\n[ CPUFloatType{59855} ]\n\n\n確かに、技術的にはこれはうまく動作する。 しかしながら、情報が失われる。 例えば、最初と3番目の場所はそれぞれ”south san francisco”と”san francisco”だ。 一度因子に変換されると、これらは意味の上で”san francisco”や他の場所と同じ距離になる。 繰り返しになるが、これが重要かはデータの詳細と目的次第だ。 これが重要なら、例えば、観測をある基準でまとめたり、緯度/経度に変換したりすることを含めさまざまな対応がありうる。 これらの考慮は全くtorchに特有ではないが、ここで述べたのはtorchの「データ統合フロー」に影響するからだ\n最後に実際のデータ科学の世界に挑むには、NAを無視するわけにはいかない。 確認しよう。\n\ntorch_tensor(c(1, NA, 3))\n\ntorch_tensor\n 1\nnan\n 3\n[ CPUFloatType{3} ]\n\n\nRのNAはNaNに変換された。 これを扱えるだろうか。 いくつかのtorchのかんすうでは可能だ。 例えば、torch_nanquantile()は単にNaNを無視する。\n\ntorch_nanquantile(torch_tensor(c(1, NA, 3)), q = 0.5)\n\ntorch_tensor\n 2\n[ CPUFloatType{1} ]\n\n\nただし、ニューラルネットワークを訓練するなら、欠損値を意味のあるように置き換える方法を考える必要があるが、この話題は後回しにする。",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>テンソル</span>"
    ]
  },
  {
    "objectID": "tensors.html#テンソルに対する操作",
    "href": "tensors.html#テンソルに対する操作",
    "title": "3  テンソル",
    "section": "3.3 テンソルに対する操作",
    "text": "3.3 テンソルに対する操作\nテンソルに対する数学的操作は全て可能だ。和、差、積など。 これらの操作は（torch_で始まる）函数や（$記法で呼ぶ）オブジェクトに対するメソッドとして利用可能だ。 次の二つは同じだ。\n\nt1 &lt;- torch_tensor(c(1, 2))\nt2 &lt;- torch_tensor(c(3, 4))\n\ntorch_add(t1, t2)\n\ntorch_tensor\n 4\n 6\n[ CPUFloatType{2} ]\n\n\n\nt1$add(t2)\n\ntorch_tensor\n 4\n 6\n[ CPUFloatType{2} ]\n\n\nどちらも新しいオブジェクトが生成され、t1もt2も変更されない。 オブジェクトをその場で変更する別のメソッドもある。\n\nt1$add_(t2)\n\ntorch_tensor\n 4\n 6\n[ CPUFloatType{2} ]\n\n\n\nt1\n\ntorch_tensor\n 4\n 6\n[ CPUFloatType{2} ]\n\n\n実は、同じパターンは他の演算にも適用される。 アンダスコアが後についているのを見たら、オブジェクトはその場で変号とされる。\n当然、科学計算の場面では行列演算は特に重要だ。 二つの一次元構造、つまりベクトルの内積から始める。\n\nt1 &lt;- torch_tensor(1:3)\nt2 &lt;- torch_tensor(4:6)\nt1$dot(t2)\n\ntorch_tensor\n32\n[ CPULongType{} ]\n\n\nこれは動かないはずだと考えただろうか。 テンソルの一つを転置（torch_t()）する必要があっただろうか。 これも動作する。\n\nt1$t()$dot(t2)\n\ntorch_tensor\n32\n[ CPULongType{} ]\n\n\n最初の呼び出しも動いたのは、torchが行ベクトルと列ベクトルを区別しないからだ。 結果として、torch_matmul()を使ってベクトルを行列にかけるときも、ベクトルの向きを心配する必要はない。\n\nt3 &lt;- torch_tensor(matrix(1:12, ncol = 3, byrow = TRUE))\nt3$matmul(t1)\n\ntorch_tensor\n 14\n 32\n 50\n 68\n[ CPULongType{4} ]\n\n\n同じ函数torch_matmul()は二つの行列をかけるときにも使う。 これがtorch_multiply()の動作、つまり引数のスカラ積とどのように異なるかよく見てほしい。\n\ntorch_multiply(t1, t2)\n\ntorch_tensor\n  4\n 10\n 18\n[ CPULongType{3} ]\n\n\nテンソル演算は他にも多数あり、勉強の途中、いくつかに出会うことになるか、特に述べておく必要な集まりが一つある。\n\n3.3.1 集計\nR行列に対して和を計算する場合、それは次の三つのうちの一つを意味する。 総和、行の和、もしくは列の和。 これら三つを見てみよう（訳あってapply()を使う）。\n\nm &lt;- outer(1:3, 1:6)\n\nsum(m)\n\n[1] 126\n\napply(m, 1, sum)\n\n[1] 21 42 63\n\napply(m, 2, sum)\n\n[1]  6 12 18 24 30 36\n\n\nそれではtorchで同じことをする。 総和から始める。\n\nt &lt;- torch_outer(torch_tensor(1:3), torch_tensor(1:6))\nt$sum()\n\ntorch_tensor\n126\n[ CPULongType{} ]\n\n\n行と列の和は面白くなる。 dim引数はtorchにどの次元の和をとるか伝える。 dim = 1を渡すと次のようになる。\n\nt$sum(dim = 1)\n\ntorch_tensor\n  6\n 12\n 18\n 24\n 30\n 36\n[ CPULongType{6} ]\n\n\n予想外にも列の和になった。 結論を導く前に、dim = 2だとどうなるか。\n\nt$sum(dim = 2)\n\ntorch_tensor\n 21\n 42\n 63\n[ CPULongType{3} ]\n\n\n今度は行の和である。 torchの次元の順序を誤解したのだろうか。そうではない。 torchでは、二つの次元があれば行が第一で列が第二である （すぐに示すように、添え字はRで一般的なのと同じで1から始まる）。\nむしろ、概念の違いは集計にある。 Rにおける集計は、頭の中にあるものをよく特徴づけている。 行（次元1）ごとに集計して行のまとめを得て、列（次元2）ごとに集計した列のまとめを得る。 torchでは考え方が異なる。 列（次元2）を圧縮して行のまとめを計算し、行（次元1）で列のまとめを得る。\n同じ考え方がより高い次元に対しても適用される。 例えば、4人の時系列データを記録しているとする。 二つの特徴量を3回計測する。 再帰型ニューラルネットワーク（詳しくは後ほど）を訓練する場合、測定を次のように並べる。\n\n次元1: 個人に亙る。\n次元2: 時刻に亙る。\n次元3: 特徴に亙る。\n\nテンソルは次のようになる。\n\nt &lt;- torch_randn(4, 3, 2)\nt\n\ntorch_tensor\n(1,.,.) = \n  0.7267  0.8300\n  1.9565  0.1990\n -0.5270  1.0657\n\n(2,.,.) = \n -0.3741 -0.2190\n  1.2192 -0.2116\n -0.3635 -0.6467\n\n(3,.,.) = \n  0.0009  0.2203\n  0.6489  1.9502\n -0.7912  0.3545\n\n(4,.,.) = \n  0.0554  0.6667\n -0.0392  0.8053\n  1.9643  0.3670\n[ CPUFloatType{4,3,2} ]\n\n\n二つの特徴量についての平均は、対象と時刻に独立で、次元1と2を圧縮する。\n\nt$mean(dim = c(1, 2))\n\ntorch_tensor\n 0.3731\n 0.4484\n[ CPUFloatType{2} ]\n\n\n一方、特徴量について平均を求めるが、各個人に対するものは次のように計算する。\n\nt$mean(dim = 2)\n\ntorch_tensor\n 0.7187  0.6982\n 0.1605 -0.3591\n-0.0471  0.8417\n 0.6602  0.6130\n[ CPUFloatType{4,2} ]\n\n\nここで圧縮されたは時刻である。",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>テンソル</span>"
    ]
  },
  {
    "objectID": "tensors.html#テンソルの部分参照",
    "href": "tensors.html#テンソルの部分参照",
    "title": "3  テンソル",
    "section": "3.4 テンソルの部分参照",
    "text": "3.4 テンソルの部分参照\nテンソルを使っていると、計算のある部分が入力テンソルの一部にのみに対する演算であることはよくある。 その部分が単一の実体（値、行、列など）なら添字参照、このような実体の範囲なら切り出しと呼ばれる。\n\n3.4.1 「R思考」\n添字参照も切り出しも基本的にはRと同じように働く。 いくつかの拡張された記法を続く節で示すが、総じてふるまいは直感に反しない。\nなぜならRと同じように、torchでも添字は1から始まるし、1要素になった次元は落とされるからだ。\n下の例では、2次元テンソルの最初の行を求め、その結果1次元つまりベクトルを得る。\n\nt &lt;- torch_tensor(matrix(1:9, ncol = 3, byrow = TRUE))\nt[1, ]\n\ntorch_tensor\n 1\n 2\n 3\n[ CPULongType{3} ]\n\n\nただし、drop = FALSEを指定すると次元は保持される。\n\nt[1, , drop = FALSE]\n\ntorch_tensor\n 1  2  3\n[ CPULongType{1,3} ]\n\n\n切り出しの時は、1要素となる次元はないので、他に考慮すべきことはない。\n\nt &lt;- torch_rand(3, 3, 3)\nt[1:2, 2:3, c(1, 3)]\n\ntorch_tensor\n(1,.,.) = \n  0.6393  0.2948\n  0.8614  0.8637\n\n(2,.,.) = \n  0.0868  0.5802\n  0.6064  0.5525\n[ CPUFloatType{2,2,2} ]\n\n\nまとめると、添字参照と切り出しはほぼRと同じように働く。 次に、前に述べた、さらに使いやすくする拡張について見る。\n\n\n3.4.2 Rを越える\n拡張の一つはテンソルの最後の要素の参照だ。 利便性のため、torchでは-1を使ってそれができる。\n\nt &lt;- torch_tensor(matrix(1:4, ncol = 2, byrow = TRUE))\nt[-1, -1]\n\ntorch_tensor\n4\n[ CPULongType{} ]\n\n\n注意すべきは、Rでは負の添字はかなり異なった効果を持ち、対応する位置の要素は取り除かれることだ。\nもう一つの便利な機能は、切り出しの記法で刻み幅を二つ目のコロンの後に指定できることだ。 ここでは、一つ目から八つ目の列を一つおきに取り出している。\n\nt &lt;- torch_tensor(matrix(1:20, ncol = 10, byrow = TRUE))\nt[ , 1:8:2]\n\ntorch_tensor\n  1   3   5   7\n 11  13  15  17\n[ CPULongType{2,4} ]\n\n\n最後に示すのは、同じコードを異なる次元のテンソルに対して動作させる方法だ。 この場合、..を使って明示的に参照されていない、存在する次元全てをまとめて指定できる。\n例えば、行列、配列、もしくは高次元の構造など、どんなテンソルが渡されても最初の次元の添字参照をしたいとする。 次の\nt[1, ..]\nは全てに対して機能する。\n\nt1 &lt;- torch_randn(2, 2)\nt2 &lt;- torch_randn(2, 2, 2)\nt3 &lt;- torch_randn(2, 2, 2, 2)\nt1[1, ..]\n\ntorch_tensor\n 2.1753\n 1.2914\n[ CPUFloatType{2} ]\n\nt2[1, ..]\n\ntorch_tensor\n 2.2479  1.1791\n 1.3373  0.5651\n[ CPUFloatType{2,2} ]\n\nt3[1, ..]\n\ntorch_tensor\n(1,.,.) = \n -1.3390 -1.3083\n  0.4102 -0.6495\n\n(2,.,.) = \n  2.4919 -0.9392\n -0.7270  0.4037\n[ CPUFloatType{2,2,2} ]\n\n\n最後の次元の添字参照がしたければ、代わりにt[.., 1]と書けばよい。 両方を組み合わせることもできる。\n\nt3[1, .., 2]\n\ntorch_tensor\n-1.3083 -0.6495\n-0.9392  0.4037\n[ CPUFloatType{2,2} ]\n\n\n次の話題は、添字参照や切り出しと同じくらい重要なテンソルの変形だ。",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>テンソル</span>"
    ]
  },
  {
    "objectID": "tensors.html#テンソルの変形",
    "href": "tensors.html#テンソルの変形",
    "title": "3  テンソル",
    "section": "3.5 テンソルの変形",
    "text": "3.5 テンソルの変形\n24要素のテンソルがあるとする。 形状はどうなっているか。 次の可能性がある。\n\n長さ24のベクトル\n24 x 1、12 x 2、6 x 4などの行列\n24 x 1 x 1, 12 x 2 x 1などの3次元配列\nその他（24 x 1 x 1 x 1 x 1という可能性もありうる）\n\n値をお手玉しなくても、view()メソッドでテンソルの形状を変更できる。 最初のテンソルは長さ24のベクトルとする。\n\nt &lt;- torch_zeros(24)\nprint(t, n = 3)\n\ntorch_tensor\n 0\n 0\n 0\n... [the output was truncated (use n=-1 to disable)]\n[ CPUFloatType{24} ]\n\n\n同じベクトルを横長の行列に変形する。\n\nt2 &lt;- t$view(c(2, 12))\n\n新しいテンソルt2を得たが、興味深いことに（そして性能の上で重要なことに）、torchはその値に対して新たに記憶域を割り付ける必要がなかったことだ。 自分で確認することができる。 二つのテンソルはデータを同じ場所に格納している。\n\nt$storage()$data_ptr()\n\n[1] \"0x14a6947c0\"\n\nt2$storage()$data_ptr()\n\n[1] \"0x14a6947c0\"\n\n\nどのように実現されているか少し議論する。\n\n3.5.1 複製なし変形と複製あり変形\ntorchにテンソルの変形をさせると、テンソルの中身に対して新たな記憶域を割り付けずに要求を達成しようとする。 これが実現可能なのは、同じデータ、究極的には同じバイト列は異なる方法で読み出すことができるからだ。 必要なのはメタデータの記憶域だけだ。\ntorchはどのようにしているか。 具体的な例を見てみる。 3 x 5行列から始める。\n\nt &lt;- torch_tensor(matrix(1:15, nrow = 3, byrow = TRUE))\nt\n\ntorch_tensor\n  1   2   3   4   5\n  6   7   8   9  10\n 11  12  13  14  15\n[ CPULongType{3,5} ]\n\n\nテンソルにはstride()メソッドがあり、各次元に対して次の要素にたどり着くまでにいくつの要素を越えたか追跡する。 上記のテンソルtに対して、次の行に進むには5要素飛ばす必要があるが、次の列には一つだけ飛ばせばよい。\n\nt$stride()\n\n[1] 5 1\n\n\nここで、テンソルを変形して、今度は5行3列にする。 データ自体は変化しないことを思い出してほしい。\n\nt2 &lt;- t$view(c(5, 3))\nt2\n\ntorch_tensor\n  1   2   3\n  4   5   6\n  7   8   9\n 10  11  12\n 13  14  15\n[ CPULongType{5,3} ]\n\n\n今回は次の行には、5要素ではなく3要素だけ飛ばせば次の行に到達する。 次の列に進むのは、ここでも1要素だけ「跳べ」ばよい。\n\nt2$stride()\n\n[1] 3 1\n\n\nここで、要素の順序を変えることができるか考えてみよう。 例えば、行列の転置はメタデータの方法で可能だろうか。\n\nt3 &lt;- t$t()\nt3\n\ntorch_tensor\n  1   6  11\n  2   7  12\n  3   8  13\n  4   9  14\n  5  10  15\n[ CPULongType{5,3} ]\n\n\n元のテンソルとその転置はメモリ上の同じ場所を指しているので、実際に可能であるはずだ。\n\nt$storage()$data_ptr()\n\n[1] \"0x1046879c0\"\n\nt3$storage()$data_ptr()\n\n[1] \"0x1046879c0\"\n\n\nこれは道理にかなっている。 次の行に到達するのに、1要素だけ跳び、次の列には5要素跳べばよいから、うまくいくだろう。 確認する。\n\nt3$stride()\n\n[1] 1 5\n\n\nその通りだ。\n可能な限り、torchは形状を変更する演算をこの方法で扱おうとする。\nこのような（今後多数見ることになる）複製なし演算の一つはsqueeze()とその対義語unsqueeeze()だ。 後者は指定位置に単一要素の次元を付け加え、前者は取り除く。 例を挙げる。\n\nt &lt;- torch_randn(3)\nt\n\ntorch_tensor\n 0.3105\n-1.3792\n-0.0361\n[ CPUFloatType{3} ]\n\nt$unsqueeze(1)\n\ntorch_tensor\n 0.3105 -1.3792 -0.0361\n[ CPUFloatType{1,3} ]\n\n\nここでは単一要素の次元を前につけた。 代わりに、t$unsqueeze(2)を使えば末尾につけることもできた。\nさて、複製なしの技法は失敗することがあるか。 そのような例を示す。\n\nt &lt;- torch_randn(3, 3)\nt$t()$view(9)\n\nError in (function (self, size) :\nview size is not compatible with input tensor's size and\nstride (at least one dimension spans across two contiguous subspaces).\nUse .reshape(...) instead.\nストライドを変える演算を連続して行うと、二つ目は失敗する可能性が高い。 失敗するかどうか決める方法はあるが、簡単な方法はview()の代わりにreshape()を使うことだ。 後者は魔法のように機能し、可能であればメタデータで、そうでなければ複製する。\n\nt &lt;- torch_randn(3, 3)\nt2 &lt;- t$t()$reshape(9)\n\nt$storage()$data_ptr()\n\n[1] \"0x14a6c9440\"\n\nt2$storage()$data_ptr()\n\n[1] \"0x14a6c5480\"\n\n\n想像通り、二つのテンソルは今度は異なる場所に格納されている。\nこの長い章の終わりに取り上げる内容は、一見手に余るように見える機能だが、性能の上で極めて重要なものである。 多くのもののように、慣れるには時間が掛かるが、安心してほしい。 この本やtorchを使った多くのプロジェクトでたびたび目にすることになる。 この機能 拡張 （ブロードキャスト） と呼ばれている。",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>テンソル</span>"
    ]
  },
  {
    "objectID": "tensors.html#拡張",
    "href": "tensors.html#拡張",
    "title": "3  テンソル",
    "section": "3.6 拡張",
    "text": "3.6 拡張\n形状が厳密に一致しないテンソルに対する演算をすることが多い。\nもちろん、長さ2のベクトルに長さ5のベクトルを足すようなことはしないかもしれない。 でもやってみたいこともありうる。 例えば、全ての要素にスカラを掛けることがあるが、これはできる。\n\nt1 &lt;- torch_randn(3, 5)\nt1 * 0.5\n\ntorch_tensor\n 1.4511  0.5745  0.6610  0.0257  0.3183\n-0.2256  0.0115  0.2676 -0.1429  0.3385\n 0.6451  0.7366 -0.3541 -0.5102  0.7267\n[ CPUFloatType{3,5} ]\n\n\nこれはおそらく大したことではなかっただろう。 Rで慣れている。 しかし、次はRでは動作しない。 同じベクトルを行列の全ての行に加えようとしている。\n\nm &lt;- matrix(1:15, ncol = 5, byrow = TRUE)\nm2 &lt;- matrix(1:5, ncol = 5, byrow = TRUE)\n\nm2をベクトルに代えてもうまくいかない。\n\nm3 &lt;- 1:5\n\nm + m3\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    2    6    5    9    8\n[2,]    8   12   11   10   14\n[3,]   14   13   17   16   20\n\n\n文法としては動いたが、意味の上では意図した通りではない。\nここで、上の二つをtorchで試してみる。 まず二つのテンソルが2次元の場合（概念的には一つは行ベクトルだが）から。\n\nt &lt;- torch_tensor(m)\nt2 &lt;- torch_tensor(m2)\n\nt$shape\n\n[1] 3 5\n\nt2$shape\n\n[1] 1 5\n\nt$add(t2)\n\ntorch_tensor\n  2   4   6   8  10\n  7   9  11  13  15\n 12  14  16  18  20\n[ CPULongType{3,5} ]\n\n\n次に足すものが1次元テンソルの場合。\n\nt3 &lt;- torch_tensor(m3)\n\nt3$shape\n\n[1] 5\n\nt$add(t3)\n\ntorch_tensor\n  2   4   6   8  10\n  7   9  11  13  15\n 12  14  16  18  20\n[ CPULongType{3,5} ]\n\n\ntorchではどちらも意図通りにうまくいった。 理由を考えてみよう。 上の例でテンソルの形状をあえて印字した。 3 x 5のテンソルには、形状3のテンソルも形状1 x 5のテンソルも足すことができた。 これらは、拡張がどのようにされるか示している。 簡単にいうと、起きたのは次の通りだ。\n\n1 x 5テンソルが加数として使われると、実際は拡張される。 つまり、同じ3行あるかのように扱われる。 このような拡張は一致しない次元が単一で一番左にある場合にのみ実行される。\n形状3のテンソルも同様だが、先に手順が追加される。 大きさが1の主要な次元が実質上左に追加される。 これにより1と同様になり、そこから手順が続く。\n\n重要なのは、物理的な拡張はされないことだ。\nルールを系統だてることにする。\n\n3.6.1 拡張のルール\nルールは次の通り。 まず、一つ目は目を引くものではないか、全ての基礎になる。\n\nテンソルの形状を右に揃える。\n\n二つのテンソル、一つのサイズは3 x 7 x 1、もう一つは1 x 5、があるとする。 これらを右に揃える。\nt1, 形状:     3 7 1\nt2, 形状:       1 5\n\n右から始めて、揃えた軸に沿う大きさが厳密に一致するか、一つが1でなければならない。 後者の場合、単一要素次元のテンソルは単一でないものに対して 拡張 される。\n\n上の例では、拡張は各テンソルに1回ずつ2回発生する。 結果は実質的に以下のようになる。\nt1, 形状:     3 7 5\nt2, 形状:       7 5\n\nもしテンソルの一つが一つ（もしくは1以上）余分な軸があれば、実質的に拡張される。\nt1, 形状: 3 7 5 t2, 形状: 1 7 5\n\nそして拡張が発生する。\nt1, 形状:     3 7 5\nt2, 形状:     3 7 5\nこの例では、拡張が両方のテンソルに同時に発生していることを見た。 覚えておくべきことは、常に右から見るということだ。 次の例は、どんな拡張をしてもうまくいかない。\ntorch_zeros(4, 3, 2, 1)$add(torch_ones(4, 3, 2)) # error\n\nおそらく、この本の中で、この章は最も長く、最も応用から離れたように見えるものだった。 しかし、テンソルに慣れることは、torchをすらすら書くための前提であると言っておく。 同様のことは次の章で扱う話題、自動微分についても言える。 違いは、torchが大変な仕事を我々の代わりにしてくれるということだ。 我々は何をしているか理解すればよいだけだ。",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>テンソル</span>"
    ]
  },
  {
    "objectID": "autograd.html",
    "href": "autograd.html",
    "title": "4  自動微分",
    "section": "",
    "text": "4.1 なぜ微分を計算するのか\n教師あり機械学習では、訓練集合が使えて、予測したい変数は既知である。 これが目的変数で真値である。 今予測アルゴリズムを開発し、これを入力変数、予測変数に基づいて訓練する。 この訓練あるいは学習過程は、アルゴリズムの予測と真値とを比べ、 現在の予測がどれくらいよいか悪いか捉える数値が出てくるような比較に基づいている。 この数値を与えるのは、 損失函数 の仕事だ。\n一度現在の損失が分かったら、アルゴリズムはパラメタ、つまりニューラルネットワークの重みを調整して、もっとよい予測にする。 アルゴリズムはどの方向に調整するか知る必要がある。 この情報は、 勾配 つまり微分のベクトルから得られる。\n例として次のような損失函数を想像してみる Figure 4.1。\nこれは二変数の二次函数 f(x_1, x_2) = 0.2x_1^2 + 0.2x_2^2 - 5 である。 最小値は(0,0)で、この点を求める。 白い点で示した点に立ち、風景を眺めれば、坂を速く降る方法は明確に分かる（坂を下るのを恐れないとする）。 でも、最良の方向を計算で見つけるには、勾配を計算する。\n\\(x_1\\)の方向を取り上げる。 \\(x_1\\)に関する函数の微分は、函数値が\\(x_1\\)とともにどのように変化するかを示す。 計算すると\\(\\partial f/ x_1 = 0.4x_1\\)となる。 これは\\(x_1\\)が増えると損失が増えることと、それがどの程度かを示している。 でも損失を減らす必要があるので、逆方向に進む必要がある。\n同じことが\\(x_2\\)軸に対しても成り立つ。 微分を計算すると、\\(\\partial f/\\partial x_2 = 0.4x_2\\)を得る。 再び、微分が示す向きと逆方向を選ぶ。 全体では、降下方向は \\[\n\\begin{bmatrix}\n-0.4x_1\\\\\n-0.4x_2\n\\end{bmatrix}\n\\] である。\nこの方法は最急降下と呼ばれている。 一般的に 勾配降下 と呼ばれ、機械学習で最も基本的な最適化アルゴリズムである。 おそらく直感に反して、最も効率の良い方法ではない。 さらに別の問いがある。 出発点で計算されたこの方向は降下中にずっと最適なのか。 代わりに、定期的に方向を計算し直した方が良いのかもしれない。 このような質問は後の章で検討する。",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>自動微分</span>"
    ]
  },
  {
    "objectID": "autograd.html#なぜ微分を計算するのか",
    "href": "autograd.html#なぜ微分を計算するのか",
    "title": "4  自動微分",
    "section": "",
    "text": "Figure 4.1: 仮想的な損失函数（放物面）。",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>自動微分</span>"
    ]
  },
  {
    "objectID": "autograd.html#自動微分の例",
    "href": "autograd.html#自動微分の例",
    "title": "4  自動微分",
    "section": "4.2 自動微分の例",
    "text": "4.2 自動微分の例\n微分がなぜ必要か分かったところで、自動微分（AD: automatic differentiation）がどのように計算しているか見てみよう。\n\n\n\n\n\n\nFigure 4.2: 計算グラフの例\n\n\n\nFigure 4.2 は上の函数が計算グラフにどのように表すことができるかを示している。 x1とx2は入力ノードで、対応する函数のパラメタは\\(x_1\\)と\\(x_2\\)である。 x7は函数の出力で、他は全て中間ノードであり正しい順序で実行するために必要である （定数-5、0.2及び2をノードとすることもできるが、定数なので特に気にせずに、簡潔なグラフを選んだ）。\n逆モードADは、torchが実装している自動微分の一種で、まず函数の出力を計算する。 これはグラフの順方向伝播である。 次に逆伝播を行い、両方の入力x1とx2に関する出力の勾配を計算する。 この過程で、右から情報が利用可能となり、積み重なっていく。\n\nx7で、x5とx6に関する偏微分を計算する。 つまり、微分する式は \\(f(x_5, x_6) = x_5 + x_6 - 5\\) なので、偏微分は両方とも1である。\nx5から左に動き、x3にどのように依存しているか確認すると、 \\(\\partial x_5/ \\partial x_3 = 0.2\\)である。 微積分の連鎖律を用いると、出力がどのようにx3に依存するか分かるので、\\(\\partial f/\\partial x_3 = 0.2 \\times 1 = 0.2\\)と計算できる。\nx_3から、xに最後の段階を踏む。 \\(\\partial x_3/ \\partial x_1 = 2x_1\\)なので、連鎖律を再度用いて函数が最初の入力にどのように依存するか定式化できる。 つまり \\(\\partial f/\\partial x_1 = 2x_1 \\times 0.2 \\times 0.1 = 0.4x_1\\) となる。\n同様に二番目の偏微分も計算し、勾配を求める。\\(\\nabla f = (\\partial f/\\partial x_1, \\partial f/\\partial x_2)^\\mathrm{T} = (0.4x_1, 0.4x_2)^\\mathrm{T}\\)\n\nこれが原理である。 実際には、フレームワークによって逆モード自動微分の実装は異なる。 次の節でtorchがどのように実装しているか簡潔に示す。",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>自動微分</span>"
    ]
  },
  {
    "objectID": "autograd.html#torch-autograd-による自動微分",
    "href": "autograd.html#torch-autograd-による自動微分",
    "title": "4  自動微分",
    "section": "4.3 torch autograd による自動微分",
    "text": "4.3 torch autograd による自動微分\nまず、用語について注意しておく。 torchではADエンジンは autograd と呼ばれ、本書の残りの多くの部分でもそのように記す。 それでは説明に戻る。\n上述の計算グラフをtorchで構築するには、入力テンソルx1とx2を作成する。 これは興味のあるパラメタを模している。 これまでしてきたように「いつも通り」テンソルを作成すると、torchはAD向けの準備をしない。 そうせずに、これらのテンソルを作るときにrequires_grad = TRUEを渡す必要がある。\n\nlibrary(torch)\n\nx1 &lt;- torch_tensor(2, requires_grad = TRUE)\nx2 &lt;- torch_tensor(2, requires_grad = TRUE)\n\n（ところで二つのテンソルに2という値を選んだのは完全に任意である。）\n次に「隠れた」ノードx3とx6を作るには、二乗して掛け算をする。 最後にx7に最終出力を格納する。\n\nx3 &lt;- x1$square()\nx5 &lt;- x3 * 0.2\n\nx4 &lt;- x2$square()\nx6 &lt;- x4 * 0.2\n\nx7 &lt;- x5 + x6 - 5\nx7\n\ntorch_tensor\n-3.4000\n[ CPUFloatType{1} ][ grad_fn = &lt;SubBackward1&gt; ]\n\n\nrequires_grad = TRUEを追加しなければならなかったのは、入力テンソルを作るときだけであることに注目してほしい。 グラフの依存するノードは全てこの属性を継承する。 確認してみよう。\n\nx7$requires_grad\n\n[1] TRUE\n\n\nこれまでに自動微分が動作するために必要な前提が全て満たされた。 あとはbackward()を呼べぱ、x7が’x1とx2`にどのように依存するかが決まる。\n\nx7$backward()\n\nこの呼び出しにより、x1とx2の$gradフィールドが埋まる。\n\nx1$grad\n\ntorch_tensor\n 0.8000\n[ CPUFloatType{1} ]\n\nx2$grad\n\ntorch_tensor\n 0.8000\n[ CPUFloatType{1} ]\n\n\nこれらは、それぞれx7のx1とx2に関する偏微分である。 上記の手計算を確認すると、どちらも0.8つまり0.4にテンソル値2及び2をかけたものになっている。\nすでに述べた、端から端の微分を積み上げるのに必要な積算過程はどうなっているのか。 積み上げられるに従って端から端までの微分を「追跡」することはできるのだろうか。 例えば、最終出力がどのようにx3に依存しているか見ることはできるだろうか。\n\nx3$grad\n\ntorch_tensor\n[ Tensor (undefined) ]\n\n\nこのフィールドは埋まっていないようである。 実は、これらを計算することは必要だが、torchは不要になったら中間集計を捨て、メモリを節約する。 しかしながら、保存するretarin_grad = TRUEを渡して保存を指示することも可能だ。\n\nx3 &lt;- x1$square()\nx3$retain_grad()\n\nx5 &lt;- x3 * 0.2\nx5$retain_grad()\n\nx4 &lt;- x2$square()\nx4$retain_grad()\n\nx6 &lt;- x4 * 0.2\nx6$retain_grad()\n\nx7 &lt;- x5 + x6 - 5\nx7$backward()\n\nそれでは、x3のgradフィールドが埋まっているか確認してみよう。\n\nx3$grad\n\ntorch_tensor\n 0.2000\n[ CPUFloatType{1} ]\n\n\nx4、x5、x6についても同様だ。\n\nx4$grad\n\ntorch_tensor\n 0.2000\n[ CPUFloatType{1} ]\n\nx5$grad\n\ntorch_tensor\n 1\n[ CPUFloatType{1} ]\n\nx6$grad\n\ntorch_tensor\n 1\n[ CPUFloatType{1} ]\n\n\nもう一つ気になることがある。 勾配の蓄積過程を「実行中の勾配」の観点から理解したが、蓄積を進めるのに必要な個々の微分はどのように計算されるのか。 例えばx3$gradが示しているのは出力が中間状態x3にどのように依存しているかであるが、ここから実際の入力ノードであるx1にどのように到達するのか。\nこの面についても、確認できる。 順伝播でtorchはすべきことを書き残しておいて、後で個々の微分を計算する。 この「レシピ」はテンソルのgrad_fnフィールドに格納される。 これがx3に対してx1への「失われたつながり」を追加する。\n\nx3$grad_fn\n\nPowBackward0\n\n\nx4、x5、x6についても同様。\n\nx4$grad_fn\n\nPowBackward0\n\nx5$grad_fn\n\nMulBackward1\n\nx6$grad_fn\n\nMulBackward1\n\n\nこれでおしまい。 この節では、torchがどのように微分を計算するかを見た上で、それをどのように行っているかの概要を示した。 ここで、自動微分を応用した最初の二つの課題に取り組む準備が整った。",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>自動微分</span>"
    ]
  },
  {
    "objectID": "optim1.html",
    "href": "optim1.html",
    "title": "5  autogradを使った函数の最小化",
    "section": "",
    "text": "5.1 最適化の古典\n最適化研究において、ローゼンブロック函数 は古典である。 この函数は二つの変数をとり、(1, 1)で最小となる。 等値線を眺めると、最小は伸びた細い谷の中にあることが分かる。\n函数の定義は次の通りだ。 aとbは自由に定めてよいパラメタだが、よく使われる値を用いる。\na &lt;- 1\nb &lt;- 5\n\nrosenbrock &lt;- function(x) {\n  x1 &lt;- x[1]\n  x2 &lt;- x[2]\n  (a - x1)^2 + b * (x2 - x1^2)^2\n}",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>*autograd*を使った函数の最小化</span>"
    ]
  },
  {
    "objectID": "optim1.html#最適化の古典",
    "href": "optim1.html#最適化の古典",
    "title": "5  autogradを使った函数の最小化",
    "section": "",
    "text": "Figure 5.1: ローゼンブロック函数",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>*autograd*を使った函数の最小化</span>"
    ]
  },
  {
    "objectID": "optim1.html#最小化を白紙から",
    "href": "optim1.html#最小化を白紙から",
    "title": "5  autogradを使った函数の最小化",
    "section": "5.2 最小化を白紙から",
    "text": "5.2 最小化を白紙から\nシナリオは次の通り。 与えられた点(x1, x2)から出発し、ローゼンブロック函数が最小になる場所を見つける。\n前の章で説明した方法に従い、現在の位置における函数の勾配を計算し、それを使って逆方向にに進む。 どれくらい遠くまで行けばよいかは分からない。 大きく進みすぎると、簡単に行き過ぎてしまう。 （等値線図を再度確認すると最小値のの西または東の急な崖に立っていると、これがすぐに生じることが分かる。）\nつまり、最良の方法は反復して進むことで、妥当な幅を取り、毎回勾配を再評価することだ。\nまとめると、最適化の手順は次のようになる。\nlibrary(torch)\n\n# これはまだ正しい手順ではない!\n\nfor (i in 1:num_iterations) {\n  \n  # 函数を呼び現在のパラメタ値を渡す。\n  value &lt;- rosenbrock(x)\n  \n  # パラメタについての勾配を計算する。\n  value$backward()\n  \n  # 手動でパラメタを更新し、勾配に比例した一部を引く。\n  # ここはまだ正しくない。\n  x$sub_(lr * x$grad)\n}\n書かれている通り、コード片は考えを示したもので、（まだ）正しくない。 また、いくつかの必要なものが欠けている。 テンソルxも変数lrやnum_iterationsも定義されていない。 まず、これらを準備しよう。 学習率lrは毎回引く勾配に比例した一部で、num_iterationsは反復する回数である。 これらは実験パラメタである。\nlr &lt;- 0.01\n\nnum_iterations &lt;- 1000\nxは最適化するパラメタ、つまり函数の入力であり、最適化の最後に可能な限り最小の函数値に近い値を与える位置であることが望まれる。 このテンソルについて函数の微分を計算するので、requires_grad = TRUEをつけて作る必要がある。\nx &lt;- torch_tensor(c(-1, 1), requires_grad = TRUE)\n初期位置(-1, 1)は任意に選択した。 さて、残っているのは最適化ループを少し修正することだ。 autograd がxについて有効化されていると、torchはこのテンソルに対して行われるすべての演算を記録する。 そのためbackward()を呼ぶたびに、すべての必要な微分を計算しようとすることになる。 しかし、勾配の一部を引くときは、微分を計算する必要はない。 torchにこれを記録しないように指示するために、with_no_grad()を囲む。\nここで説明しておかなければならないことがある。 既定でtorchはgradフィールドに格納された勾配を蓄積する。 新しい計算をするたびにgrad$zero_()を使ってゼロに消去する必要がある。\nこれらを考慮すると、パラメタの更新は次のように書ける。\nwith_no_grad({\n  x$sub_(lr * x$grad)\n  x$grad$zer_()\n})\n完成したコードは次のようになる。 ログをとる文を追加して何が起きているか分かるようにしてある。\n\nlibrary(torch)\n\nnum_iterations &lt;- 1000\n\nlr &lt;- 0.01\nx &lt;- torch_tensor(c(-1, 1), requires_grad = TRUE)\n\nfor (i in 1:num_iterations) {\n  if (i %% 100 == 0) cat(\"Iteration: \", i, \"\\n\")\n  \n  value &lt;- rosenbrock(x)\n  if (i %% 100 == 0) {\n    cat(\"Value is : \", as.numeric(value), \"\\n\")\n  }\n  \n  value$backward()\n  if (i %% 100 == 0 ) {\n    cat(\"Gradient is : \", as.matrix(x$grad), \"\\n\")\n  }\n  \n  with_no_grad({\n    x$sub_(lr * x$grad)\n    x$grad$zero_() \n  })\n}\n\nIteration:  100 \nValue is :  0.3502924 \nGradient is :  -0.667685 -0.5771312 \nIteration:  200 \nValue is :  0.07398106 \nGradient is :  -0.1603189 -0.2532476 \nIteration:  300 \nValue is :  0.02483024 \nGradient is :  -0.07679074 -0.1373911 \nIteration:  400 \nValue is :  0.009619333 \nGradient is :  -0.04347242 -0.08254051 \nIteration:  500 \nValue is :  0.003990697 \nGradient is :  -0.02652063 -0.05206227 \nIteration:  600 \nValue is :  0.001719962 \nGradient is :  -0.01683905 -0.03373682 \nIteration:  700 \nValue is :  0.0007584976 \nGradient is :  -0.01095017 -0.02221584 \nIteration:  800 \nValue is :  0.0003393509 \nGradient is :  -0.007221781 -0.01477957 \nIteration:  900 \nValue is :  0.0001532408 \nGradient is :  -0.004811743 -0.009894371 \nIteration:  1000 \nValue is :  6.962555e-05 \nGradient is :  -0.003222887 -0.006653666 \n\n\n1000回の反復後、函数値は0.0001よりも小さくなった。 対応する(x1,x2)の位置はどこになったか。\n\nx\n\ntorch_tensor\n 0.9918\n 0.9830\n[ CPUFloatType{2} ][ requires_grad = TRUE ]\n\n\nこれは真の最小(1,1)にかなり近い。 気が向いたら、学習率がどのような違いを生じるか試してみよう。 例えば、0.001と0.1をそれぞれ使ってみるとよい。\n次の章では、白紙からニューラルネットワークを構築する。 そこで最小化するのは 損失函数 、つまり回帰問題から現れる平均二乗誤差である。",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>*autograd*を使った函数の最小化</span>"
    ]
  },
  {
    "objectID": "network1.html",
    "href": "network1.html",
    "title": "6  ニューラルネットワークを白紙から",
    "section": "",
    "text": "6.1 ネットワークの考え方\n簡単にいうと、ネットワークは入力から出力への関数だ。 適切な函数が求めるものだ。\nこれを決めるには、会期として線型回帰を考えてみよう。 線型回帰がしているのは、掛け算と足し算だ。 独立変数一つ一つに対して、これに掛ける係数がある。 それから、バイアス と呼ばれる項を末尾に加える。 （二次元では、回帰係数とバイアスは回帰直線の傾きとy切片である。）\nこれらを考慮すると、乗算と加算はテンソルでできることで、テンソルはこれらの演算のためにあると言ってもいいくらいだ。 例として、入力が百個の観測からなり、特徴量がそれぞれ三つあるものを示す。\nlibrary(torch)\n\nx &lt;- torch_randn(100, 3)\nx$size()\n\n[1] 100   3\nxに掛ける特徴量ごとの係数を格納するために、特徴量の数である長さ3の列ベクトルが必要だ。 あるいは、この後すぐ行う修正に備えて、列の長さが三の行列にしてもよい。 つまり、三行を持つ行列である。 列はいくつ必要か。 単一の特徴量を予測したいとすれば、行列は大きさ3 x 1となる。\nよさそうな候補を乱数で初期化する。 テンソルはrequires_grad = TRUEをつけて作成されていることに留意する。 この変数はネットワークに学習てもらいたいパラメタを表しているからだ。\nw &lt;- torch_randn(3, 1, requires_grad = TRUE)\nバイアステンソルは大きさ1 x 1となる。\nb &lt;- torch_zeros(1, 1, requires_grad = TRUE)\nこれで、データに重みwをかけ、バイアスbを加えて「予測」を得ることができる。\ny &lt;- x$matmul(w) + b\nprint(y, n = 10)\n\ntorch_tensor\n  3.7318\n -0.1890\n -3.4294\n -1.2528\n  1.3601\n  6.2656\n  3.0182\n -3.7571\n -3.1763\n -2.3714\n... [the output was truncated (use n=-1 to disable)]\n[ CPUFloatType{100,1} ][ grad_fn = &lt;AddBackward0&gt; ]\n数学の表記では、ここでは次の函数を実装した。\n\\[\nf(\\mathbf{X}) = \\mathbf{X}\\mathbf{W} + \\mathbf{b}\n\\]\nこれのどこがニューラルネットワークなのか。",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ニューラルネットワークを白紙から</span>"
    ]
  },
  {
    "objectID": "network1.html#ネットワークの層",
    "href": "network1.html#ネットワークの層",
    "title": "6  ニューラルネットワークを白紙から",
    "section": "6.2 ネットワークの層",
    "text": "6.2 ネットワークの層\nニューラルネットワークの用語に戻れば、ここでしたのは単層のネットワーク、出力層の動作の原型である。 単層ネットワークは構築したい類とはとてもいえない。 単に線型内挿すれば済むのに、なぜネットワークを作るのか。 実際、ニューラルネットワークの特徴は、（理論的に）無数の層を連鎖できることになある。 もちろん出力層以外は「隠れ」層と呼ばれるが。深層学習フレームワークを使う上では全く 隠れ ている訳ではない。\n例えば、隠れ層が一つあるネットワークを作りたいとしよう。 その大きさ、つまり層が持つユニットの数はネットワークの力を決める重要な要因になる。 この数は作成する重み行列の大きさに反映される。 八つのユニットを持つ層には八つの列を持つ行列が必要である。\nw1 &lt;- torch_randn(3, 8, requires_grad = TRUE)\n各ユニットにはそれぞれバイアス値もある。\nb1 &lt;- torch_zeros(1, 8, requires_grad = TRUE)\n以前見たように、隠れ層は受け取った入力に重みを掛けてバイアスを加える。 つまり、上に示した函数\\(f\\)を適用する。 そして、別の函数を適用する。 この函数は隠れそうから入力を受け取り、最終出力を生成する。 結局、ここで行われているのは函数の合成である。 二番目の函数\\(g\\)を呼び出すと、全体の変換は\\(g(f(\\mathbf{X}))\\)つまり\\(g\\circ f\\)となる。\n上述の単層構造に類似した出力を\\(g\\)がするには、その重み行列は八つの列の隠れ層を取り、単一の列にしなければならない。 つまり、w2 は次のようになる。\nw2 &lt;- torch_randn(8, 1, requires_grad = TRUE)\nバイアスb2はb1のように単一の値である。\nb2 &lt;- torch_randn(1, 1, requires_grad = TRUE)\nもちろん、単一の隠れ層に止める理由はなく、仕組みを完成させたら、自由にコードをいじってほしい。 でも最初に、追加しなければならない部品がいくつかある。 その一つは、今の構造では函数を連鎖、あるいは合成しているのはよいが、これらの函数がしているのは加算と乗算であり、線型である。 しかし、ニューラルネットワークの力は通常 非線型性 にある。なぜか。",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ニューラルネットワークを白紙から</span>"
    ]
  },
  {
    "objectID": "network1.html#活性化函数",
    "href": "network1.html#活性化函数",
    "title": "6  ニューラルネットワークを白紙から",
    "section": "6.3 活性化函数",
    "text": "6.3 活性化函数\nここで、三つの層からなるネットワークがあったとして、各層では入力に重みを掛けただけだとする。 （バイアス項を考慮することで変わるものはないにもないが、例がややこしくなるだけなので、省略する。）\nこのネットワークは行列積の連鎖 \\(f(\\mathbf{X}) = ((\\mathbf{XW}_1)\\mathbf{W}_2)\\mathbf{W}_3\\) になる。 さて、この式を変形するとすべての重み行列を掛け合わせてから\\(\\mathbf{X}\\)に作用させ\\(f(\\mathbf{X}) = \\mathbf{X}(\\mathbf{W}_1\\mathbf{W}_2\\mathbf{W}_3)\\)と書くことができる。 つまり三つの層のネットワークは単層のものに簡略化され、\\(f(\\mathbf{X}) = \\mathbf{XW}_4\\)となる。 こうなると、深層ニューラルネットワークの利点がすべて失われてしまう。\nここで活性化函数、時に「非線型性」が登場する。 これは非線型演算を導入するもので、行列演算でモデル化することはできない。 歴史的には典型的な活性化函数は シグモイド であり、現在でも極めて重要である。 その本質的な作用は入力を0と1との間に圧縮して確率と解釈できる量を与える。 回帰では、これが求めるものではなく、ほとんどの隠れ層についても同様だ。\n代わりに、ネットワークの中で最も利用されている活性化函数は ReLU、Rectified Linear Unit（正規化線型函数）と呼ばれるものだ。 名前は長いが単に全ての負の値を0するだけだ。 torchではrelu()でこれができる。\n\nt &lt;- torch_tensor(c(-2, 1, 5, -7))\n\nなぜこれが非線型なのか。 線型である規準の一つは、二つの入力に対し先に加えてから変換しても、先に個々の入力を変換してから加えても結果が同じになることだ。 ReLUではそのようにはならない。\n\nt1 &lt;- torch_tensor(c(1, 2, 3))\nt2 &lt;- torch_tensor(c(1, -2, 3))\n\nt1$add(t2)$relu()\n\ntorch_tensor\n 2\n 0\n 6\n[ CPUFloatType{3} ]\n\n\n\nt1_clamped &lt;- t1$relu()\nt2_clamped &lt;- t2$relu()\n\nt1_clamped$add(t2_clamped)\n\ntorch_tensor\n 2\n 2\n 6\n[ CPUFloatType{3} ]\n\n\n結果は同じではない。\nこれまでをまとめると、層を重ねて活性化函数を作用されることを説明した。 あと一つ概念を示してからネットワークを完成される。 その概念とは損失函数だ。",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ニューラルネットワークを白紙から</span>"
    ]
  },
  {
    "objectID": "network1.html#損失函数",
    "href": "network1.html#損失函数",
    "title": "6  ニューラルネットワークを白紙から",
    "section": "6.4 損失函数",
    "text": "6.4 損失函数\n抽象的にいえば、損失とは目的からどれくらい離れているかという尺度だ。 函数を最小化するとき、前の章で行ったように、これは現在の函数値と取りうる最小の値との差である。 ニューラルネットワークでは、目的に適合している限り損失函数を自由に選ぶことができる。 回帰問題では、平均二乗誤差（MSE: mean square error）がよく用いられるが、それには限らない。 平均絶対誤差を代わりに用いるべきこともあるだろう。\ntorchでは、平均二乗誤差は一行で書ける。\n\ny &lt;- torch_randn(5)\ny_pred &lt;- y + 0.01\n\nloss &lt;- (y_pred - y)$pow(2)$mean()\n\nloss\n\ntorch_tensor\n9.99998e-05\n[ CPUFloatType{} ]\n\n\n損失函数が決まれば、重みの更新は勾配に比例した一部を引くことでできる。 やり方は既に前の章で見たが、すぐにまた行うことになる。\nそれでは説明した部品を集めて組み立てよう。",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ニューラルネットワークを白紙から</span>"
    ]
  },
  {
    "objectID": "network1.html#ネットワークの実装",
    "href": "network1.html#ネットワークの実装",
    "title": "6  ニューラルネットワークを白紙から",
    "section": "6.5 ネットワークの実装",
    "text": "6.5 ネットワークの実装\n実装は三つの部分に分かれる。 こうしておくと、後でtorchの高レベルの機能を利用してそれぞれの部分を改訂する際に、カプセル化やモジュール化が行われる部分が分かりやすい。\n\n6.5.1 ランダムデータの生成\n例として使うデータは百個の観測からなる。 入力xは三つの特徴量を持つ。 目的yは一つの値で、yはxから生成されるが、ノイズが加えられる。\n\nlibrary(torch)\n\n# 入力次元（入力の特徴量の数）\nd_in &lt;- 3\n# 訓練集合の観測数\nn &lt;- 100\n\nx &lt;- torch_randn(n, d_in)\ncoefs &lt;- c(0.2, -1.3, -0.5)\ny &lt;- x$matmul(coefs)$unsqueeze(2) + torch_randn(n, 1)\n\n\n\n6.5.2 ネットワークの構築\nネットワークは隠れ層と出力層の二層とする。 つまり二つの重み行列と二つのバイアステンソルが必要である。 特に理由はないが、隠れ層には三十二ユニット配置する。\n\n# 隠れ層の次元\nd_hidden &lt;- 32\n# 出力次元（予測される特徴量の数）\nd_out &lt;- 1\n\n# 入力を隠れ層に結合する重み\nw1 &lt;- torch_randn(d_in, d_hidden, requires_grad = TRUE)\n# 隠れ層を出力に結合する重み\nw2 &lt;- torch_randn(d_hidden, d_out, requires_grad = TRUE)\n\n# 隠れ層のバイアス\nb1 &lt;- torch_zeros(1, d_hidden, requires_grad = TRUE)\n# 出力層のバイアス\nb2 &lt;- torch_zeros(1, d_out, requires_grad = TRUE)\n\n現在の値、乱数初期化の結果は、重みとバイアスは有用ではない。 ネットワークを訓練する時が来た。\n\n\n6.5.3 ネットワークの訓練\nネットワークの訓練は入力を層に伝播させ、損失を計算し、パラメタ（重みとバイアス）を調整して、予測を向上するようにする事である。 性能が十分（実際の応用では、非常に注意深く定義する必要がある）と思われるまで、これらの計算を繰り返す。 技術的には、これらの手順を反復して適用する各回を エポック と呼ぶ。\n函数の最小化のように、適切な学習率（差し引く勾配の比率）は実験的によって決める。\n以下に示す訓練ループを見ると、必然的に四つの部分に分かれることが分かる。\n\n順伝播してネットワークの予測を得る（一行に書くのが好みでないなら、分けて書いてもよい）。\n損失を計算する（これも一行で、いくらかのログ出力を追加しただけだ）。\nautograd に損失のパラメタに対する勾配を計算させる。\nパラメタを適切に更新する（ここでも全ての演算をwith_no_grad()で囲み、gradフィールドを反復ごとにゼロにしている）。\n\n\nlearning_rate &lt;- 1e-4\n\n### 訓練ループ----------------------------------------\n\nfor (t in 1:200) {\n  \n  ### ------- 順伝播 -------\n  \n  y_pred &lt;- x$mm(w1)$add(b1)$relu()$mm(w2)$add(b2)\n  \n  ### ------- 損失の計算 -------\n  loss &lt;- (y_pred - y)$pow(2)$mean()\n  if (t %% 10 == 0)\n    cat(\"Epoch: \", t, \"   Loss: \", loss$item(), \"\\n\")\n  \n  ### ------- 逆伝播 -------\n  \n  # `requires_grad = TRUE がついた\n  # 全てのデンソルについて損失の勾配を計算\n  loss$backward()\n  \n  ### ------- 重みの更新 -------\n  \n  # この部分は自動勾配計算のために記録したくない部分なので\n  # with_no_grad()で囲む。\n  \n  with_no_grad({\n    w1 &lt;- w1$sub_(learning_rate * w1$grad)\n    w2 &lt;- w2$sub_(learning_rate * w2$grad)\n    b1 &lt;- b1$sub_(learning_rate * b1$grad)\n    b2 &lt;- b2$sub_(learning_rate * b2$grad)\n    \n    # 伝播ごとにゼロにする。\n    # そうしないと蓄積してしまう。\n    w1$grad$zero_()\n    w2$grad$zero_()\n    b1$grad$zero_()\n    b2$grad$zero_()\n  })\n\n}\n\nEpoch:  10    Loss:  70.71572 \nEpoch:  20    Loss:  63.92153 \nEpoch:  30    Loss:  57.88784 \nEpoch:  40    Loss:  52.52216 \nEpoch:  50    Loss:  47.75594 \nEpoch:  60    Loss:  43.49489 \nEpoch:  70    Loss:  39.67999 \nEpoch:  80    Loss:  36.25851 \nEpoch:  90    Loss:  33.1709 \nEpoch:  100    Loss:  30.38891 \nEpoch:  110    Loss:  27.88091 \nEpoch:  120    Loss:  25.61374 \nEpoch:  130    Loss:  23.56756 \nEpoch:  140    Loss:  21.7197 \nEpoch:  150    Loss:  20.04354 \nEpoch:  160    Loss:  18.5186 \nEpoch:  170    Loss:  17.13169 \nEpoch:  180    Loss:  15.8768 \nEpoch:  190    Loss:  14.73451 \nEpoch:  200    Loss:  13.69401 \n\n\n損失は最初急速に減少し、その後それほど速く減少なくなる。 この例はすばらしい性能を示すために作られたのでなく、わずかな行のコードで「本物の」ニューラルネットワークが構築できることを示すことが意図である。\n層や、損失、パラメタ更新はまだかなら粗削りで、（文字通り）単なるテンソルである。 このような小さなネットワークには問題だが、より複雑な設計ではすぐに面倒になる。 以下の二つの章では、重みとバイアスをネットワークに抽象化し、自作の損失函数を組込のものに取り替え、冗長なパラメタ更新部分を取り除く。",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ニューラルネットワークを白紙から</span>"
    ]
  },
  {
    "objectID": "modules.html",
    "href": "modules.html",
    "title": "7  モジュール",
    "section": "",
    "text": "7.1 組込のnn_moudle()\ntorchでは、線型層はnn_linear()で作る。 nn_linear()は、in_featuresとout_featuresの（最低）二つの引数が必要である。 入力データに観測が50あり、それぞれ五つの特徴量がある場合、大きさは50 x 5となる。 潜在層には、16のユニットを置く。 この場合、in_featuresは5、out_featuresは16である。 （自分で重み行列を作る場合、同じ5と16が行と列の数である。）\nlibrary(torch)\nl &lt;- nn_linear(in_features = 5, out_features = 16)\n一度作られれば、モジュールのパラメタの情報は簡単に得られる。\nl\n\nAn `nn_module` containing 96 parameters.\n\n── Parameters ──────────────────────────────────────────────────────────────────\n• weight: Float [1:16, 1:5]\n• bias: Float [1:16]\nカプセル化されていても、重みとバイアステンソルを確認することができる。\nl$weight\n\ntorch_tensor\n 0.0069  0.1187  0.1180 -0.2051 -0.1429\n-0.0322  0.4077  0.1930 -0.2636 -0.1679\n-0.0833 -0.0067 -0.1784 -0.0671 -0.3823\n 0.0963 -0.0906  0.3179  0.1573 -0.1087\n 0.4087  0.1525 -0.3092 -0.1145  0.1441\n-0.1689 -0.1969  0.4418  0.1826  0.2059\n 0.1147  0.4426  0.0702  0.4239 -0.2637\n-0.3778  0.4017  0.3898 -0.0727 -0.2514\n 0.0679 -0.0832  0.1479 -0.3340 -0.4074\n-0.0141 -0.0657  0.0268 -0.1415 -0.0273\n 0.4142 -0.4185  0.3092 -0.4252  0.1825\n 0.4440  0.2890 -0.0818  0.2971 -0.2486\n-0.1513 -0.1589  0.3036 -0.3993  0.0477\n 0.3389  0.0092 -0.0379  0.4024  0.1427\n 0.2206 -0.2165 -0.0490 -0.2519 -0.0862\n-0.2025  0.3549 -0.2445 -0.2580  0.1618\n[ CPUFloatType{16,5} ][ requires_grad = TRUE ]\nl$bias\n\ntorch_tensor\n 0.1953\n-0.4240\n-0.3092\n 0.3073\n-0.2308\n 0.3445\n-0.2624\n 0.3072\n-0.0770\n 0.0082\n 0.4355\n-0.2723\n-0.1773\n-0.0696\n 0.1013\n 0.0307\n[ CPUFloatType{16} ][ requires_grad = TRUE ]\nこの時点で、ちょっと立ち止まってほしい。 torchが返した重み行列の大きさは16 x 5で、白紙からコードを書いたときの5 x 16ではない。 これは基礎となるC++実装libtorchに由来する。 性能のため、libtorchの線型モジュールは重みとバイアスを転置で格納している。 Rからは、この点を指摘しておくことくらいしかできない。 混乱が軽減されることを望む。\n続けよう。 このモジュールにデータを適用するには函数のように「呼び出す」だけだ。\nx &lt;- torch_randn(50, 5)\noutput &lt;- l(x)\noutput$size()\n\n[1] 50 16\nこれが順伝播だ。 勾配計算はどうするのか。 以前は、勾配計算の「入力」として必要なテンソルを作るとき、torchに明示的にrequires_grad = TRUEを渡す必要があった。 組込nn_module()ではその必要はない。 すぐにoutputがbackward()でどのような計算をすべきか調べることができる。\noutput$grad_fn\n\nAddmmBackward0\n確認のため、outputに基づいて、適当な損失を計算して、backward()を読んでみよう。 線型モジュールのweightテンソルには埋められたgradフィールドがあることが分かる。\nloss &lt;- output$mean()\nloss$backward()\nl$weight$grad\n\ntorch_tensor\n0.001 *\n 2.0974 -7.3801 -16.3910  0.9399  2.4050\n  2.0974 -7.3801 -16.3910  0.9399  2.4050\n  2.0974 -7.3801 -16.3910  0.9399  2.4050\n  2.0974 -7.3801 -16.3910  0.9399  2.4050\n  2.0974 -7.3801 -16.3910  0.9399  2.4050\n  2.0974 -7.3801 -16.3910  0.9399  2.4050\n  2.0974 -7.3801 -16.3910  0.9399  2.4050\n  2.0974 -7.3801 -16.3910  0.9399  2.4050\n  2.0974 -7.3801 -16.3910  0.9399  2.4050\n  2.0974 -7.3801 -16.3910  0.9399  2.4050\n  2.0974 -7.3801 -16.3910  0.9399  2.4050\n  2.0974 -7.3801 -16.3910  0.9399  2.4050\n  2.0974 -7.3801 -16.3910  0.9399  2.4050\n  2.0974 -7.3801 -16.3910  0.9399  2.4050\n  2.0974 -7.3801 -16.3910  0.9399  2.4050\n  2.0974 -7.3801 -16.3910  0.9399  2.4050\n[ CPUFloatType{16,5} ]\nつまり、nn_moduleを使うと、torchは自動的に勾配計算が必要だとみなされる。\nnn_linearは簡単に見えるが、ほとんど全てのモデル構成において必須の要素だ。",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>モジュール</span>"
    ]
  },
  {
    "objectID": "modules.html#組込のnn_moudle",
    "href": "modules.html#組込のnn_moudle",
    "title": "7  モジュール",
    "section": "",
    "text": "nn_conv1d()、nn_conv2d()、およびnn_conv3d()、いわゆる畳み込み層は入力要素に対するフィルタを異なる次元で適用する。\nnn_lstm()とnn_gru()は状態を引き継ぐ再帰層。\nnn_embedding()は高次元の質的データにつ分かれる。\nその他多数",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>モジュール</span>"
    ]
  },
  {
    "objectID": "modules.html#モデルの構築",
    "href": "modules.html#モデルの構築",
    "title": "7  モジュール",
    "section": "7.2 モデルの構築",
    "text": "7.2 モデルの構築\n組込のnn_module()は普通の呼び方では層だが、どのようにモデルに組み上げるのか。 「工場函数」nn_module()を使って、モデルを任意の複雑さで定義できる。 でも、必ずしもそうする必要はない。\n\n7.2.1 層の列としてのモデル: nn_sequential()\nモデルが単に層を伝播するだけであれば、モデルを作るときにnn_sequential()が使える。 線型層だけからなるモデルは多層パーセプトロン（MPL: Multi-Layer Perceptron）として知られている。\n\nmlp &lt;- nn_sequential(\n  nn_linear(10, 32),\n  nn_relu(),\n  nn_linear(32, 64),\n  nn_relu(),\n  nn_linear(64, 1)\n)\n\n関係する層を詳しく見てみよう。 ReLU活性化を実装した函数をであった。 （nnf_のfは汎函数であることを示す。） 以下はnn_reluはnn_linear()と同様でモジュール、つまりオブジェクトで、引数は全てモジュールでなければならない。\n組込モジュール同様、このモデルをデータに適用するには呼び出せばよい。\n\nmlp(torch_randn(5, 10))\n\ntorch_tensor\n 0.2209\n 0.2230\n 0.0965\n 0.0728\n 0.1378\n[ CPUFloatType{5,1} ][ grad_fn = &lt;AddmmBackward0&gt; ]\n\n\n一回の呼び出しによりネットワークを通じた順伝播が始動した。 同様に、backward()を呼び出せ全ての層を通じて逆伝播される。\n\n\n7.2.2 独自処理のモデル\n既に暗示されているように、ここがnn_moduleの使いどころだ。\nnn_module|()は独自に作成されたR6オブジェクトに対するコンストラクタを作る。 以下、my_linear()はそのようなコンストラクタだ。 呼び出させれると、組込のnn_linear()に似た線型モジュールを返す。\nコンストラクタの定義の中で、二つのメソッドが実装されなければならない。initialize()とforward()である。 initialize()はモジュールのオブジェクトフィールドを作る。 すなわちそれが持つオブジェクトまたは値でどのメソッドの中からも見える。 forward()はモジュールが入力があったときの動作を定義する。\n\nmy_linear &lt;- nn_module(\n  initialize = function(in_features, out_features) {\n    self$w &lt;- nn_parameter(torch_randn(\n      in_features, out_features\n    ))\n    self$b &lt;- nn_parameter(torch_zeros(out_features))\n  },\n  forward = function(input) {\n    input$mm(self$w) + self$b\n  }\n)\n\nnn_parameter()の使い方を見てほしい。 nn_parameter()は渡されたテンソルかモジュールの パラメタ として登録されていることを確認する、つまり逆伝播されるのが既定だ。\n新たに定義されたモジュールのインスタンスを作るには、コンストラクタを呼び出す。\n\nl &lt;- my_linear(7, 1)\nl\n\nAn `nn_module` containing 8 parameters.\n\n── Parameters ──────────────────────────────────────────────────────────────────\n• w: Float [1:7, 1:1]\n• b: Float [1:1]\n\n\nこの例には、モジュールを定義するために必要な独自の計算手順はないが、ここではどんな利用形態にも適用できる雛型である。 後に、より複雑なinitialize()とforward()を検討したが、モジュールに定義される追加のメソッドを示す。 基本的な仕組みは一緒だ。\nここでは、前の章でモジュールを使ったニューラルネットワークを書き換えることができると感じるかもしれない。 自由に取り組んでもよいし、最適化手法と組込損失函数を学ぶ次の章まで待ってもよい。 それが済んだら、函数最小化と回帰ネットワークの二つの例に戻ることができる。 そして、torchにより自作した余計なものを取り除くことができる。",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>モジュール</span>"
    ]
  },
  {
    "objectID": "optimizers.html",
    "href": "optimizers.html",
    "title": "8  最適化器",
    "section": "",
    "text": "8.1 何のための最適化器か\nこの質問には、主に二つの答えがある。 一つは技術的なものだ。 最初のニューラルネットワークのコードをどのように書いたか思い出すと、次のように進めていたことが分かる。\n最後の部分を再掲する。\nこの例は小さなネットワークだったが、このような計算手順を数十、数百層を持つ構造についてコードを書かなければならないとしたら大変だ。 当然、それは深層学習フレームワークの開発者がユーザにしてほしいことではない。 だから、重みの更新は専用のオブジェクト、当該の最適化器が担当する。\nつまり、技術面の答えは使い勝手と利便性に関係する。 しかし、他のことも関係している。 以上の方法では、良好な学習率は試行錯誤により決めるしかない。 そしておそらく、最適な学習率が訓練過程を通じて一定であることもない。 ありがたいことに、これまでのたくさんの研究で確立した更新方法がいくつも存在する。 これらの方法は、通常演算の間の状態に依存する。 これが、torchにおいて最適化器がモジュールのようにオブジェクトである、もう一つの理由だ。\nこれらの方法を詳しく見る前に、手動の重み更新過程を最適化器を使った版に置き換える方法を示す。",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>最適化器</span>"
    ]
  },
  {
    "objectID": "optimizers.html#何のための最適化器か",
    "href": "optimizers.html#何のための最適化器か",
    "title": "8  最適化器",
    "section": "",
    "text": "予測（順伝播）を計算し、\n損失を算出し、\nautograd を使って（loss$backward()を呼びだし）偏微分を計算して、\nパラメタを更新するため、勾配の一定の割合をそれぞれ引く。\n\n\nlibrary(torch)\n\n# requires_grad = TRUEがついた\n# 全てのテンソルに関して損失の勾配を計算\nloss$backward()\n\n### -------- 重みを更新 --------\n\n# この部分は自動勾配計算に記録したくないので、\n# `with_no_grad()`で包む。\nwith_no_grad({\n  w1 = w1$sub_(learning_rate * w1$grad)\n  w2 = w2$sub_(learning_rate * w2$grad)\n  b1 = b1$sub_(learning_rate * b1$grad)\n  b2 = b2$sub_(learning_rate * b2$grad)\n  \n  # 毎回勾配を0にする。\n  # さもないと積み上がってしまう。\n  w1$grad$zero_()\n  w2$grad$zero_()\n  b1$grad$zero_()\n  b2$grad$zero_()\n})",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>最適化器</span>"
    ]
  },
  {
    "objectID": "optimizers.html#torch組込最適化器の利用",
    "href": "optimizers.html#torch組込最適化器の利用",
    "title": "8  最適化器",
    "section": "8.2 torch組込最適化器の利用",
    "text": "8.2 torch組込最適化器の利用\n最適化器は何を最適化するか知る必要がある。 ニューラルネットワークの文脈では、それはネットワークのパラメタである。 とは言っても、「モデルのモジュール」と「層のモジュール」に実質的な差はないので、どのように動作するかをnn_linearのような単一の組込モジュールで説明する。\nまず勾配降下最適化器をある線型モジュールのパラメタに対して働くように作る。\n\nlibrary(torch)\n\nl &lt;- nn_linear(10, 2)\n\nopt &lt;- optim_sgd(l$parameters, lr = 0.1)\n\nいつも必要であるを最適化対象のテンソルへの参照に加えて、optim_sgd()は一つだけ任意でないパラメタ学習率lrがある。\n一度最適化器オブジェクトを作れば、パラメタの更新はstep()を呼び出すことで実行される。 でも変わってないことが一つある。 訓練の反復に亙って勾配が積み上がらないようにしなければならない。 つまり、zero_grad()を呼び出す必要があるが、今度は最適化器オブジェクトに対して呼べばよい。\n以下に示す完全なコードは、上述の手動の手順を置き換えたものだ。\n# requires_grad = TRUEがついた\n# 全てのテンソルに関して損失の勾配を計算\n# ここは変化なし\nloss$bacward()\n\n# 依然逆伝播の前に勾配を0にする必要がある。\n# 今度は最適化器オブジェクトに対してする。\noptimizer$zero_grad()\n\n# 最適化器を使ってモデルパラメタを更新\noptimizer$step\n使いやすくなったことは納得してもらえると思う。 これはすごい改善だ。 ここで、元の質問「何のための最適化器か」に戻り、二つ目の手法面での答えについてもっと議論する。",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>最適化器</span>"
    ]
  },
  {
    "objectID": "optimizers.html#パラメタの更新手法",
    "href": "optimizers.html#パラメタの更新手法",
    "title": "8  最適化器",
    "section": "8.3 パラメタの更新手法",
    "text": "8.3 パラメタの更新手法\nよい学習率を試行錯誤で探すのは大変だ。 それに学習率だけがはっきりしないものではない。 学習率が指定するのは、歩幅の大きさだけだ。 だが、それだけが未解決の問題ではない。\nこれまで、勾配が与える最急降下方向が最良の方向だと仮定してきた。 それはいつも正しいわけではない。 そのため、パラメタ更新の大きさと方向の両方とも不明ということになる。\nありがたいことに、ここ十年間でニューラルネットワークにおける重みの更新に関する研究が大きく進展した。 ここでは、関係する主要な問題について述べ、torchにより提供されている最もよく使われる最適化器のいくつかを位置付ける。\n比較対象となる基準は 勾配降下 あるいは 最急降下 で、このアルゴリズムを函数最適化やニューラルネットワークの訓練を手動で実装するときに用いてきた。 簡単にその指導原理をおさらいしよう。\n\n8.3.1 勾配降下法\n最急降下や確率的勾配降下法（SGD: stochastic gradient descent）としても知られている。 勾配は偏微分のベクトルで、各入力特徴量に対して一つの要素がある。 これは、関数が最も増加する方向を表す。 その反対方向に進めば、最も速く降下できる。 そうだろうか。\n残念ながら、そう簡単ではない。 取り囲む地形、より技術的には最小化したい函数の等値線に依存する。 説明のため、二つの状況を考える。\n一つは初めて自動微分を学んだときに出てきたものだ。 その例では二次元の二次函数があった。 そのときは指摘しなかったが、この特定の函数の重要な点は勾配が二つの次元とも同じだった。 そのような条件では、最急降下は最適である。\n確認しよう。 函数は\\(f(x_1,x_2) = 0.2x_1^2 + 0.2x_2^2-5\\)で勾配は\\(\\begin{bmatrix}0.4\\\\0.4\\end{bmatrix}\\)だった。\n例えば点\\((x_1,x_2) = (6, 6)\\)にいるとする。 それぞれの座標軸について、現在の値の0.4倍を引く。 これは学習率1の場合だが、その必要はない。 もし学習率2.5を使えば、一歩で最小に到達できる。 \\((x_1,x_2) = (6 - 2.5 * 0.4 * 6, 6 - 2.5 * 0.4 * 6) = (0, 0)\\). 次の図を見ると、それぞれの場合に何が起きているか分かる。 \nまとめると、このような等方的な函数は二つの方向の分散が等しいので、学習率を正しく決める「だけ」の問題になる。\n次に両方の方向の傾きが大きく異なるとどうなるか、これと比較してみる。\n今度は\\(x_2\\)の係数が\\(x_1\\)の十倍大きく、\\(f(x_1,x_2) = 0.2x_1^2 + 2x_2^2 - 5\\)である。 つまり\\(x_2\\)方向に進むと函数値は大きくなるが、\\(x_1\\)方向にはもっとゆっくり上昇する。 すなわち、勾配降下の間、一つの方向が他の方向よりも大きく進む。\nまた異なる学習率を使うとどうなるか調べる。 以下では三つの異なる設定を比較する。 最も小さい学習率を使うと、最終的に最小に到達するものの、対称な場合よりもかなり遅い。 わずかに大きな学習率を使うと、ジグザクを繰り返して、より影響の大きい\\(x_2\\)が正と負の値の間で振動する。 最後に、学習率をもう少しだけ大きくすることは最悪の効果をもたらす。 函数値は発散し、ジグザクに無限大に大きくなる。\n\n\n\nわずかに異なる学習率を用いた非等方的な放物面上の最急降下\n\n\nこれは非常に説得力がある。 二変数だけのよくある函数でも最急降下は万能からは程遠い。 そして深層学習では、損失函数は もっと 質が悪い。 ここがより洗練されたアルゴリズムが必要になるところである。 最適化器に再度登場願う。\n\n\n8.3.2 問題となること\n概念から見ると、最急降下法の主な改良は、駆動の考え方、つまり解こうとしている問題により分類できる。\nまず、毎回勾配を再計算する度に、全く新しい方向から始める代わりに、古い方向、技術的には慣性を残したい。 これは上の例で見た非効率なジグザクの防止に役立つはずである。\n次に非対称函数の最小化の例では、本当に全ての変数に対して同じ学習率を使わなければならないのか。 明らかに全ての変数が同程度に変化しないことが明らかであるとき、それらを個別に適切な方法で更新したらよいのではないか。\n三つ目は、過度に影響が大きい特徴量に対して学習率を下げようとした場合にだけ生じる問題に対する修正で、学習が進み、パラメタが更新されることを確実にしたい。\nこれらの考慮すべき点は、最適化アルゴリズムの中でいくつかの古典に示されている。\n\n\n8.3.3 軌道に留まる: 慣性付勾配降下法\n慣性付勾配降下法では、勾配を 直接 重みの更新には使用しない。 代わりに、軌道上の粒子として重みの更新を考える。 粒子は進んでいる方向を維持しようとする、物理で言う慣性が働くが、衝突により向きが変化する。 この「衝突」により 現在 の位置での勾配を考慮するように突かれる。 これらの力学から二段階の更新手順が得られる。\n以下の式で、記号の選択は物理の類比によるものだ。 \\(\\mathbf{x}\\)は位置で、パラメタ空間で現在いるところ、より簡単には 現在のパラメタ値である。 時間変化は上付添字で表し、\\(\\mathbf{y}^{(k)}\\)は変数\\(\\mathbf{y}\\)の現在時刻\\(k\\)での状態である。 時刻\\(k\\)における瞬間速度は勾配\\(\\mathbf{g}^{(k)}\\)で測る。 位置の更新にはこれを直接用いない。 代わりに各反復で、更新された速度は古い速度に慣性パラメタ\\(m\\)で重み付された値と新たに計算した勾配（学習率で重み付けする）の組み合わせとする。 二段階の最初はこの方法を表している。\n\\[\n\\mathbf{v}^{(k+1)} = m\\mathbf{v}^{(k)} + lr\\mathbf{g}^{(k)}\n\\tag{8.1}\\]\n二段階目は\\(\\mathbf{x}\\)の更新を「折衷」された速度でする。\n\\[\n\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} - \\mathbf{v}^{(k+1)}\n\\tag{8.2}\\]\n物理の類比以外にも、有用かもしれないのは、時系列解析で有名な概念だ。 \\(m\\)と\\(lr\\)の和が1であるように選べば、結果は 指数函数的加重移動平均 となる。 （この概念の適用は理解の助けにはなるが、実際は\\(m\\)と\\(lr\\)との和を1にする必要はない。）\nでは、非等方的な放物面に戻り、慣性なしとありのSGDを比較しよう。 後者（明るい曲線）では、\\(lr = 0.5\\)と\\(m = 0.1\\)を用いた。 SGD（暗い曲線）で学習率は上の「よい値」を用いた。 慣性付SDGはかなり少ない歩数で最小に到達するFigure 8.1。\n\n\n\n\n\n\nFigure 8.1: 慣性付SGD（白）と素のSGD（灰）との比較\n\n\n\n\n\n8.3.4 Adagrad\nまだ改善できるだろうか。 用いている例では、一つの特徴量がもう一つよりもかなり速く変化することとが最適化を遅くしている。 異なる学習率をパラメタ毎に使うべきなのは明白だ。 実際、深層学習でよく使われる最適化手法はパラメタ毎に学習率を変えている。 でもどのように決めるのか。\nこの点にアルゴリズムの違いが現れる。 例えば、Adagradは個々のパラメタの更新をその偏微分の（正確には二乗）積算和で割っている。 ここで、「積算」は最初の反復から積み上げる。 「積算変数」\\(s\\)の対象とするパラメタ\\(i\\)、反復回\\(k\\)の値は、次の式で更新する。\n\\[\n\\mathbf{s}_i^{(k)} = \\sum_{j=1}^k(g_i^{(j)})^2\n\\tag{8.3}\\]\n（ところで、数式が苦手なら読み飛ばして構わない。できるだけ言葉で伝えるようにしているので、基本的な情報を失うことはない。）\nここで、個々の変数に対する更新では、勾配の一部割合を引くのは素の最急降下と同じだが、割合は（大域的な）学習率だけでなく、前述の二乗した偏微分の積算和も用いて決める。 その和が大きければ、つまり訓練中の勾配が大きければ大きいほど、調整は小さくなる。1\n\\[\nx_i^{(k+1)} = x_i^{(k)} - \\frac{lr}{\\epsilon + \\sqrt{s_i^{(k)}}}g_i^{(k)}\n\\tag{8.4}\\]\nこの手法の全体的な効果は、パラメタの勾配が一貫して大きいと影響は抑制される。 ずっと勾配が小さいパラメタが変化すると、十分に考慮される。 このアルゴリズムでは、大域的な学習率\\(lr\\)の重要性は低い。 現在の例で最もよい結果に対して、非常に大きな学習率3.7を使える（使う必要がある）。 今回も素の勾配降下（灰の曲線）と比較した結果をFigure 8.2に示す。\n\n\n\n\n\n\nFigure 8.2: Adagrad（白）と素のSGD（灰）との比較\n\n\n\nこの例では、Adagradは非常によい性能を示す。 でもニューラルネットワークの訓練では反復をたくさん行う。 その場合、勾配が積算されていくので、実質的な学習率は次第に減少し、行き止まりに達する。\n他の方法で学習率を個別、パラメタ毎にできないだろうか。\n\n\n8.3.5 RMSProp\nRMSPropはAdagradの積算勾配の方法を重み付き平均に置き換える。 各点で、「簿記」されたパラメタ毎の変数\\(s_i\\)は前の値と前の（二乗）勾配の加重平均とする。\n\\[\ns_i^{(k+1)} = \\gamma s_i^{(k)} + (1 - \\gamma) (g_i^{(k)})^2\n\\tag{8.5}\\]\n更新はAdagradと同様にする。\n\\[\nx_i^{(k+1)} = x_i^{(k)} - \\frac{lr}{\\epsilon + \\sqrt{s_i^{(k)}}}g_i^{(k)}\n\\tag{8.6}\\]\nこの方法では、各パラメタは適切な重みが得られ、全体として学習が遅くなることがない。\n基準であるSGDとの比較を示すFigure 8.3。\n\n\n\n\n\n\nFigure 8.3: RMSProp（白）と素のSGD（灰）との比較\n\n\n\n現時点で、RMSPropは次に述べるAdamに次いで深層学習で最もよく使わている手法である。\n\n\n8.3.6 Adam\nAdamはこれまでに見た二つの概念を組み合わせている。 慣性で「軌道」に留まり、パラメタ依存の更新で速く変化するパラメタへの過剰な依存を回避している。 Adamの手順は次の通りである。2\nまず、慣性付SGDのように、勾配の指数函数的加重平均を維持する。 ここでは加重係数\\(\\gamma_v\\)は通常0.9に設定される。\n\\[\nv_i^{(k+1)} = \\gamma_vv_i^{(k)} + (1 - \\gamma_v)g_i^{(k)}\n\\tag{8.7}\\]\nまた、RSMPropのように二乗勾配の指数函数的加重平均を求め、加重係数\\(\\gamma_s\\)は通常0.999を使う。 \\[\ns_i^{(k+1)} = \\gamma_ss_i^{(k)} + (1 - \\gamma_s)(g_i^{(k)})^2\n\\tag{8.8}\\]\n\\[\nx_i^{(k+1)} = x_i^{(k)} - \\frac{lrv_i^{(k+1)}}{\\epsilon + \\sqrt{s_i^{(k+1)}}}g_i^{(k)}\n\\tag{8.9}\\]\nこの章の締めくくりに、Adamを同じ例で試してみるFigure 8.4。\n\n\n\n\n\n\nFigure 8.4: Adam（白）と素のSGD（灰）\n\n\n\n次の章で扱う損失函数は、取り上げる最後の構成要素で、回帰ネットワークと最小化の例を書き直し、torchのモジュールと最適化器の恩恵を受ける。",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>最適化器</span>"
    ]
  },
  {
    "objectID": "optimizers.html#footnotes",
    "href": "optimizers.html#footnotes",
    "title": "8  最適化器",
    "section": "",
    "text": "ここで\\(\\epsilon\\)は小さな値で0で割ることを防止する。↩︎\n実装は通常追加の手順があるが、ここではその詳細は必要ない。↩︎",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>最適化器</span>"
    ]
  },
  {
    "objectID": "loss-functions.html",
    "href": "loss-functions.html",
    "title": "9  損失函数",
    "section": "",
    "text": "9.1 torchの損失函数\ntorchでは、損失函数はnn_またはnnf_で始まる。\nnnf_を使う場合、直接函数呼び出しを行う。 これに対応して、その（推定と目的の）引数はどちらもテンソルだ。 例えば、nnf_mse_loss()という組込函数は手作業でコードを書いたものに類似している。\nlibrary(torch)\nnnf_mse_loss(torch_ones(2, 2), torch_zeros(2, 2) + 0.1)\n\ntorch_tensor\n0.81\n[ CPUFloatType{} ]\n一方nn_では、まずオブジェクトを作成する。\nl &lt;- nn_mse_loss()\nこのオブジェクトはテンソルに対して呼び出すと、求める損失が得られる。\nl(torch_ones(2, 2), torch_zeros(2, 2) + 0.1)\n\ntorch_tensor\n0.81\n[ CPUFloatType{} ]\nオブジェクトまたは函数のどちらを選択するかは、主に好みと文脈次第だ。 大きなモデルでは、いくつかの損失函数を合わせてつかうことになる。 その際、損失オブジェクトを作る方が部品として扱いやすく、維持しやすいコードになる。 本書では、最初の方法を主に用いるが、特別な理由があれば別の方法を取ることにする。",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>損失函数</span>"
    ]
  },
  {
    "objectID": "loss-functions.html#どの損失函数を選択すべきか",
    "href": "loss-functions.html#どの損失函数を選択すべきか",
    "title": "9  損失函数",
    "section": "9.2 どの損失函数を選択すべきか。",
    "text": "9.2 どの損失函数を選択すべきか。\n深層学習や機械学習全般では、ほとんどの応用は数値の予測または確率の推定のどちらか一つ、または両方を行う。 現在の例の回帰問題は前者を行なっている。 実際の応用では、気温や従業員の離職率を推定したり、売り上げを予測したりする。 後者では、典型的な問題は分類だ。 例えば画像を最も顕著な内容に基づいて分類するには、実際にはそれぞれの確率を計算する。 そして「犬」の確率が0.7で、「猫」の確率が0.3なら犬と判定する。\n\n9.2.1 最尤法\n分類も回帰も最も使われている損失函数は最尤原理に基づいている。 最尤とは、モデルのパラメタの選択がデータ、つまり観測したものや観測できたかもしれないことが最大限起こりやすいようにする。 この原理は基本的である「だけ」でなく、直感に訴えるものだ。 簡単な例を考えよう。\n例えば、7.1, 22.14, 11.3という値があり、生じさせる過程が正規分布に則っているものとする。 その場合、これらのデータは平均14、標準偏差7の分布により生じたものである可能性が、平均20、標準偏差 1よりもはるかに可能性が高い。\n\n\n9.2.2 回帰\n回帰（目的の分布が正規分布であるという暗黙の仮定をおいたもの^1）では、尤度を最大化するには、これまで計算してきた損失である、平均二乗誤差を引き続き用いればよい。 最尤推定値は望まれる全ての統計的な性質を持つ。 しかしながら、特定の用途では他の損失を使う理由があるかもしれない。 ^1: 仮定があり得ない場合、分布適合損失函数が提供されている（例: Poisson負対数は`nnf_poisson_nll_loss()）。\n例えば、データセットに外れ値があり、何らかの理由で予測と目的がかなりずれていることがある。 平均二乗誤差は外れ値に大きな重みを置く。 そのような場合、代替となりうるのは平均絶対誤差（nnf_l1_loss()）と滑らかなL1損失（`nn_smooth_l1_loss()）である。 後者は混合した型で、既定では絶対（L1）誤差を計算するが二乗（L2）に絶対誤差が非常に小さいところで切り替える。",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>損失函数</span>"
    ]
  },
  {
    "objectID": "optim2.html",
    "href": "optim2.html",
    "title": "10  L-BFGSを用いた函数の最適化",
    "section": "",
    "text": "10.1 L-BFGS見参\nこれまで、深層学習でよく使われる最適化器、確率的勾配降下法（SGD）やモーメンタム付SGDに加えて、適応学習率を使う種類、RMSPropや、Adadelta、Adagrad、Adamのような古典的な手法について議論した。 これらには一つの共通点がある。 それは、勾配、つまり一階微分のベクトルだけを使うことだ。 従って、これらは一次アルゴリズムである。 しかしながら、これらの手法はヘシアン、二階微分の行列から得られる有用な情報が欠けていることを意味している。",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>L-BFGSを用いた函数の最適化</span>"
    ]
  },
  {
    "objectID": "optim2.html#l-bfgs見参",
    "href": "optim2.html#l-bfgs見参",
    "title": "10  L-BFGSを用いた函数の最適化",
    "section": "",
    "text": "10.1.1 変化する傾き\n一階微分からは地形の 勾配 が得られる。 上がっているか、下がっているか。それはどれくらいか。 一歩先に進めると、二階微分は傾きがどれくらい変化するかを表す。\nなぜこれが重要なのか。\n現在\\(\\mathbf{x}_n\\)にいて、適切な降下方向を決めたとする。 事前に定めた学習率で決めた幅の一歩を進んで、\\(\\mathbf{x}_{n+1}\\)に到達して完了する。 分からないのは、傾きが到着するまでにどれくらい変化したかだ。 途中でかなり平らになったからもしれない。 その場合、行き過ぎて遠いところに行ってしまい、途中で（傾きが再び上りに転じることも含めて）いろいろなことが起きていたかもしれない。\nこれを単変数の関数で説明できる。 放物線、例えば\n\\[\ny = 10x^2\n\\] を考える。 その微分は\\(\\mathrm{d}y/\\mathrm{d}x= 20 x\\)だ。 現在の\\(x\\)が例えば3で、学習率が\\(0.1\\)であれば、20 * 3 * 0.1 = 6を引いて、-3に至る。\nでも2で減速して、そこでの傾きを調べると、緩やかになっているので、20 * 2 * 0.1 = 4を差し引くことになる。\n運を天に任せ、「目を閉じて飛び込む」という方法がうまくいくのは、問題となっている函数に対する適切な学習率を用いた場合に限る。 （選んだ学習率だと、別の放物線\\(y=5x^2\\)の場合が該当する。） しかし、最初から判断に二階微分を含めておくほうが賢明ではないだろうか。\nこの形を実践するアルゴリズムがニュートン法の一群である。 最初に最も「純粋な」種類を見る。 これは、原理を説明するには最適だが、実際に使うのが容易であることは稀である。\n\n\n10.1.2 厳密ニュートン法\n高次元で厳密ニュートン法は勾配にヘシアン逆行列をかけ、降下方向を座標毎に伸縮する。 我々の例は独立変数は一つだけなので、一階微分を二階微分で割るということを意味する。\n勾配の伸縮をしたところで、どの位の割合を差し引くべきか。 原型では、厳密ニュートン法は学習率を使用しないので、慣れ親しんだ試行錯誤からは解放される。 例えば、我々の例では二階微分は20なので、(20 * 3) / 20 = 3を引く。 確かに、最小値の場所である0に一歩でたどり着く。\nこの結果が素晴らしいのに、なぜいつも使わないのか。 一つ理由は、示した例のような二次函数には完璧に働くが、通常は何らかの「調整」、例えば学習率を使うことなども必要となることだ。\nしかし、主要な理由は別にある。 もっと現実的な応用で、機械学習と深層学習の分野では、ヘシアンの逆行列を毎回計算するのが高くつくからだ。 （そもそも可能ではないかもしれない。） そこで、準ニュートン法として知られる近似が使われる。",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>L-BFGSを用いた函数の最適化</span>"
    ]
  },
  {
    "objectID": "optim2.html#ニュートンの近似-bfgsとl-bfgs",
    "href": "optim2.html#ニュートンの近似-bfgsとl-bfgs",
    "title": "10  L-BFGSを用いた函数の最適化",
    "section": "10.2 ニュートンの近似: BFGSとL-BFGS",
    "text": "10.2 ニュートンの近似: BFGSとL-BFGS\nニュートン法の近似のうち、最もよく使われているのは、Broyden-Fletcher-Goldfarb-Shannoアルゴリズム、 BFGS である。 ヘシアンの厳密な逆行列を連続して計算する代わりに、反復更新された逆行列の近似を保持する。 BFGSはメモリに優しい派生型、 メモリ制約BFGS（ L-BFGS ）で実装されることが多い。 これが torch 最適化器の一部として提供されている。\n試してみる前に、もう一つの概念を説明しておく。\n\n10.2.1 直線探索\n厳密な形式同様に、近似ニュートン法は学習率なしで使うことができる。 その場合、降下方向を計算して伸縮した勾配にそのまま従う。 問題となる函数次第で、これはうまく行ったり行かなかったりする。 うまく行かない場合は、二つの対応がある。 一つは、短い幅をとる、つまり学習率を導入する。 もう一つは直線探索だ。\n直線探索を使うと、どこまで降下方向に沿うのかの評価に時間にをかける。 これを行う二つの主要な方法がある。\n最初は、厳密な直線探索で、もう一つの最適化問題を伴う。 現在の位置で、降下方向を計算し、これを学習率だけに依存する 第二の 函数として書く。 そしてこの函数を微分して、その最小を見つける。 解は、歩幅を最適化する学習率となる。\n代替の方法は、近似探索だ。 もう驚かないだろうが、厳密ニュートン法よりも近似ニュートン法が現実的に実行可能であるように、近似探索は厳密直線探索よりも実行がしやすい。\n直線探索では、最適解の近似は以下の折り紙付きの経験則に基づいている。 基本的に、十分であるものを探す。 確立した経験則の最たるものが 強ウルフ 条件であり、これが torch のoptim_lbfgs() に実装されている。 次の節では、 optim_lbfgs()の使い方を学び、ローゼンブロック函数を最適化で、直線探索ありとなしの両方を使う。",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>L-BFGSを用いた函数の最適化</span>"
    ]
  },
  {
    "objectID": "optim2.html#optim_lbfgs-を使ったローゼンブロック函数の最小化",
    "href": "optim2.html#optim_lbfgs-を使ったローゼンブロック函数の最小化",
    "title": "10  L-BFGSを用いた函数の最適化",
    "section": "10.3 optim_lbfgs() を使ったローゼンブロック函数の最小化",
    "text": "10.3 optim_lbfgs() を使ったローゼンブロック函数の最小化\nローゼンブロック函数を再掲する。\n\nlibrary(torch)\n\na &lt;- 1\nb &lt;- 5\n\nrosenbrock &lt;- function(x) {\n  x1 &lt;- x[1]\n  x2 &lt;- x[2]\n  (a - x1)^2 + b * (x2 - x1^2)^2\n}\n\n自作で最小化を試みた時は、次の手順で行った。 最初だけ、まずパラメタテンソルを定義し、現在の \\(\\mathbf{x}\\) を格納した。\nx &lt;- torch_tensor(c(-1, 1), requires_grad = TRUE)\nそして、反復して次の演算を実行した。\n\n現在の \\(\\mathbf{x}\\) での函数値を計算する。\n考えている場所におけるその値の勾配を計算する。\n現在の \\(\\mathbf{x}\\) から勾配の一定の割合を差し引く。\n\nどのようにこの青写真が変わるのか。\n最初の手順は変わらない。\nvalue &lt;- rosenbrock(x)\n二つ目の手順も変わらず、直接出力テンソルに対してbackward()を呼ぶ。\nvalue$backward()\nその理由は最適化器は勾配を 計算 せず、ひとたび計算されたら勾配をどうするか決めている。\n変わるのは、三番目の手順で、最も面倒でもあった。\n更新を適用するのは最適化器である。 それを可能にするには、前提がある。 ループを始める前に、最適化器はどのパラメタに対して作用するか決められている必要がある。 実は、これはとても重要であり、そのパラメタを渡すことなく、最適化器を作ることすらできない。\nopt &lt;- otpim_lbfgs(x)\nループでは最適化オブジェクトに対して step() メソッドを呼んでパラメタを最適化する。 自作での手続のうち新しいやり方に引き継がなければならないことが一つだけある。 依然として各反復で勾配を0にしなければならない。 今回は、パラメタ x に対してではなく、最適化オブジェクトそのものに対してである。 基本的に、各反復して次の動作を実行することになる。\nvalue &lt;- rosenbrock(x)\n\nopt$zero_grad()\nvalue$backward()\n\nopt$step()\nなぜ「基本的に」なのか。 実は、これは optimizer_lbdgs() 以外 に書かなければならないものだ。\noptim_lbfgs() については、step() は無名函数、クロージャに渡す必要がある。 前の勾配を0にするもの、函数呼び出し、勾配計算が全てクロージャの中で行われる。\ncalc_loss &lt;- function() {\n  optimzer$zero_grad()\n  value &lt;- rosenbrokc(x_star)\n  value$backward()\n  value\n}\nこれらの操作を実行したら、クロージャは函数値を返す。 step() でどのように呼ぶか示す。\nfor (i in 1:num_iterations) {\n  optimizer$step(calc_loss)\n}\n組み上がったので、少しログ出力を加えて、直線探索ありとなしで何が起きるか比較してみよう。\n\n10.3.1 optim_lbfgs()の既定の動作\n基準として、まず直線探索なしで走らせる。 反復2回で十分だ。 以下の出力で、各反復でクロージャは何回か評価される。 これがまずクロージャを書いた技術的な理由だ。\n\nnum_iterations &lt;- 2\n\nx &lt;- torch_tensor(c(-1,1), requires_grad = TRUE)\n\noptimizer &lt;- optim_lbfgs(x)\n\ncalc_loss &lt;- function() {\n  optimizer$zero_grad()\n  \n  value &lt;- rosenbrock(x)\n  cat(\"Value is: \", as.numeric(value), \"\\n\")\n  value\n}\n\nfor (i in 1:num_iterations) {\n  cat(\"\\nIteration: \", i, \"\\n\")\n  optimizer$step(calc_loss)\n}\n\n\nIteration:  1 \nValue is:  4 \n\nIteration:  2 \nValue is:  4 \n\n\n最小が見つかったか確認するするには、xを印字してみる。\n\nx\n\ntorch_tensor\n-1\n 1\n[ CPUFloatType{2} ][ requires_grad = TRUE ]\n\n\n\n\n10.3.2 直線探索付optim_lbfgs()\n\nnum_iterations &lt;- 2\n\nx &lt;- torch_tensor(c(-1, 1), requires_grad = TRUE)\n\noptimizer &lt;- optim_lbfgs(x, line_search_fn = \"strong_wolfe\")\n\ncalc_loss &lt;- function() {\n  optimizer$zero_grad()\n  \n  value &lt;- rosenbrock(x)\n  cat(\"Value is: \", as.numeric(value), \"\\n\")\n\n  value$backward()\n  value\n}\n\nfor (i in 1:num_iterations) {\n  cat(\"\\nIteration: \", i, \"\\n\")\n  optimizer$step(calc_loss)\n}\n\n\nIteration:  1 \nValue is:  4 \nValue is:  6 \nValue is:  3.802412 \nValue is:  3.680712 \nValue is:  2.883048 \nValue is:  2.5165 \nValue is:  2.064779 \nValue is:  1.383838 \nValue is:  1.073062 \nValue is:  0.8844348 \nValue is:  0.5554548 \nValue is:  0.2501073 \nValue is:  0.8948812 \nValue is:  0.1619076 \nValue is:  0.06823033 \nValue is:  0.01653571 \nValue is:  0.004060294 \nValue is:  0.003537784 \nValue is:  0.0003913814 \nValue is:  4.303527e-06 \nValue is:  2.033371e-08 \nValue is:  6.870948e-12 \n\nIteration:  2 \nValue is:  6.870948e-12 \n\n\n直線探索を使うと、一回の反復で十分に最小値に到達する。 順に損失を確認すると、アルゴリズムは函数を調べる度にほぼ毎回損失を減少させるが、使わない場合はそうではないことが分かる。",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>L-BFGSを用いた函数の最適化</span>"
    ]
  },
  {
    "objectID": "network2.html",
    "href": "network2.html",
    "title": "11  ニューラルネットワークのモジュール化",
    "section": "",
    "text": "11.1 データ\n準備として、データを前のように生成する。\nlibrary(torch)\n\n# 入力次元（入力する特徴量の数）\nd_in &lt;- 3\n# 訓練集合に含まれる観測の数\nn &lt;- 100\n\nx &lt;- torch_randn(n, d_in)\ncoefs &lt;- c(0.2, -1.3, -0.5)\ny &lt;- x$matmul(coefs)$unsqueeze(2) + torch_randn(n, 1)",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>ニューラルネットワークのモジュール化</span>"
    ]
  },
  {
    "objectID": "network2.html#ネットワーク",
    "href": "network2.html#ネットワーク",
    "title": "11  ニューラルネットワークのモジュール化",
    "section": "11.2 ネットワーク",
    "text": "11.2 ネットワーク\n二つの線型層がReLU活性化函数により接続するには、sequentialモジュールを使うのが最も簡単である。 これは、モジュールを導入したときに見たものによく似ている。\n\n# 隠れ層の次元\nd_hidden &lt;- 32\n# 出力層の次元（予測される特徴量の数）\nd_out &lt;- 1\n\nnet &lt;- nn_sequential(\n  nn_linear(d_in, d_hidden),\n  nn_relu(),\n  nn_linear(d_hidden, d_out)\n)",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>ニューラルネットワークのモジュール化</span>"
    ]
  },
  {
    "objectID": "network2.html#訓練",
    "href": "network2.html#訓練",
    "title": "11  ニューラルネットワークのモジュール化",
    "section": "11.3 訓練",
    "text": "11.3 訓練\n更新された訓練の過程を示す。 よく選ばれている、Adam最適化器を使う。\n\nopt &lt;- optim_adam(net$parameters)\n\n### 訓練ループ --------------------------------------\n\nfor (t in 1:200) {\n  \n  ### -------- 順伝播 --------\n  y_pred &lt;- net(x)\n  \n  ### -------- 損失計算 --------\n  loss &lt;- nnf_mse_loss(y_pred, y)\n  if (t %% 10 == 0)\n    cat(\"エポック: \", t, \" 損失: \", loss$item(), \"\\n\")\n  \n  ### -------- 逆伝播 --------\n  opt$zero_grad()\n  loss$backward()\n  \n  ### -------- 重みの更新 --------\n  opt$step()\n}\n\nエポック:  10  損失:  2.832665 \nエポック:  20  損失:  2.665599 \nエポック:  30  損失:  2.506462 \nエポック:  40  損失:  2.353123 \nエポック:  50  損失:  2.203764 \nエポック:  60  損失:  2.057494 \nエポック:  70  損失:  1.916324 \nエポック:  80  損失:  1.783049 \nエポック:  90  損失:  1.659109 \nエポック:  100  損失:  1.546306 \nエポック:  110  損失:  1.446665 \nエポック:  120  損失:  1.361843 \nエポック:  130  損失:  1.292294 \nエポック:  140  損失:  1.237168 \nエポック:  150  損失:  1.195005 \nエポック:  160  損失:  1.162809 \nエポック:  170  損失:  1.137725 \nエポック:  180  損失:  1.11738 \nエポック:  190  損失:  1.100669 \nエポック:  200  損失:  1.086426 \n\n\nコードが短くなり、効率的になっただけでなく、変更により性能も大きく向上した。",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>ニューラルネットワークのモジュール化</span>"
    ]
  },
  {
    "objectID": "dl-overview.html",
    "href": "dl-overview.html",
    "title": "12  概要",
    "section": "",
    "text": "本書のこの部は、全て深層学習の応用に割かれている。 掘り下げることが二つある。 一つ目の話題は手順に関係することで、もう一つは分野への適合についてである。\n手順に関して、学ぶことは、\n\n入力データを準備して、モデルが扱える形式にすること。\n有効かつ効率的にモデルを訓練し、進捗を監視し、ハイパパラメタを学習中に調整すること。\nモデルを保存し、読み込むこと。\nモデルを汎化して訓練データ以外を扱えるようにすること。\n訓練を高速化すること。\n等\n\n二番目に、効率的な手順の次には、問題とする課題が重要となる。 第一部で torch を学ぶために用いた線型層を重ねる方法は、目的には不十分であり、画像や時系列のモデル化は難しい。 深層学習をうまく使うということは、モデルの構造を問題としている分野に適合させることを意味する。 その目的には、具体的な課題から始め、適用可能な構造を直接例示する。\n具体的な計画は以下の通りである。 以下の二つの章では、手順に関して実践で不可欠な手法を示す。 別のパッケージ luz を知ることで、層を抽象化し、手順をかなり簡素化することを学ぶ。 使い方を学んだら、最初の応用、画像分類を見る準備が整う。 初期結果を改善するために、後退して二つのより進んだ手順についての話題を探索する。 それは、汎化の改善と学習の高速化だ。 この知識を得た上で画像に戻り、その後、応用に関して、表形式データや、時系列、音声についての技術を高める。",
    "crumbs": [
      "torchで深層学習",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>概要</span>"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "13  データの読み込み",
    "section": "",
    "text": "13.1 データと dataset() とdataloader() の違い\n本書では、「データセット」（可変幅フォント、括弧なし）あるいは単に「データ」は、通常Rの行列、data.frameやそれらに含まれるものを指す。 dataset() （固定幅フォント、括弧つき）は、torch のオブジェクトでできることが一つある。 それは、呼び出し相手に対して単一の項目を渡すことだ。 その項目は通常リストで、一つの入力と一つの目的テンソルから構成されている。 （実は課題に対して意味をなすものであればなんでもよい。例えば、単一のテンソルで入力と出力を兼ねることもありうる。二つ以上のテンソルが異なる入力として異なるモジュールに渡されることも考えられる。）\n上述の目的を満たす限り、dataset() は必要なことを自由にできる。 例えば、データをインターネットからダウンロードし一時的な場所に保存したり、何んらかの前処理をしたり、 ある種類のモデルに必要とされるデータのバイト単位の塊を返したりすることもある。 裏で何をしようとも、呼び出しの相手が求めるのは単一の項目を返すことだ。 呼び出すのは、dataloader() である。\ndataloader() の役割は、モデルにバッチで入力することだ。 一つの直接的な理由は、コンピュータのメモリだ。 多くの dataset() は一度にモデルに渡すには大きすぎる。 他にもバッチにする利点がある。 勾配の計算（そしてモデルの重みの更新）をバッチ毎に行うと、過程に対する固有の確率的な性質があり、これがモデルの訓練に役立つ。 これは、後の章で詳しく議論する。",
    "crumbs": [
      "torchで深層学習",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>データの読み込み</span>"
    ]
  },
  {
    "objectID": "data.html#dataset-の使い方",
    "href": "data.html#dataset-の使い方",
    "title": "13  データの読み込み",
    "section": "13.2 dataset() の使い方",
    "text": "13.2 dataset() の使い方\ndataset() には様々なものがある。 torchvision や torchdatasets 、その他torch で使えるように準備済のデータを提供しているパッケージに含まれ、すぐに使えるようになっているものから、完全に変更できる（つまりユーザ自身による準備が必要な）ものまである。 dataset() を作るのは簡単だ。 これらはR6オブジェクトで、実装が必要なメソッドは次の三つだけだ。\n\ninitialize(...)。 initialize()に渡すパラメタは、dataset()のインスタンスが作られた時に渡される。 考えられるのは、Rの data.frame やファイルシステムのパス、ダウンロードのURL、dataset() が期待する様々な設定やパラメタがあるが、それらに限定されない。\n.getitem(i)。これは契約を満たすことを担うメソッドである。 返すものは全て単一の要素である。 パラメタ i は、多くの場合、背景のデータ構造（例えば、ファイルシステムのパスの data.frame） における開始位置を示すインデックスである。 でも、 dataset() は、このパラメタを使わなければならないわけではない。 例えば、非常に巨大な dataset() や、クラスにかなりの不均衡がある場合、代わりに標本抽出に基づいた要素を返すこともできる。\n.length() 通常これは一行で、唯一の目的は dataset() で利用できる要素の数を返すことだ。\n\ndataset() の青写真を示す。\nds &lt;- dataset()(\n  initialize = function(...) {\n    ...\n  },\n  .getitem = function(index) {\n    ...\n  },\n  .length = function() {\n    ...\n  }\n)\nということで、作業に用いる dataset() を得る三つの方法について、注文仕立てから、最も楽なものまで、比較してみよう。\n\n13.2.1 自作 dataset()\niris の代替である palmerpenguins に基づく判別器を構築したいとする。\n\nlibrary(torch)\nlibrary(palmerpenguins)\n\nstr(penguins)\n\ntibble [344 × 8] (S3: tbl_df/tbl/data.frame)\n $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ bill_length_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ bill_depth_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\n $ body_mass_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n $ year             : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\n\n\nspecies の予測において bill_length_mm と bill_depth_mm 、 flipper_length_mm 、 body_mass_g の一部を用いる。 必要なものをちょうど返す dataset() を作る。\n\npenguins_dataset &lt;- dataset(\n  name = \"penguins_dataset\",\n  initialize = function(df) {\n    df &lt;- na.omit(df)\n    self$x &lt;- as.matrix(df[, 3:6]) |&gt; torch_tensor()\n    self$y &lt;- torch_tensor(\n      as.numeric(df$species)\n    )$to(torch_long())\n  },\n  .getitem = function(i) {\n    list(x = self$x[i, ], y = self$y[i])\n  },\n  .length = function() {\n    dim(self$x)[1]\n  }\n)\n\n一度 penguins_dataset のインスタンスを作成したら、すぐに簡単な確認をしよう。 まず、期待される長さから。\n\nds &lt;- penguins_dataset(penguins)\nlength(ds)\n\n[1] 333\n\n\n次に、ここの要素は期待通りの形状とデータ型であるか。 インデックスを使って、dataset() の要素にテンソルの値のように表示できるのは便利だ。\n\nds[1]\n\n$x\ntorch_tensor\n   39.1000\n   18.7000\n  181.0000\n 3750.0000\n[ CPUFloatType{4} ]\n\n$y\ntorch_tensor\n1\n[ CPULongType{} ]\n\n\nこれは dataset() を掘り下げて行ったときにも動作するし、しなければならない。 dataset() のインデックスは、裏では 求める位置 i で .getitem() を呼び出す。\n実際には、自作の dataset() を作成しなかった。 ほとんど前処理が不要な、代替となる tensor_dataset() がある。\n\n\n13.2.2 tensor_dataset()\nテンソルが既にあるときや、容易に変換できるときは、組込の dataset() ジェネレータ である tensor_dataset() を使うことができる。 この函数は任意の数のテンソルを受け取ることができる。 個々のバッチ要素は、テンソル値のリストである。\n\nthree &lt;- tensor_dataset(\n  torch_randn(10), torch_randn(10), torch_randn(10)\n)\nthree[1]\n\n[[1]]\ntorch_tensor\n-2.6347\n[ CPUFloatType{1} ]\n\n[[2]]\ntorch_tensor\n 0.5663\n[ CPUFloatType{1} ]\n\n[[3]]\ntorch_tensor\n 1.0616\n[ CPUFloatType{1} ]\n\n\npenguins の文脈では、二行のコードになる。\n\npenguies &lt;- na.omit(penguins)\nds &lt;- tensor_dataset(\n  torch_tensor(as.matrix(penguins[, 3:6])),\n  torch_tensor(\n    as.numeric(penguins$species)\n  )$to(torch_long())\n)\n\nds[1]\n\n[[1]]\ntorch_tensor\n   39.1000    18.7000   181.0000  3750.0000\n[ CPUFloatType{1,4} ]\n\n[[2]]\ntorch_tensor\n 1\n[ CPULongType{1} ]\n\n\nしかしながら、データセットの全ての列を使っていないことは認めざるを得ない。 より多く前処理を dataset() に行わせるには、より多くのコードを書く必要がある。\n三番目かつ、最後に最も楽な方法を示す。\n\n\n13.2.3 torchvision::mnist_dataset()\ntorch のエコシステムにおけるパッケージを使っているとき、実例かデータ自体が目的かは別として、何らかの dataset() を含んでいる可能性が高い。 torchvision は、古典的な画像のデータセットを提供している。 その中で、典型中の典型が MNIST である。\n後の章で画像処理を議論するので、 minst_dataset() の引数についてここで述べることにする。 ここでは、データが期待されるものに適合しているか簡単に確認する。\n\nlibrary(torchvision)\n\ndir &lt;- \"~/.torch-datasets\"\n\nds &lt;- mnist_dataset(\n  root = dir,\n  train = TRUE, # 既定\n  download = TRUE,\n  transform = function(x) {\n    x |&gt; transform_to_tensor()\n  }\n)\n\nfirst &lt;- ds[1]\ncat(\"Image shape: \", first$x$shape, \" Label: \", first$y, \"\\n\")\n\nImage shape:  1 28 28  Label:  6 \n\n\nこの時点では、dateset() について知っておくことは全てだ。 本書を読み進める中で多く出会うことになる。 それでは、単一から複数のデータに進もう。",
    "crumbs": [
      "torchで深層学習",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>データの読み込み</span>"
    ]
  },
  {
    "objectID": "data.html#dataloader-の使い方",
    "href": "data.html#dataloader-の使い方",
    "title": "13  データの読み込み",
    "section": "13.3 dataloader() の使い方",
    "text": "13.3 dataloader() の使い方\n新たに作成したMNISTの dataset() を使って、これに対する dataloader() のインスタンスを作成する。 dataloader() は、一度に32個のバッチにおいて画像とラベルの組みを返す。 各エポックで、異なる順序で返す（shuffle = TRUE）。\n\ndl &lt;- dataloader(ds, batch_size = 32, shuffle = TRUE)\n\ndataset() 同様に dataloader() も長さを確認できる。\n\nlength(dl)\n\n[1] 1875\n\n\n今回は、返り値は要素の数ではなく、バッチの数である。\nバッチに亙り繰り返すには、先にイテレータを得る。 このオブジェクトは、この dataloader() の要素を辿りかたを知っている。 datalader_next() を呼び足すと、一つずつ次のバッチを得ることができる。\n\nfirst_batch &lt;- dl |&gt;\n  # この dataloader に対するイテレータを得る。\n  dataloader_make_iter() |&gt;\n  dataloader_next()\n\ndim(first_batch$x)\n\n[1] 32  1 28 28\n\ndim(first_batch$y)\n\n[1] 32\n\n\nx の形状、画像部分と個々の（上で確認した）画像、今追加された次元が、バッチにある画像の数を反映していることが分かる。\n次の段階は、バッチをモデルに与えることである。 実は、これと完全な、端から端までの深層学習の手順は次の章で扱う内容である。",
    "crumbs": [
      "torchで深層学習",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>データの読み込み</span>"
    ]
  },
  {
    "objectID": "other-overview.html",
    "href": "other-overview.html",
    "title": "14  概要",
    "section": "",
    "text": "これまで機械学習について多くの議論を重ねてきた。 しかし torch は他の目的の科学的な応用においても効果的に利用されている。 例えば、パターンや関係、構造を発見する数学的な手法に依存する例が挙げられる。\nこの部では、三つの話題に絞る。 最初は行列計算で、その重要性は疑うことが困難である。 実際、科学計算と機械学習の全ての計算は行列計算（テンソルは高次元の行列）である。 具体的には、最小二乗法を行列分解により解く。 その際に、 linalg_cholesky() 、linalg_qr() 、 linalg_svd() のような函数を使う。 また、どのように（本来の信号処理の意味での）畳み込みが効率的に実装されているかについても触れる。\n次に、有名な数学的手法で、既に（間接的だが非常に便利に）使ったことがある離散フーリエ変換（DST: Discrete Fourier Transform） 今回はただ使うだけではなく、なぜそしてどのように動作するか理解することを目的する。 理解できたら、簡単な実装はわずか数行できる。 二番目の章では、高速フーリエ変換（FFT: Fast Fourier Transform）を使って、DFTを効率的に実装することに専念する。 自作の一つが驚くほど性能の上で torch_fft_fft() に近いか確認することになる。\n最後に探求するのは、フーリエ法よりもはるかに新しい考え方、ウェーブレット変換である。 この変換はデータ解析において広く用いられているが、その理由が明らかになる。 torch にはウェーブレット変換を計算する専用の方法はないが、 torch_fft_fft() を繰り返し使うことで、効率的な実装ができる。",
    "crumbs": [
      "他にも`torch` でできること: 行列計算・フーリエ変換・ウェーブレット",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>概要</span>"
    ]
  },
  {
    "objectID": "leastsquares.html",
    "href": "leastsquares.html",
    "title": "15  行列演算: 最小二乗問題",
    "section": "",
    "text": "15.1 最小二乗を求める五つの方法\n線型最小二乗回帰をどのように計算するか。 Rでは、lm() を用いる。 torch には、linalg_lstsq() がある。 Rが利用者からややこしいことを隠蔽するのに対し、torch のような高性能計算フレームワークではより明示的な作業を求める傾向がある。 作業とは、説明書を丁寧に読むことや、少し試すこと、もしくはその両方だ。 例えば、次に示す linalg_lstsq() の説明の主要部は driver パラメタが詳しく記述されている。\ndriver chooses the LAPACK/MAGMA function that will be used.\nFor CPU inputs the valid values are 'gels', 'gelsy', 'gelsd', 'gelss'. For CUDA input, the only valid driver is 'gels', which assumes that A is full-rank.\nTo choose the best driver on CPU consider:\nSee also the full description of these drivers\nこれを知る必要があるかどうかは、解こうとしている問題次第だ。 でも必要なら、ここで何が議論されているか理解することは、抽象的であっても、役立つだろう。\n以下の例では、幸運にもどのドライバを用いても同一の結果が得られる。 ただし、「秘訣」の類を適用することが必要だ。 それでも、 linalg_lstsq() が使っている様々な手法やよく使われる他の方法を深掘りする。 具体的には、以下の方法で最小二乗問題を解く。\n全ての方法は、現実のデータセット、次に安定性に欠けることが知られているベンチマーク問題にに適用される。",
    "crumbs": [
      "他にも`torch` でできること: 行列計算・フーリエ変換・ウェーブレット",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>行列演算: 最小二乗問題</span>"
    ]
  },
  {
    "objectID": "leastsquares.html#最小二乗を求める五つの方法",
    "href": "leastsquares.html#最小二乗を求める五つの方法",
    "title": "15  行列演算: 最小二乗問題",
    "section": "",
    "text": "If A is well-conditioned (its condition number is not too large), or you do not mind some precision loss.\nFor a general matrix: 'gelsy' (QR with pivoting) (default)\nIf A is full-rank: 'gels' (QR)\nIf A is not well-conditioned.\n'gelsd' (tridiagonal reduction and SVD)\nBut if you run into memory issues: 'gelss' (full SVD).\n\n\n\n\n\nいわゆる 正規方程式 を用いた、最も直接的な方法、その意味は問題の数学的な記述から直ちに得られることを指す。\nここでも正規方程式から出発するが、 コレスキー分解 を用いて問題を解く。\nまたまた正規方程式を出発点とし、 LU分解 を用いて進める。\n四つ目と最後の方法では、異なる種類の分解 QR を用いる。 この分解は「現実に」適用される主要なものだ。 QR分解を用いる場合、アルゴリズムは正規方程式から出発しない。\n最後の五つ目は、 特異値分解（SVD: Singular Value Decomposition） を用いる。 ここでも、正規方程式は不要である。",
    "crumbs": [
      "他にも`torch` でできること: 行列計算・フーリエ変換・ウェーブレット",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>行列演算: 最小二乗問題</span>"
    ]
  },
  {
    "objectID": "leastsquares.html#天気予報についての回帰",
    "href": "leastsquares.html#天気予報についての回帰",
    "title": "15  行列演算: 最小二乗問題",
    "section": "15.2 天気予報についての回帰",
    "text": "15.2 天気予報についての回帰\n用いる データセット はUCI機械学習リポジトリ から得られる。 Zipを展開して、作業ディレクトリ（ここでは data）に配置する。 ここでの利用方法は、本来の目的とは異なる。 機械学習で気温を予測する代わりに、原論文 (Cho et al. 2020) では数値天気予報モデルの予測データのバイアス修正について述べられている。 本来の用途を気にする必要ない。 ここでの主眼は行列演算にあり、データセット自体はここで行う調査に非常に適している。\n\nset.seed(777)\n\nlibrary(torch)\n\nweather_df &lt;- na.omit(read.csv(\"data/Bias_correction_ucl.csv\"))\nhead(weather_df)\n\n  station       Date Present_Tmax Present_Tmin LDAPS_RHmin LDAPS_RHmax\n1       1 2013-06-30         28.7         21.4    58.25569    91.11636\n2       2 2013-06-30         31.9         21.6    52.26340    90.60472\n3       3 2013-06-30         31.6         23.3    48.69048    83.97359\n4       4 2013-06-30         32.0         23.4    58.23979    96.48369\n5       5 2013-06-30         31.4         21.9    56.17410    90.15513\n6       6 2013-06-30         31.9         23.5    52.43713    85.30725\n  LDAPS_Tmax_lapse LDAPS_Tmin_lapse LDAPS_WS  LDAPS_LH LDAPS_CC1 LDAPS_CC2\n1         28.07410         23.00694 6.818887  69.45181 0.2339475 0.2038957\n2         29.85069         24.03501 5.691890  51.93745 0.2255082 0.2517714\n3         30.09129         24.56563 6.138224  20.57305 0.2093437 0.2574694\n4         29.70463         23.32618 5.650050  65.72714 0.2163720 0.2260024\n5         29.11393         23.48648 5.735004 107.96554 0.1514069 0.2499953\n6         29.21934         23.82261 6.182295  50.23139 0.1852788 0.2808180\n  LDAPS_CC3 LDAPS_CC4 LDAPS_PPT1 LDAPS_PPT2 LDAPS_PPT3 LDAPS_PPT4     lat\n1 0.1616969 0.1309282          0          0          0          0 37.6046\n2 0.1594441 0.1277273          0          0          0          0 37.6046\n3 0.2040915 0.1421253          0          0          0          0 37.5776\n4 0.1611574 0.1342487          0          0          0          0 37.6450\n5 0.1788925 0.1700210          0          0          0          0 37.5507\n6 0.2328410 0.1463629          0          0          0          0 37.5102\n      lon      DEM  Slope Solar.radiation Next_Tmax Next_Tmin\n1 126.991 212.3350 2.7850        5992.896      29.1      21.2\n2 127.032  44.7624 0.5141        5869.312      30.5      22.5\n3 127.058  33.3068 0.2661        5863.556      31.1      23.9\n4 127.022  45.7160 2.5348        5856.965      31.7      24.3\n5 127.135  35.0380 0.5055        5859.552      31.2      22.5\n6 127.042  54.6384 0.1457        5873.781      31.5      24.0\n\n\nここでの問題の組み立て方では、基本的にデータセット内の全てが予測変数である。（データセットに残しておけば予測因子になりうる。詳しくは以下で説明する） 目的変数としては、 Next_Tmax 翌日に取った最高気温を用いる。 この場合、 Next_Tmin を予測変数から取り除く必要がある。 さもないと、これが強力すぎる手掛かりになる。 同様に station 測候所の識別番号と Date も削除する。 結果として、21個の予測変数が得られ、これらには実際の気温（ Prexent_Tmax 、 Present_Tmin ）、様々な変数に対するモデルの予報（ LDAPS_* ）、そして補助的な情報（ lat や、 lon Solar radiation など）がある。\n\nweather_df &lt;- data.frame(scale(\n  weather_df[!(names(weather_df) %in% c(\"station\", \"Date\", \"Next_Tmin\"))]))\n\nここで予測変数を 標準化 する scale() を追加したことに目を向けてほしい。 これは、上述した「秘訣」である。 なぜそのようにするかは、すぐに議論する。\ntorchには、データを二つのテンソルに分け、全ての予測変数を行列 A に、目的変数を格納した b に格納する。\n\nweather &lt;- torch_tensor(weather_df |&gt; as.matrix())\nA &lt;- weather[, 1:-2]\nb &lt;- weather[, -1]\n\ndim(A)\n\n[1] 7588   21\n\n\nそれでは、最初に期待される出力を求めよう。\n\n15.2.1 最小二乗法 (I): lm() による参照値\n「信用できる」最小二乗法の実装があるとすれば、もちろん lm() に違いない。\n\nfit &lt;- lm(Next_Tmax ~ . , data = weather_df)\nfit |&gt; summary()\n\n\nCall:\nlm(formula = Next_Tmax ~ ., data = weather_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.94439 -0.27097  0.01407  0.28931  2.04015 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       1.901e-12  5.390e-03   0.000 1.000000    \nPresent_Tmax      1.456e-01  9.049e-03  16.089  &lt; 2e-16 ***\nPresent_Tmin      4.029e-03  9.587e-03   0.420 0.674312    \nLDAPS_RHmin       1.166e-01  1.364e-02   8.547  &lt; 2e-16 ***\nLDAPS_RHmax      -8.872e-03  8.045e-03  -1.103 0.270154    \nLDAPS_Tmax_lapse  5.908e-01  1.480e-02  39.905  &lt; 2e-16 ***\nLDAPS_Tmin_lapse  8.376e-02  1.463e-02   5.726 1.07e-08 ***\nLDAPS_WS         -1.018e-01  6.046e-03 -16.836  &lt; 2e-16 ***\nLDAPS_LH          8.010e-02  6.651e-03  12.043  &lt; 2e-16 ***\nLDAPS_CC1        -9.478e-02  1.009e-02  -9.397  &lt; 2e-16 ***\nLDAPS_CC2        -5.988e-02  1.230e-02  -4.868 1.15e-06 ***\nLDAPS_CC3        -6.079e-02  1.237e-02  -4.913 9.15e-07 ***\nLDAPS_CC4        -9.948e-02  9.329e-03 -10.663  &lt; 2e-16 ***\nLDAPS_PPT1       -3.970e-03  6.412e-03  -0.619 0.535766    \nLDAPS_PPT2        7.534e-02  6.513e-03  11.568  &lt; 2e-16 ***\nLDAPS_PPT3       -1.131e-02  6.058e-03  -1.866 0.062056 .  \nLDAPS_PPT4       -1.361e-03  6.073e-03  -0.224 0.822706    \nlat              -2.181e-02  5.875e-03  -3.713 0.000207 ***\nlon              -4.688e-02  5.825e-03  -8.048 9.74e-16 ***\nDEM              -9.480e-02  9.153e-03 -10.357  &lt; 2e-16 ***\nSlope             9.402e-02  9.100e-03  10.331  &lt; 2e-16 ***\nSolar.radiation   1.145e-02  5.986e-03   1.913 0.055746 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4695 on 7566 degrees of freedom\nMultiple R-squared:  0.7802,    Adjusted R-squared:  0.7796 \nF-statistic:  1279 on 21 and 7566 DF,  p-value: &lt; 2.2e-16\n\n\n分散説明率78%は、予報がうまくいっていることを示している。 これを基準として、他の手法を比較調査する。 そのためにそれぞれの予測と予測誤差を保存する（後で平均二乗誤差 RMSE: root mean squared errorを計算するため）。 今は、 lm() に対する入力だけがある。\n\nrmse &lt;- function(y_true, y_pred) {\n  (y_true - y_pred)^2 |&gt; sum() |&gt; sqrt()\n}\n\nall_preds &lt;- data.frame(\n  b = weather_df$Next_Tmax,\n  lm = fit$fitted.values\n)\nall_errs &lt;- data.frame(lm = rmse(all_preds$b, all_preds$lm))\nall_errs\n\n       lm\n1 40.8369\n\n\n\n\n15.2.2 最小二乗法 (II): linalg_lstsq()の利用\n\nx_lstsq &lt;- linalg_lstsq(A, b)$solution\n\nall_preds$lstsq &lt;- as.matrix(A$matmul(x_lstsq))\nall_errs$lstsq &lt;- rmse(all_preds$b, all_preds$lstsq)\n\ntail(all_preds)\n\n              b         lm      lstsq\n7745 -1.1380931 -1.3544620 -1.3544625\n7746 -0.8488721 -0.9040997 -0.9040998\n7747 -0.7203294 -0.9675286 -0.9675287\n7748 -0.6239224 -0.9044044 -0.9044047\n7749 -0.5275154 -0.8738639 -0.8738641\n7750 -0.7846007 -0.8725795 -0.8725797\n\n\n予測は lm() と非常によく似ている。 実際、想像されるのは、わずかな差が数値誤差がそれぞれの呼び出しスタックの奥から表面化したものだけではないかということで、すなわち、同じはずだということだ。\n\nall_errs\n\n       lm   lstsq\n1 40.8369 40.8369\n\n\n確かに。これは満足のいく結果だ。 しかし、これはあの「秘訣」、標準化に基づく結果だ。 もちろん、「秘訣」というほどのものではない。 データの標準化は通常の操作で、特にニューラルネットワークでは日常的に使われていおり、訓練を高速化している。 強調したい点は、 torch のような高性能計算のためのフレームワークは適用される領域の知識や、事前の解析がより多く利用者側に求められることが多いということだ。\n\n\n15.2.3 一休み: 標準化しなかった場合\n素早く比較するために、今度はデータを標準化 せずに 別の予測変数の行列を作ってみる。\n\nweather_df_alt &lt;- data.frame(\n  read.csv(\"data/Bias_correction_ucl.csv\") |&gt;\n  na.omit() |&gt;\n  subset(select = c(-station, -Date, -Next_Tmin)))\n\nweather_alt &lt;- torch_tensor(weather_df_alt |&gt; as.matrix())\nA_alt &lt;- weather_alt[, 1:-2]\nb_alt &lt;- weather_alt[, -1]\n\n\nfit_alt &lt;- lm(Next_Tmax ~ ., data = weather_df_alt)\nall_preds_alt &lt;- data.frame(\n  b = weather_df_alt$Next_Tmax,\n  lm = fit_alt$fitted.values\n)\n\nall_errs_alt &lt;- data.frame(\n  lm = rmse(\n    all_preds_alt$b,\n    all_preds_alt$lm\n  )\n)\n\nall_errs_alt\n\n        lm\n1 127.0765\n\n\n次に、前のように既定の引数とともにlinalg_lstsq() を呼ぶ。\n\nx_lstsq_alt &lt;- linalg_lstsq(A_alt, b_alt)$solution\n\nall_preds_alt$lstsq &lt;- as.matrix(A_alt$matmul(x_lstsq_alt))\nall_errs_alt$lstsq &lt;- rmse(\n  all_preds_alt$b, all_preds_alt$lstsq\n)\n\nall_errs_alt\n\n        lm    lstsq\n1 127.0765 217.7987\n\n\nなんと、何が起きたのか。 引用した説明書の一部を思い出すと、今度は既定の引数がうまくいかなかったのかもしれない。 理由を見つけよう。\n\n15.2.3.1 問題の調査\n最小二乗問題を効率的に解くため、 torch はLAPACKというFortranのルーチン一式を呼んでいる。 LAPACKは効率的かつ規模の拡大に対応しつつ、線型代数においてよくある問題を解けるように設計されており、線型方程式の解や固有ベクトルと固有値、特異値を求めることができる。\nlinalg_lstsqt() で利用できる driver はLAPACKの異なる手続1に対応し、これらの手続は皆異なるアルゴリズムで問題を解く。 これは、今後行っていくことに類似している。\nつまり、何が起きているか調べるには、第一歩はどの手法がなぜ利用されたのか、（可能ならば）なぜ結果が満足いかないのか、代わりに使いたいLAPACKのルーチンを定め、実際に用いたときに何が起こるか確認することである。 （もちろん、わずかな手間がかかるだけなら、全ての方法を試せばよい。）\n\n\n15.2.3.2 概念(I): 行列の階数\n「ちょっと待て！」上述の説明書の一部から、最初にすべきことは階数ではなく、 条件数、つまり行列が「良条件」 であると思うかもしれない。 確かに条件数は重要で、この後すぐにこの問題に戻る。 でも、もっと基本的にことが起きている。 実際に「目に飛び込んでくる」ものではないことが。\nlinalg_lstsq() について参照してる、LAPACKの説明書の一部に重要な情報がある。 四つのルーチン GELS 、 GELSY 、 GELSD 、 GELSS の間て、違いは実装に限らない。 最適化の目的も異なっている。 根拠は次の通りだ。 一貫して、行列は行が列よりも多い（観測が特徴量より多い通常の場合）と仮定する。\n\n行列が完全階数、つまり列が線型独立なら「完璧な」回は存在しない。 問題は優決定である。 できることは、最良の近似を見つけることだ。 これは予測誤差を最小化することである。 この点は正規方程式について議論するときに再検討する。 予測誤差の最小化が GELS が行うことで、 GELS が予測変数の完全階数の行列があるときに使うべきルーチンである。\n行列が完全階数でない場合、問題は劣決定である。 このとき、解は無数にある。 残りの全てのルーチン GELSY 、 GELSD 、 GELSS はこの解に適している。 計算の進め方は異なっているが、同一の方法をとる。 これは GELS のものとは異なる。 予測誤差だけではなく、 加えて 係数のベクトルも最小化する。 これは最小ノルム最小二乗解を見つけることである。\n\nまとめると GELS （完全階数行列向け）と GELSY 、 GELSD 、 GELSS の三種（階数不足の向け行列向け）は意図的に異なる最適化の基準に従っている。\nさて、linalg_lstsq() の説明書に従って、 driver が明示的に渡されていないときは GELSY が呼び出される。 これは行列が階数不足のときに適切なはずだが、そうなっているだろうか。\n\nlinalg_matrix_rank(A_alt)\n\ntorch_tensor\n21\n[ CPULongType{} ]\n\n\n行列は21列あり、階数は21なら、完全階数である。 呼び出すべきは確実に GELS ルーチンだ。\n\n\n15.2.3.3 正しい linalg_lstsq() の呼び出し方\n何を driver に渡すか分かったので、呼び出しを修正する。\n\nx_lstsq_alt &lt;- linalg_lstsq(\n  A_alt, b_alt,\n  driver = \"gels\"\n)$solution\n\nall_preds_alt$lstsq &lt;- as.matrix(A_alt$matmul(x_lstsq_alt))\nall_errs_alt$lstsq &lt;- rmse(\n  all_preds_alt$b, all_preds_alt$lstsq\n)\n\nall_errs_alt\n\n        lm    lstsq\n1 127.0765 127.9489\n\n\n今度はそれぞれのRMSEが非常に近くなった。 標準化された行列を使ったときは、なぜFortranのルーチンを指定する必要がなかったのか疑問に思うだろう。\n\n\n15.2.3.4 標準化が役立つ理由\n用いた行列に対して、標準化がしたことは特異値が範囲をかなり狭めたということだ。 標準化された行列 A では、特異値の最大値は最小値の約10倍である。\n\nsvals_normalized_A &lt;- linalg_svdvals(A) / linalg_svdvals(A)[1]\nsvals_normalized_A |&gt; as.numeric()\n\n [1] 1.0000000 0.7473212 0.5929527 0.5233989 0.5188765 0.4706139 0.4391665\n [8] 0.4249272 0.4034658 0.3815900 0.3621314 0.3557949 0.3297923 0.2707912\n[15] 0.2489560 0.2229860 0.2175170 0.1852890 0.1627083 0.1553169 0.1075778\n\n\n一方 A_alt では約100万倍大きい。\n\nsvals_normalized_A_alt &lt;- linalg_svdvals(A_alt) / linalg_svdvals(A_alt)[1]\nsvals_normalized_A_alt |&gt; as.numeric()\n\n [1] 1.000000e+00 1.014368e-02 6.407313e-03 2.881967e-03 2.236537e-03\n [6] 9.633786e-04 6.678379e-04 3.988165e-04 3.584047e-04 3.137257e-04\n[11] 2.699152e-04 2.383504e-04 2.234152e-04 1.803385e-04 1.625245e-04\n[16] 1.300102e-04 4.312534e-05 3.463854e-05 1.964121e-05 1.689914e-05\n[21] 8.419588e-06\n\n\nこれがなぜ重要なのか。 ここでついに 条件数 に話が戻る。\n\n\n15.2.3.5 概念 (II): 条件数\nいわゆる 条件数 が大きいほど、計算の際に数値安定性の問題が生じる可能性が高くなる。 torch では、 linalg_cond() を用いて条件数を計算できる。 A と A_alt の条件数をそれぞれ比較しよう。\n\nlinalg_cond(A)\n\ntorch_tensor\n9.2956\n[ CPUFloatType{} ]\n\nlinalg_cond(A_alt)\n\ntorch_tensor\n118771\n[ CPUFloatType{} ]\n\n\nかなりの差だ。 どこから来たのか。\n条件数は A の行列ノルムをその逆行列のノルムで割ったものとして定義される。 様々な種類のノルムが用いられるが、既定は2ノルムである。 その場合、条件数は行列の特異値はから計算できる。 つまり、 A の2ノルムは最大の特異値であり、その逆行列のものは最小値で与えられる。\n以前のように linalg_svdvals() でこれを確認できる。\n\nlinalg_svdvals(A)[1] / linalg_svdvals(A)[21]\n\ntorch_tensor\n9.2956\n[ CPUFloatType{} ]\n\nlinalg_svdvals(A_alt)[1] / linalg_svdvals(A_alt)[21]\n\ntorch_tensor\n118771\n[ CPUFloatType{} ]\n\n\n繰り返しになるが、これはかなりの違いだ。 同時に A_alt の場合、 linalg_lstsq() に対するRMSEが lm() に対するものよりもわずかに悪くなっていることをご記憶だろうか。 適切なルーチン GELS を使ったにもかかわらず。 どちらも基本的に同一のアルゴリズム（すぐに説明するQR分解）を用いているとすると、数値誤差が A_alt の高い条件数により生じている可能性が高い。\nここまでで、torch の linalg の部分を使うと、最もよく使われている最小二乗アルゴリズムがどのように動いているか理解することに役立つごとに納得してもらえたと思う。 これらに馴染もう。\n\n\n\n15.2.4 最小二乗法 (III): 正規方程式\n目的を示すことから始める。 特徴量を列に観測を行に格納した行列 \\(\\mathbf{A}\\) 及び観測された結果のベクトル \\(\\mathbf{b}\\) が与えられたとき、各特徴量について \\(\\mathbf{b}\\) を最もよく近似する回帰係数を一つ求めたい。 回帰係数のベクトルを \\(\\mathbf{x}\\) とする。 求めるには、連立方程式を解く必要があり、その行列表記は次のようになる。\n\\[\n\\mathbf{Ax} = \\mathbf{b}\n\\]\n\\(\\mathbf{A}\\) が正方行列なら、解を直接求めることができて、\\(\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}\\) となる。 しかし、これはほとんど不可能である。 予測変数よりも多くの観測が（おそらく）常にある。 別の方法が必要である。\n\\(\\mathbf{A}\\) の列を \\(\\mathbf{b}\\) の近似に使う場合、この近似は \\(\\mathbf{A}\\) の列空間に存在しなければならない。 \\(\\mathbf{b}\\) は通常そうではない。 これらは可能な限り近づいてほしい。 つまり、これらの距離を最小化したい。 距離として2ノルムを使うと、目的は次のようになる。\n\\[\n\\text{minimize} \\|\\mathbf{Ax} - \\mathbf{b}\\|\n\\]\n距離は（二乗された）予測誤差のベクトルの長さである。 このベクトルは \\(\\mathbf{A}\\) と直交していなければならない。\n\\[\n\\mathbf{A}^\\mathrm{T}(\\mathbf{Ax} - \\mathbf{b}) = 0\n\\]\nこれを並び替えるといわゆる 正規方程式 が得られる。\n\\[\n\\mathbf{A}^\\mathrm{T} = \\mathbf{A}^\\mathrm{T}\\mathbf{b}\n\\]\nこれを \\(\\mathbf{x}\\) について解くため、 \\(\\mathbf{A}^\\mathrm{T}\\mathbf{A}\\) の逆行列を計算する。\n\\[\n\\mathbf{x} = (\\mathbf{A}^\\mathbf{T}\\mathbf{A})^{-1}\\mathbf{A}^\\mathbf{T}\\mathbf{b}\n\\]\n\\(\\mathbf{A}^\\mathrm{T}\\mathbf{A}\\) は正方行列である。 逆行列が求まらないかもしれないが、その場合は擬逆行列を代わりに計算すればよい。 例としている問題はその必要はない。 \\(\\mathbf{A}\\) は完全階数なので、 \\(\\mathbf{A}^\\mathrm{T}\\mathbf{A}\\) も完全階数である。\nこのように、正規方程式から \\(\\mathbf{b}\\) を予測する方法が導出された。 これを使って、lm() や linalg_lstsq() を用いて得られたものと比較しよう。\n\nAtA &lt;- A$t()$matmul(A)\nAtb &lt;- A$t()$matmul(b)\ninv &lt;- linalg_inv(AtA)\nx &lt;- inv$matmul(Atb)\n\nall_preds$neq &lt;- as.matrix(A$matmul(x))\nall_errs$neq &lt;- rmse(all_preds$b, all_preds$neq)\n\nall_errs\n\n       lm   lstsq     neq\n1 40.8369 40.8369 40.8369\n\n\n直説法がうまくいくことを確かめたので、より洗練された手法を試す。 四つの行列分解が現れる。 コレスキー、LU、QR、そして特異値分解だ。 目的は、どの場合でも、（擬）逆行列の重い計算を回避することだ。 この計算は全ての手法に共通する。 しかしながら、行列の分解の仕方「だけ」ではなく、どの行列を分解するかも異なる。 これは様々な手法が課す制約と関係している。 大まかに述べると、上に並べた順序は前提条件の程度を反映している。 別の言い方をすれば、後ろほど一般性が高くなる。 関係する制約に応じて、最初の二つ（コレスキーとLU分解）は \\(\\mathbf{A}^\\mathrm{T}\\mathbf{A}\\) に対して行い、後の二つ（QRとSVD）は \\(\\mathbf{A}\\) に直接作用させる。 これらを使うと \\(\\mathbf{A}^\\mathrm{T}\\mathbf{A}\\) を計算する必要がない。\n\n\n15.2.5 最小二乗法 (IV): コレスキー分解\nコレスキー分解では、行列は二つの同じ大きさの三角行列に分解される。一方は他方の転置となっている。 通常次のように表す。\n\\[\n\\mathbf{A} = \\mathbf{LL}^\\mathbf{T}\n\\]\nまたは\n\\[\n\\mathbf{A} = \\mathbf{R}^\\mathbf{T}\\mathbf{R}\n\\] ここで記号 \\(\\mathbf{L}\\) 及び \\(\\mathbf{R}\\) はそれぞれ下三角及び上三角行列である。\nコレスキー分解が可能であるためには、行列は対称かつ正定値でなければならない。 これはかなり強い条件で、実際に満たされることは多くない。 用いている例では \\(\\mathbf{A}\\) は対称ではないので、代わりに \\(\\mathbf{A}^\\mathrm{T}\\mathbf{A}\\) に作用される必要があることが直ち示唆される。 また \\(\\mathbf{A}\\) はすでに正定値なので、 \\(\\mathbf{A}^\\mathrm{T}\\mathbf{A}\\) も正定値であることが分かっている。\ntorch では、コレスキー分解を得るには linalg_cholesky() を用いる。 既定では、呼び出すと下三角行列 \\(\\mathbf{L}\\) が返ってくる。\n\n# AtA = L L_t\nAtA &lt;- A$t()$matmul(A)\nL &lt;- linalg_cholesky(AtA)\n\n\\(\\mathbf{A}\\) が \\(\\mathbf{L}\\) から再構築できるか確認する。\n\nLLt &lt;- L$matmul(L$t())\nlinalg_norm(LLt - AtA, ord = \"fro\")\n\ntorch_tensor\n0.00372015\n[ CPUFloatType{} ]\n\n\nここでは、元の行列と再構築したものとの差に対してフロベニウスノルムを計算した。 フロベニウスノルムは全ての行列の要素について和を取り、平方根を返す。 理論的には、ここでは零を見たいが、数値誤差の存在の下では、結果は分解が十分にうまくいったことを示している。\n\\(\\mathbf{LL}^\\mathrm{T}\\) が \\(\\mathbf{A}^\\mathrm{T}\\mathbf{A}\\) の代わりに得られたことがどのように役立つのか。 ここが魔法が起きるところで、同様な魔法が残りの三つの手法でも働くことを見出すことになる。 着想は、ある分解のために、問題を構成する連立方程式を解くためのより性能の良い方法が得られるということにある。 それはよく分かるのは小さな例だ。\n\\[\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n2 & 3 & 0 \\\\\n3 & 4 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 \\\\\n11 \\\\\n15\n\\end{bmatrix}\n\\]\n最初の行から始めると、直ちに \\(x_1\\) が \\(1\\) であることが分かる。 これが分かると、二番目の行から \\(x_2\\) は \\(3\\) に違いないと計算することは容易だ。 最後の量は \\(x_3\\) は \\(0\\) であることを示している。\nコードでは、 torch_trianglular_solv() を使うと効率的に予測変数の行列が下または上三角行列である連立方程式を効率的に解くことができる。 追加の条件は行列が対称であることであるが、これはコレスキー分解を使うために既に満たされている。\n既定では、 torch_triangular_solve() は上（下ではなく）三角行列を想定するが、函数のパラメタに upper があり、想定を修正することができる。 返り値はリストで、最初の要素が求める解である。 説明のため、 torch_triangular_solv() を、上で暗算で解いたおもちゃの例に適用したものを示す。\n\nsome_L &lt;- torch_tensor(\n  matrix(c(1, 0, 0, 2, 3, 0, 3, 4, 1), nrow = 3, byrow = TRUE)\n)\nsome_b &lt;- torch_tensor(matrix(c(1, 11, 15), ncol = 1))\n\nx &lt;- torch_triangular_solve(\n  some_b,\n  some_L,\n  upper = FALSE\n)[[1]]\nx\n\ntorch_tensor\n 1\n 3\n 0\n[ CPUFloatType{3,1} ]\n\n\n現在の例に戻ると、正規方程式は次のようになる。\n\\[\n\\mathbf{LL}^\\mathrm{T}\\mathbf{x} = \\mathbf{A}^\\mathrm{T}\\mathbf{b}\n\\]\n新しい変数 \\(\\mathbf{y}\\) を \\(\\mathbf{L}^\\mathrm{T}\\mathbf{x}\\) を表すために導入する。\n\\[\n\\mathbf{Ly} = \\mathbf{A}^\\mathrm{T}\\mathbf{b}\n\\] そして この 系の解を求める。\n\nAtb &lt;- A$t()$matmul(b)\n\ny &lt;- torch_triangular_solve(\n  Atb$unsqueeze(2),\n  L,\n  upper = FALSE\n)[[1]]\n\n\\(\\mathbf{y}\\) がもとまったので、これがどのように定義されていたか振り返る。\n\\[\n\\mathbf{y} = \\mathbf{L}^\\mathrm{T}\\mathbf{x}\n\\]\n\\(\\mathbf{x}\\) を定めるには、また torch_triangular_solve() を用いる。\n\nx &lt;- torch_triangular_solve(y, L$t())[[1]]\n\nこれで完了。\n通常通り、誤差を計算する。\n\nall_preds$chol &lt;- as.matrix(A$matmul(x))\nall_errs$chol &lt;- rmse(all_preds$b, all_preds$chol)\n\nall_errs\n\n       lm   lstsq     neq    chol\n1 40.8369 40.8369 40.8369 40.8369\n\n\nコレスキー分解の理論的根拠を理解した。 既に述べたが、この考え方は他の全ての分解にも適用される。 実は、専用の便利な函数 torch_cholesky_solve を利用すると作業を節約できる。 これを使うと、二つの torch_triangular_solve() の呼び出しが不要になる。\n\nL &lt;- linalg_cholesky(AtA)\n\nx &lt;- torch_cholesky_solve(Atb$unsqueeze(2), L)\n\nall_preds$chol2 &lt;- as.matrix(A$matmul(x))\nall_errs$chol2 &lt;- rmse(all_preds$b, all_preds$chol2)\nall_errs\n\n       lm   lstsq     neq    chol   chol2\n1 40.8369 40.8369 40.8369 40.8369 40.8369\n\n\n次の手法、つまり次の分解に進もう。\n\n\n15.2.6 最小二乗法 (V): LU分解\nLU分解という名前は、分解により得られる二つの因子に由来している。 下三角行列 \\(\\mathbf{L}\\) と上三角行列 \\(\\mathbf{U}\\) である。 理論的には、LU分解に制約はない。 つまり、行の交換を許して、実質的に \\(\\mathbf{A} = \\mathbf{LU}\\) を \\(\\mathbf{A} = \\mathbf{PLU}\\) （ここで \\(\\mathbf{P}\\) は置換行列）とすれば、どんな行列も分解できる。\n実際には、 torch_triangular_solve() を使う場合は、入力する行列は対称でなければならない。 そのため、 \\(\\mathbf{A}\\) ではなく、 \\(\\mathbf{A}^\\mathrm{T}\\mathbf{A}\\) を使うことになる。 （そのため、LU分解をコレスキーの直後に説明している。これらは使い方が似ているものの、考え方は全く異なる。）\n\\(\\mathbf{A}^\\mathrm{T}\\mathbf{A}\\) を使うということは、この方法も正規方程式から出発することを意味する。 \\(\\mathbf{A}^\\mathrm{T}\\mathbf{A}\\) を分解し、二つの三角行列を解いて、最終的な解を得る。 ここで手順を示す。 常に必要とは限らない置換行列 \\(\\mathbf{P}\\) も含まれている。\n\\[\n\\begin{aligned}\n\\mathbf{A}^\\mathrm{T}\\mathbf{Ax} &= \\mathbf{A}^\\mathrm{T}\\mathbf{b}\\\\\n\\mathbf{PLUx} &= \\mathbf{A}^\\mathrm{T}\\mathbf{b}\\\\\n\\mathbf{Ly} &= \\mathbf{P}^\\mathrm{T}\\mathbf{A}\\mathrm{T}\\mathbf{b}\\\\\n\\mathbf{y} &= \\mathbf{Ux}\n\\end{aligned}\n\\]\n\\(\\mathbf{P}\\) が必要な場合、追加の計算が生じる。 コレスキーで使った方法と同様に \\(\\mathbf{P}\\) を左辺から右辺に移したい。 幸いにも、大変そうな逆行列の計算は重くない。 置換行列に対しては、その転置が逆演算となる。\nコード上は、しなければならないことは既に馴染みのあるものだ。 てできてないのは torch_lu() だけだ。 torch_lu()は二つのテンソルのリストを返す。 最初の要素は三つの行列 \\(\\mathbf{P}\\) 、 \\(\\mathbf{L}\\) 、 \\(\\mathbf{U}\\)で、二つ目はピボットだ。 torch_lu_unpack() で三つの行列に展開できる。\n\nlu &lt;- torch_lu(AtA)\nplu &lt;- torch_lu_unpack(lu[[1]], lu[[2]])\nnames(plu) &lt;- c(\"P\", \"L\", \"U\")\n\n\\(\\mathbf{P}\\) を右辺に移す。\n\nAtb &lt;- plu$P$t()$matmul(Atb)\n\nあとは、二つの三角行列を解けば完了する。\n\ny &lt;- torch_triangular_solve(\n  Atb$unsqueeze(2),\n  plu$L,\n  upper = FALSE\n)[[1]]\nx &lt;- torch_triangular_solve(y, plu$U)[[1]]\n\nall_preds$lu &lt;- as.matrix(A$matmul(x))\nall_errs$lu &lt;- rmse(all_preds$b, all_preds$lu)\nall_errs\n\n       lm   lstsq     neq    chol   chol2      lu\n1 40.8369 40.8369 40.8369 40.8369 40.8369 40.8369\n\n\nコレスキー分解と同様に、torch_triangular_solve() を二回呼び出す手間を省ける。 torch_lu_solve() は分解を取り、直接最終的な解を返す。\n\nlu &lt;- torch_lu(AtA)\nx &lt;- torch_lu_solve(Atb$unsqueeze(2), lu[[1]], lu[[2]])\n\nall_preds$lu2 &lt;- as.matrix(A$matmul(x))\nall_errs$lu2 &lt;- rmse(all_preds$b, all_preds$lu2)\nall_errs\n\n       lm   lstsq     neq    chol   chol2      lu     lu2\n1 40.8369 40.8369 40.8369 40.8369 40.8369 40.8369 40.8369\n\n\n次に、\\(\\mathbf{A}^\\mathrm{T}\\mathbf{A}\\) の計算が必要ない二つの方法を見る。\n\n\n15.2.7 最小二乗法 (VI): QR分解\nどんな行列も直交行列 \\(\\mathbf{Q}\\) と上三角行列 \\(\\mathbf{R}\\) に分解できる。 QR分解は、最小二乗問題の解法としておそらく最もよく使われており、Rの lm() にも採用されている。 どのようにして、問題を簡単にしているのか。\n\\(\\mathbf{F}\\) については、どのように便利かを既に述べた。 三角行列により、代入だけで順に解くことができる連立方程式が得られる。 \\(\\mathbf{Q}\\) はもっと便利である。 直交行列は、列が直交するものを指す。 これが意味するのは、相互内積が全て0で、ノルムが1であるということだ。 そのため、こののような行列はその逆行列が転置に等しい。 一般的には、逆行列は計算が困難である一方、転置は容易である。 逆行列を計算し、 \\(\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}\\) を解くことが最小二乗法の中心的な問題であることを考慮すると、その重要性は明らかだ。\nこれまでの方法と比較して、手順は少し短くなる。 ダミー変数 \\(\\mathbf{y}\\) はもう不要だ。 代わりに \\(\\mathbf{Q}\\) を右辺に移し、転置（逆行列）を求める。 残りは、後退代入だけだ。 また、全ての行列に対してQR分解が存在するので、 \\(\\mathbf{A}\\) から直接始め、\\(\\mathbf{A}\\mathrm{T}\\mathbf{A}\\)は不要だ。\n\\[\n\\begin{aligned}\n\\mathbf{Ax} &= \\mathbf{b}\\\\\n\\mathbf{QRx} &= \\mathbf{b}\\\\\n\\mathbf{Rx} &= \\mathbf{Q}^\\mathrm{T}\\mathbf{b}\n\\end{aligned}\n\\]\ntorch では linalg_qr() が行列 \\(\\mathbf{Q}\\) と\\(\\mathbf{R}\\) を計算する。\n\nqr &lt;- linalg_qr(A)\nnames(qr) &lt;- c(\"Q\", \"R\")\n\n右辺に \\(\\mathbf{A}\\mathrm{T}\\mathbf{b}\\) を格納した「便宜的な」変数を用いてきたがこの手順は省略でき、変わりに「すぐに役立つ」ことをする。 つまり\\(\\mathbf{Q}\\) を右辺に移す。\n\nQtb &lt;- qr$Q$t()$matmul(b)\n\n残りの手順は、三角行列を解くことだけだ。\n\nx &lt;- torch_triangular_solve(Qtb$unsqueeze(2), qr$R)[[1]]\n\nall_preds$qr &lt;- as.matrix(A$matmul(x))\nall_errs$qr &lt;- rmse(all_preds$b, all_preds$qr)\nall_errs[1, -c(5,7)]\n\n       lm   lstsq     neq    chol      lu      qr\n1 40.8369 40.8369 40.8369 40.8369 40.8369 40.8369\n\n\nここまで読んでくると、節の終わりに「専用の函数が torch / torch_linalg にある…」という記述を期待しているかもしれない。 文字通りのものはないが、実質上はある。 linalg_lstsq() を呼び出すときに driver = \"gels\" を渡すと、QR分解が使われる。\n\n\n15.2.8 最小二乗法 (VII): 特異値分解（SVD）\nまさに、頂点に向かう順序で、最後に説明する分解手法は万能で、用途が広く、意味論において価値がある、特異値分解（SVD: singular value decomposition）である。 三つ目の側面は、興味深いものの、現在の問題に関係しないので省略する。 ここでは、汎用性が重要である。 全ての行列は、SVDの形式に分解して構成することができる。\n特異値分解は、入力 \\(\\mathbf{A}\\) を二つの直交行列 \\(\\mathbf{U}\\) と \\(\\mathbf{V}^\\mathrm{T}\\) と対角行列 \\(\\boldsymbol{\\Sigma}\\) に分解し、 \\(\\mathbf{A} = \\mathbf{U}\\boldsymbol{\\Sigma}\\mathbf{V}^\\mathrm{T}\\) という形にする。 ここで、 \\(\\mathbf{U}\\) と \\(\\mathbf{V}^\\mathrm{T}\\) は 左 と 右 特異ベクトルであり、 \\(\\boldsymbol{\\Sigma}\\) には特異値が格納されている。\n\\[\n\\begin{aligned}\n\\mathbf{Ax} &= \\mathbf{b}\\\\\n\\mathbf{U}\\boldsymbol{\\Sigma}\\mathbf{V}^\\mathrm{T}\\mathbf{x} &= \\mathbf{b}\\\\\n\\boldsymbol{\\Sigma}\\mathbf{V}^\\mathrm{T}\\mathbf{x} &= \\mathbf{U}^\\mathrm{T}\\mathbf{b}\\\\\n\\mathbf{V}^\\mathrm{T}\\mathbf{x} &= \\mathbf{y}\n\\end{aligned}\n\\]\n行列の分解を linalg_svd() を用いて求めることから始める。 引数 full_matrices = FALSE は torch に \\(\\mathbf{U}\\) が \\(\\mathbf{A}\\) と同じ次元であることを伝え、7588 x 7588に展開されるのを防ぐ。\n\nsvd &lt;- linalg_svd(A, full_matrices = FALSE)\nnames(svd) &lt;- c(\"U\", \"S\", \"Vt\")\n\ndim(svd$U)\n\n[1] 7588   21\n\ndim(svd$S)\n\n[1] 21\n\ndim(svd$Vt)\n\n[1] 21 21\n\n\n\\(\\mathbf{U}\\)を右辺に移す。 これは \\(\\mathbf{U}\\) が直交行列なので軽い計算である。\n\nUtb &lt;- svd$U$t()$matmul(b)\n\n\\(\\mathbf{U}^\\mathrm{T}\\mathbf{b}\\) と \\(\\boldsymbol\\Sigma\\) は同じ長さのベクトルなので、要素毎の積を使って \\(\\boldsymbol\\Sigma\\)　を右辺に移すことができる。 一時変数 y に結果を保存する。\n\ny &lt;- Utb / svd$S\n\n最後に残ったのは、方程式系 \\(\\mathbf{V}^\\mathrm{T}\\mathbf{x} = \\mathbf{y}\\) を解くことだが、今回は行列 \\(\\mathbf{V}\\) に関して再び直交性が役立つ。\n\nx &lt;- svd$Vt$t()$matmul(y)\n\nまとめに予測値と予測誤差を計算しよう。\n\nall_preds$svd &lt;- as.matrix(A$matmul(x))\nall_errs$svd &lt;- rmse(all_preds$b, all_preds$svd)\n\nall_errs[1, -c(5,7)]\n\n       lm   lstsq     neq    chol      lu      qr     svd\n1 40.8369 40.8369 40.8369 40.8369 40.8369 40.8369 40.8369\n\n\nこれで、重要な最小二乗アルゴリズムが一巡した。 この例の仕上げに、簡単に性能を見ておこう。\n\n\n15.2.9 実行時間の比較\n既に述べたように、本章の主眼は概念であり、性能ではない。 しかしより大きなデータセットを扱い始めると、スピードが気になることは避けられない。 また、これらの手法がどのくらい速いか確認すること自体は、興味深い。 ということで、簡単な性能比較をしよう。 ただし、結果を外挿しないように。 扱うデータに対して、類似のコードを実行すること。\n時間計測のために各アルゴリズムを対応する函数にまとめる。\n\n# 正規方程式\nls_normal_eq &lt;- function(A, b) {\n  AtA &lt;- A$t()$matmul(A)\n  x &lt;- linalg_inv(AtA)$matmul(A$t())$matmul(b)\n  x\n}\n\n# 正規方程式にコレスキー分解（自作）\n# A_t A x = A_t b\n# L L_t x = A_t b\n# L y = A_t b\n# L_t x = y\nls_cholesky_diy &lt;- function(A, b) {\n  AtA &lt;- A$t()$matmul(A)\n  Atb &lt;- A$t()$matmul(b)\n  L &lt;- linalg_cholesky(AtA)\n  y &lt;- torch_triangular_solve(\n    Atb$unsqueeze(2),\n    L,\n    upper = FALSE\n  )[[1]]\n  x &lt;- torch_triangular_solve(y, L$t())[[1]]\n  x\n}\n\n# torchのコレスキー分解\nls_cholesky_solve &lt;- function(A, b) {\n  AtA &lt;- A$t()$matmul(A)\n  Atb &lt;- A$t()$matmul(b)\n  L &lt;- linalg_cholesky(AtA)\n  x &lt;- torch_cholesky_solve(Atb$unsqueeze(2), L)\n  x\n}\n\n# 正規方程式にLU分解（自作）\n# A_t A x = A_t b\n# P L U x = A_t b\n# L y = P_t A_t b\n# U x = y\nls_lu_diy &lt;- function(A, b) {\n  AtA &lt;- A$t()$matmul(A)\n  Atb &lt;- A$t()$matmul(b)\n  lu &lt;- torch_lu(AtA)\n  plu &lt;- torch_lu_unpack(lu[[1]], lu[[2]])\n  names(plu) &lt;- c(\"P\", \"L\", \"U\")\n  Atb &lt;- plu$P$t()$matmul(Atb)\n  y &lt;- torch_triangular_solve(\n    Atb$unsqueeze(2),\n    plu$L,\n    upper = FALSE\n  )[[1]]\n  x &lt;- torch_triangular_solve(y, plu$U)[[1]]\n  x\n}\n\n# torchのLU分解\nls_lu_solve &lt;- function(A, b) {\n  AtA &lt;- A$t()$matmul(A)\n  Atb &lt;- A$t()$matmul(b)\n  lu &lt;- torch_lu(AtA)\n  x &lt;- torch_lu_solve(Atb$unsqueeze(2), lu[[1]], lu[[2]])\n  x\n}\n\n# QR分解\n# A x = b\n# Q R x = b\n# R x = Q_t b\nls_qr &lt;- function(A, b) {\n  qr &lt;- linalg_qr(A)\n  names(qr) &lt;- c(\"Q\", \"R\")\n  Qtb &lt;- qr$Q$t()$matmul(b)\n  x &lt;- torch_triangular_solve(Qtb$unsqueeze(2), qr$R)[[1]]\n  x\n}\n\n# SVD\n# A x = b\n# U S V_t x = b\n# S V_t x = U_t b\n# S y = U_t b\n# V_t x = y\nls_svd &lt;- function(A, b) {\n  svd &lt;- linalg_svd(A, full_matrices = FALSE)\n  names(svd) &lt;- c(\"U\", \"S\", \"Vt\")\n  Utb &lt;- svd$U$t()$matmul(b)\n  y &lt;- Utb / svd$S\n  x &lt;- svd$Vt$t()$matmul(y)\n  x\n}\n\nmircrobenchmark バッケージを使って、これらの手法を計測する。\n\nlibrary(microbenchmark)\n\nset.seed(777)\ntorch_manual_seed(777)\n\nres &lt;- microbenchmark(\n  ls_normal_eq(A, b),\n  ls_cholesky_diy(A, b),\n  ls_cholesky_solve(A, b),\n  ls_lu_diy(A, b),\n  ls_lu_solve(A, b),\n  ls_qr(A, b),\n  ls_svd(A, b),\n  linalg_lstsq(A, b),\n  times = 1000,\n  unit = \"ms\"\n)\n\nWarning in microbenchmark(ls_normal_eq(A, b), ls_cholesky_diy(A, b),\nls_cholesky_solve(A, : less accurate nanosecond times to avoid potential\ninteger overflows\n\nylabs &lt;- c(\"NE\", \"CHD\", \"CHS\", \"LUD\", \"LUS\", \"QR\", \"SVD\", \"LST\")\n\nboxplot(res, unit = \"us\", horizontal = TRUE, xlab = \"method\",\n     main = \"Least Squares Benchmark\", yaxt = \"n\")\naxis(side = 2, las = 2, at = 1:length(ylabs), labels = ylabs)\n\n\n\n\n\n\n\n\n結論として、様々な方法で行列を分解することが、最小二乗問題を解くために役立つことが分かった。 また、これらの手法の時間を計測する方法を簡単に示したが、スピードだけが重要ではない。 同時に、解が信頼できるものであることも望まれる。 これを専門用語では 数値安定性 と呼ぶ。",
    "crumbs": [
      "他にも`torch` でできること: 行列計算・フーリエ変換・ウェーブレット",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>行列演算: 最小二乗問題</span>"
    ]
  },
  {
    "objectID": "leastsquares.html#簡単な数値安定性の確認",
    "href": "leastsquares.html#簡単な数値安定性の確認",
    "title": "15  行列演算: 最小二乗問題",
    "section": "15.3 簡単な数値安定性の確認",
    "text": "15.3 簡単な数値安定性の確認\n既に条件数について議論した。 数値安定性の概念は、考え方としては類似しているが、行列ではなくアルゴリズムを参照している。 どちらの場合も、考え方は入力における微小な変化は出力においても微小であるべきだというものだ。 この話題について一冊全体割いた本がいくつもあるので、詳細に立ち入ることは避ける。2\n代わりに、悪条件な最小二乗問題の例を用いる。 つまり、行列が悪条件となっている。 この例を用いて、既に議論したアルゴリズムの数値安定性について理解する。3\n予測変数の行列は、悪条件になるように作られた100 x 15のファンデルモンド行列である。\n\nset.seed(777)\ntorch_manual_seed(777)\n\nm &lt;- 100\nn &lt;- 15\nt &lt;- torch_linspace(0, 1, m)$to(dtype = torch_double())\n\nA &lt;- torch_vander(t, N = n, increasing = TRUE)$to(\n  dtype = torch_double()\n)\n\n条件数はとても大きい。\n\nlinalg_cond(A)\n\ntorch_tensor\n2.27178e+10\n[ CPUDoubleType{} ]\n\n\n次に、予測対象を作成する。\n\nb &lt;- torch_exp(torch_sin(4 * t))\nb &lt;- b / 2006.787453080206\n\n上記の最小二乗問題では全ての手法で同じRMSEを得た。 今回の問題では興味深いことが起きる。 以前に示したもののうち、「自作」函数だけに絞る。 便宜上、再掲する。\n\n# 正規方程式\nls_normal_eq &lt;- function(A, b) {\n  AtA &lt;- A$t()$matmul(A)\n  x &lt;- linalg_inv(AtA)$matmul(A$t())$matmul(b)\n  x\n}\n\n# 正規方程式にコレスキー分解（自作）\n# A_t A x = A_t b\n# L L_t x = A_t b\n# L y = A_t b\n# L_t x = y\nls_cholesky_diy &lt;- function(A, b) {\n  # 単位行列に微小な係数を掛けて\n  # 数値不安定に対処する。\n  # コレスキー分解が失敗する場合はepsを大きくする。\n  eps &lt;- 1e-10\n  id &lt;- eps * torch_diag(torch_ones(dim(A)[2]))\n  AtA &lt;- A$t()$matmul(A) + id\n  Atb &lt;- A$t()$matmul(b)\n  L &lt;- linalg_cholesky(AtA)\n  y &lt;- torch_triangular_solve(\n    Atb$unsqueeze(2),\n    L,\n    upper = FALSE\n  )[[1]]\n  x &lt;- torch_triangular_solve(y, L$t())[[1]]\n  x\n}\n# 正規方程式にLU分解（自作）\n# A_t A x = A_t b\n# P L U x = A_t b\n# L y = P_t A_t b\n# U x = y\nls_lu_diy &lt;- function(A, b) {\n  AtA &lt;- A$t()$matmul(A)\n  Atb &lt;- A$t()$matmul(b)\n  lu &lt;- torch_lu(AtA)\n  plu &lt;- torch_lu_unpack(lu[[1]], lu[[2]])\n  names(plu) &lt;- c(\"P\", \"L\", \"U\")\n  Atb &lt;- plu$P$t()$matmul(Atb)\n  y &lt;- torch_triangular_solve(\n    Atb$unsqueeze(2),\n    plu$L,\n    upper = FALSE\n  )[[1]]\n  x &lt;- torch_triangular_solve(y, plu$U)[[1]]\n  x\n}\n\n# torchのLU分解\nls_lu_solve &lt;- function(A, b) {\n  AtA &lt;- A$t()$matmul(A)\n  Atb &lt;- A$t()$matmul(b)\n  lu &lt;- torch_lu(AtA)\n  x &lt;- torch_lu_solve(Atb$unsqueeze(2), lu[[1]], lu[[2]])\n  x\n}\n\n# QR分解\n# A x = b\n# Q R x = b\n# R x = Q_t b\nls_qr &lt;- function(A, b) {\n  qr &lt;- linalg_qr(A)\n  names(qr) &lt;- c(\"Q\", \"R\")\n  Qtb &lt;- qr$Q$t()$matmul(b)\n  x &lt;- torch_triangular_solve(Qtb$unsqueeze(2), qr$R)[[1]]\n  x\n}\n\n# SVD\n# A x = b\n# U S V_t x = b\n# S V_t x = U_t b\n# S y = U_t b\n# V_t x = y\nls_svd &lt;- function(A, b) {\n  svd &lt;- linalg_svd(A, full_matrices = FALSE)\n  names(svd) &lt;- c(\"U\", \"S\", \"Vt\")\n  Utb &lt;- svd$U$t()$matmul(b)\n  y &lt;- Utb / svd$S\n  x &lt;- svd$Vt$t()$matmul(y)\n  x\n}\n\nそれでは実行してみよう。\n\nalgorithms &lt;- c(\n  \"ls_normal_eq\",\n  \"ls_cholesky_diy\",\n  \"ls_lu_diy\",\n  \"ls_qr\",\n  \"ls_svd\"\n)\n\nrmses &lt;- lapply(\n  algorithms,\n  function(m) {\n    rmse(\n      as.numeric(b),\n      as.numeric(A$matmul(get(m)(A, b)))\n    )\n  }\n)\n\nrmse_df &lt;- data.frame(\n  algorithm = algorithms,\n  rmse = unlist(rmses)\n)\n\nrmse_df\n\n        algorithm         rmse\n1    ls_normal_eq 4.730447e-04\n2 ls_cholesky_diy 3.013914e-06\n3       ls_lu_diy 1.170186e-07\n4           ls_qr 3.436748e-08\n5          ls_svd 3.436749e-08\n\n\nこれはかなり興味深い！ 正規方程式は簡単だが、問題が良条件でなくなった場合には良い選択ではないかもしれないことが明らかだ。 コレスキーもLU分解もかなり改善されているが、明らかな「勝者」はQR分解とSVDだ。 これら二つ（とそれぞれの派生型）が linalg_lstsq() で使われているのは当然だ。\n\n\n\n\nCho, D., C. Yoo, J. Im, and D.-H. Cha, 2020: Comparative assessment ofvarious machine learning-based bias correction methods for numerical weather prediction model forecasts of extreme air temperatures in urban areas. Earth and Space Science, 7, e2019EA000740, https://doi.org/10.1029/2019EA000740.\n\n\nTrefethen, L. N., and D. Bau, 1997: Numerical linear algebra. SIAM,.",
    "crumbs": [
      "他にも`torch` でできること: 行列計算・フーリエ変換・ウェーブレット",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>行列演算: 最小二乗問題</span>"
    ]
  },
  {
    "objectID": "leastsquares.html#footnotes",
    "href": "leastsquares.html#footnotes",
    "title": "15  行列演算: 最小二乗問題",
    "section": "",
    "text": "上で引用した driver の説明は、基本的にLAPACKの説明書 からの抜粋である。 問題のページは便利のため linalg_lstsq() の説明からリンクされているので、これを確認することは容易だ。↩︎\n詳しくは、そのような本の一つ、例えば、広く使われ（かつ簡明な） Trefethen and Bau (1997) の記述を参照。↩︎\nこの例は上の脚注にある Trefethen and Bau (1997) の本から取った。Computational Linear Algebra for Code （プログラマのための数値線型代数）の講義資料を通じて、この例を教えてくれたRachel Thomasに感謝する。↩︎",
    "crumbs": [
      "他にも`torch` でできること: 行列計算・フーリエ変換・ウェーブレット",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>行列演算: 最小二乗問題</span>"
    ]
  },
  {
    "objectID": "convolution.html",
    "href": "convolution.html",
    "title": "16  行列計算: 畳み込み",
    "section": "",
    "text": "16.1 畳み込みの意義\n信号処理では、 フィルタ を用いることにより、信号を変えて目的の形、例えば高振動数が除去されたものにする。 手元に、時系列をフーリエ変換した表現、つまり振動数毎に大きさと位相があるとする。 ある閾値より高い振動数をを除去したいとしよう。 最も簡単な方法は、振動数のデータに1と0の列を掛けることだ。 このようにすると、フィルタの作用は振動数領域で生じ、これが断然最も便利な方法だ。\nもし、同様の結果を時間領域で得たい、つまり生の時系列データを扱うとしたらどうするか。 その場合、フィルタの時間領域での表現（逆フーリエ変換から得られる）を見つけなくてはならない。 そして、この表現を時系列に 畳み込む 。 言い換えれば、時間領域における畳み込みは振動数領域における掛け算に対応する。 この基本的な事実は常に利用される。\nそれでは、畳み込みは何にをするものかや、どのように実装されているかについて、より詳しく理解しよう。 単一の次元から始めて、二次元の場合にどのようになるか、少し探究する。",
    "crumbs": [
      "他にも`torch` でできること: 行列計算・フーリエ変換・ウェーブレット",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>行列計算: 畳み込み</span>"
    ]
  },
  {
    "objectID": "convolution.html#一次元の畳み込み",
    "href": "convolution.html#一次元の畳み込み",
    "title": "16  行列計算: 畳み込み",
    "section": "16.2 一次元の畳み込み",
    "text": "16.2 一次元の畳み込み\n簡単な信号 x と簡単なフィルタ h を作るところから取り掛かる。 変数の名前は思いつきで決めたのではない、信号処理で \\(h\\) は、この後すぐに現れる用語 インパルス 応答を表すのに通常使われる記号だ。\n\nlibrary(torch)\n\nx &lt;- torch_arange(start = 1, end = 4)\nh &lt;- torch_tensor(c(-1, 0, 1))\n\nさて、 torch_conv1d() があるので、これを呼んで何が起こるか見てみよう。 畳み込みの定義によると、出力の長さは入力の長さとフィルタの長さとの和より一つ少ない。 torch_conv1d() を使って、長さ6の出力を得るには、フィルタの長さが3なので、両側に二つずつ広げる必要がある。\n次のコードで、 view() の呼び出しに気を取られてはいけない。 これはただ torch が3次元の入力を必要としているためだ。 最初と次の次元は、いつも通りそれぞれバッチとチャンネルを表す。\n\ntorch_conv1d(\n  x$view(c(1, 1, 4)),\n  h$view(c(1, 1, 3)),\n  padding = 2\n)\n\ntorch_tensor\n(1,.,.) = \n  1  2  2  2 -3 -4\n[ CPUFloatType{1,1,6} ]\n\n\ntorchconv1d() は畳み込みではなく相互相関を計算するのだったのではないか、と考えているかもしれない。 Rには convolve() があるので確認しよう1\n\nx_ &lt;- as.numeric(x)\nh_ &lt;- as.numeric(h)\n\nconvolve(x_, h_, type = \"open\")\n\n[1]  1  2  2  2 -3 -4\n\n\n結果は同じである。 しかしながら、 convolve() の説明を読むと\n\n二つの列 x と y の畳み込みに対する通常の定義は次で与えられることに注意する。 convolve(x, rev(y), type=\"o\")\n\n明らかに、フィルタの要素の順序を逆転させる必要がある。\n\nconvolve(x_, rev(h_), type = \"open\")\n\n[1] -1 -2 -2 -2  3  4\n\n\n確かに、今度の結果は異なっている。 同様なことを torch_conv1d() でやってみよう。\n\ntorch_conv1d(\n  x$view(c(1, 1, 4)),\n  h$flip(1)$view(c(1, 1, 3)),\n  padding = 2\n)\n\ntorch_tensor\n(1,.,.) = \n -1 -2 -2 -2  3  4\n[ CPUFloatType{1,1,6} ]\n\n\nここでも結果は torch とRで同じである。 convolve() の説明の中の “Details”部にある、あの簡潔な記述は相互総観と畳み込みの違いを完璧に捉えている。 畳み込みでは、二つ目の引数は逆順になっている。 信号処理の言葉では、 反転 である。 （「反転」の方がかなり良い用語だ。高次元にも一般化できるからだ。）\n技術的にはさはわずかだ。 単に符号が逆転するだけである。 しかし、数学的には、フィルタが何であり、何をするかに関わるという意味で重要である。 これは、この後説明する。\n畳み込みの基礎となる演算は二つの方法で示すことができる。",
    "crumbs": [
      "他にも`torch` でできること: 行列計算・フーリエ変換・ウェーブレット",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>行列計算: 畳み込み</span>"
    ]
  },
  {
    "objectID": "convolution.html#畳み込みの二つの考え方",
    "href": "convolution.html#畳み込みの二つの考え方",
    "title": "16  行列計算: 畳み込み",
    "section": "16.3 畳み込みの二つの考え方",
    "text": "16.3 畳み込みの二つの考え方\n一つは、単一の出力に着目しどのように得られたか定めるものだ。 つまり、どの入力要素がその値に寄与したか、どのように組み合わされたかを問う。 これは「出力からの見方」と呼ぶことができ、相互相関で馴染みのあるものである。\n相互相関と同様に、次のようにも述べることもできる。 フィルタが画像を「移動」し、個々の画像中の位置（画素）で、周囲の入力画素と対応する「重ねられた」フィルタ値とを掛け算し、全ての画素を足し合わせる。 つまり、個々の出力画素は入力とフィルタ値の組の間の内積の計算から得られる。\n二つ目の考え方は、入力の観点から見たもの（「入力からの見方」と呼ぶ）だ。 問いは、どのように各入力が出力に寄与するかである。 この見方は、最初の見方よりも慣れを要するが、単に馴染み、ニューラルネットワークの文脈でのよくある説明のされ方の問題かもしれない。 どちらにしても、入力からの見方は、畳み込みの数学的な意味について学ぶという点で非常に有益である。\n両方の見方を学ぶ。 まず、馴染みのある、出力からの見方から始める。\n\n16.3.0.1 出力からの見方\n出力からの見方では、torch_conv1d() で padding = 2 を指定したときのように、まず入力信号の両側を延長する。 指定通りに、インパルス応答を反転し、1, 0, -1 とする。 次に、「移動」を示す。\n以下は、表形式でこれを示したものだ（Table 16.1）。\n\n\n\nTable 16.1: 畳み込み: 出力からの見方\n\n\n\n\n\n信号\n反転 IR\n\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n\n\n\n0\n0\n1\n\n\n\n\n\n\n1\n-1\n0\n1\n\n\n\n\n\n2\n\n-1\n0\n1\n\n\n\n\n3\n\n\n-1\n0\n1\n\n\n\n4\n\n\n\n-1\n0\n1\n\n\n0\n\n\n\n\n-1\n0\n\n\n0\n\n\n\n\n\n-1\n\n\n結果\n-1\n-2\n-2\n-2\n3\n4\n\n\n\n\n\n\n結局この話題について既に述べたように、書き出してみた結果に驚くことは何もない。 次に入力からの見方を考える。\n\n\n16.3.0.2 入力からの見方\n入力からの考え方で重要なことは、入力信号をどのように考えるかである。 個々の要素は、拡大縮小され、移動されたインパルスと見なされる。\nインパルス は単位標本（またはインパルス）函数、デルタ（\\(\\delta\\)）で与えられる。 この函数は値が1となる0を除いたすべての位置において0である。\n\\[\n\\delta[n] = \\begin{cases}1,\\;n = 0\\\\0,\\;n \\ne 0\\end{cases}\n\\]\nこれは、クロネッカーのデルタ \\(\\delta_{ij}\\)2 に似ており、一つの添字を 0 に固定したものである。\n\\[\n\\delta[n] = \\delta_{n0} = \\delta_{0n}\n\\]\nつまり、この函数 \\(\\delta[n]\\) だけがあれば、 \\(n\\) が、例えば離散時間を表すとすると、厳密に一つの信号値、時刻 \\(n=0\\)3 における値を表すことができ、その唯一とりうる値は 1 である。 今、この操作に拡大縮小と移動を加える。\n\n拡大縮小により、\\(n=0\\) でのいかなる値も作れる。例えば \\(x_0 = 0 * \\delta[n]\\)。\n移動により、他の時刻での値に影響を与えることができる。例えば、時刻 \\(n = 3\\)は \\(n - 3 = 0\\) なので\\(\\delta[n-3] = \\delta_{n3}\\) で与えられる。\n両方を組み合わせることにより、どんな時刻のどんな値でも表すことができる。例えば \\(x_5 = 1.11 * \\delta[n-5]\\)。\n\nこれまで、信号についてだけ考えてきた。 フィルタはどうなるか。 インパルスを特徴づけたのと同様に、フィルタも * インパルス応答* で完全に記述できる[^4]。 インパルス応答は、定義により、入力がインパルスである（つまり、時刻 \\(n=0\\) で生じる）ときに現れるものである。 信号に対して用いた表記にならい、\\(h\\) をインパルス応答とすると次のように書ける。\n\\[\nh[n] = h[n - 0] \\equiv h(\\delta[n-0])\n\\] [^4]: この章ではどこでも、フィルタについて話題にするときは、線型時間不変系だけを考えている。 時間不変系への制約は、畳み込み操作に内在している。\n扱っている例では、数列 -1, 0, 1 となるだろう。 でも信号は0だけでなく、他の時刻においても表すことが必要であるように、フィルタも他の位置でも適用できなければならない。 その目的のためにも、移動操作が用いられ、似た方法で定式化される。 例えば、\\(h[n-1]\\) はフィルタが時刻1、\\(n-1\\)が0となる時刻に適用されることを意味する。 このような移動は、俗に「ずらし」と呼んでいるものに相当する。\nさて、残されている仕事は部品を組み合わせることだ。 時刻 \\(n=0\\) において、移動なしのインパルス応答を受け取り、信号の振幅で拡大縮小する。 例においては、その値は1なので、\\(1 * h[n- 0] = 1 * [-1, 0, 1] = [-1, 0, 1]\\)となる。 他の時刻においては、インパルスオス等を考えている入力位置に移動し掛け算する。 最後に、すべての入力位置からすべての寄与を得たら、足し合わせて畳み込まれた出力を得る。\n次の表は、それを説明するものであるTable 16.2。\n\n\n\nTable 16.2: 畳み込み: 入力からの見方\n\n\n\n\n\n信号\nインパルス入力\n積\n\n\n\n\n1\nh[n - 0]\n-1  0  1  0  0  0\n\n\n2\nh[n - 1]\n0 -2  0  2  0  0\n\n\n3\nh[n - 2]\n0  0 -3  0  3  0\n\n\n4\nh[n - 3]\n0  0  0 -4  0  4\n\n\n和\n\n-1 -2 -2 -2  3  4\n\n\n\n\n\n\n個人的には、出力からの見方の方が理解しやすいものの、入力からの見方から得られる洞察が多いように感じる。 特に、避けられない問い、なぜインパルス応答を反転させるのかに答えている。\n謎の力のようなものからは程遠く、負号は信号の表現のされ方から生じた機械的な結果にすぎない。 時刻 \\(n=2\\) において測定された信号は \\(\\delta[n-2]\\) （2から2を引くと0）と表され、その信号に適用されるフィルタも \\(h[n-2]\\) となる。\n\n\n16.3.1 実装\n出力からの見方についての説明から、どのようにコードにするか述べることはあまりないと思うかもしれない。 入力ベクトルに亙ってループし、期待される出力位置において内積を計算するのは容易に見える。 しかし、これは多数のベクトル積を計算することを意味し、さらに入力が長ければ長いほどその計算が増加する。\n幸運にも、もっと良い方法がある。 単一次元（線型）畳み込みは、対角成分がいくつかの定数で、他の要素が0であるToeplitz行列により計算される。 フィルタがToeplitz行列で定式化れれば、行われる積はToeplitz行列と入力との間の一つだけである。 行列の列数は入力の長さと同じだけ必要である（さもなくば積が計算できない）といっても、行列は「ほぼ空」なので計算量は少ない。\n扱っている例に対して、Toeplitz行列を作ったものを示す。\n\nh &lt;- torch_tensor(\n  rbind(c(-1, 0, 0, 0),\n        c(0, -1, 0, 0),\n        c(1, 0, -1, 0),\n        c(0, 1, 0, -1),\n        c(0, 0, 1, 0),\n        c(0, 0, 0, 1)\n  ))\nh\n\ntorch_tensor\n-1  0  0  0\n 0 -1  0  0\n 1  0 -1  0\n 0  1  0 -1\n 0  0  1  0\n 0  0  0  1\n[ CPUFloatType{6,4} ]\n\n\n例の入力との積が期待通りの結果になるか確認しよう。\n\nh$matmul(x)\n\ntorch_tensor\n-1\n-2\n-2\n-2\n 3\n 4\n[ CPUFloatType{6} ]\n\n\n期待通りになった。 次に二次元の場合を考えよう。 概念的には、差はないが、実際の計算（「自作」と行列の両方）はもっと複雑になる。 つまり、手計算の（一般化可能な）一部を示すことで満足し、計算の部分ではいちいち詳細を示さないことにする。",
    "crumbs": [
      "他にも`torch` でできること: 行列計算・フーリエ変換・ウェーブレット",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>行列計算: 畳み込み</span>"
    ]
  },
  {
    "objectID": "convolution.html#二次元での畳み込み",
    "href": "convolution.html#二次元での畳み込み",
    "title": "16  行列計算: 畳み込み",
    "section": "16.4 二次元での畳み込み",
    "text": "16.4 二次元での畳み込み\n一次元と二次元の畳み込みがどのように類似しているか外見的に示すために、出力からの見方を考える。\n\n16.4.1 仕組み（出力からの見方）\n今回の例では、入力は二次元で、次のようなものである。\n\\[\n\\begin{bmatrix}\n1 & 4 & 1\\\\\n2 & 5 & 3\n\\end{bmatrix}\n\\]\nフィルタについても同様である。 ありうる例を示す。\n\\[\n\\begin{bmatrix}\n1 & 1\\\\\n1 & -1\n\\end{bmatrix}\n\\]\n出力からの見方では、フィルタを入力に対して「ずらす」。 しかし、見やすくするために、一つの出力値（「画素」）に着目する。 入力が m1 x n1 で出力が m2 x n2 なら出力の大きさは (m + m2 -1) x (n1 + n2 -1) である。 つまり、例では 3 x 4 となる。 (0, 1) での値を選ぶ。 画像処理では、通常行を下から数える。\n\\[\n\\begin{bmatrix}\n\\cdot & \\cdot & \\cdot & \\cdot\\\\\n\\cdot & \\cdot & \\cdot & \\cdot\\\\\n\\cdot & y_{01} & \\cdot & \\cdot\n\\end{bmatrix}\n\\]\n存在しない（負）位置を含めた表で入力を示す。\n\n\n\n位置 (x/y)\n-1\n0\n1\n2\n\n\n\n\n1\n\n1\n4\n1\n\n\n0\n\n2\n5\n3\n\n\n-1\n\n\n\n\n\n\n\n対応する位置に値を置いたフィルタは次のように表される。\n\n\n\n位置 (x/y)\n-1\n0\n1\n2\n\n\n\n\n1\n\n1\n1\n\n\n\n0\n\n1\n-1\n\n\n\n-1\n\n\n\n\n\n\n\n一次元の場合のように、まずはフィルタを反転させる。 反転は180°の回転である。\n\n\n\n位置 (x/y)\n-1\n0\n1\n2\n\n\n\n\n1\n\n\n\n\n\n\n0\n-1\n1\n\n\n\n\n-1\n1\n1\n\n\n\n\n\n次にフィルタを必要ない出力位置に移動する。 ここでは右に一つ移動して、上下には移動しない。\n\n\n\n位置 (x/y)\n-1\n0\n1\n2\n\n\n\n\n1\n\n\n\n\n\n\n0\n\n-1\n1\n\n\n\n-1\n\n1\n1\n\n\n\n\n(0, 1)での出力を計算する準備が完了した。 計算は重なっている画像とフィルタ値の内積である。\n\n\n\n位置 (x/y)\n-1\n0\n1\n2\n\n\n\n\n1\n\n\n\n\n\n\n0\n\n-1*2=-2\n1*5=5\n\n\n\n-1\n\n\n\n\n\n\n\n最終結果は -2 + 5 = 3 である。\n\\[\n\\begin{bmatrix}\n\\cdot & \\cdot & \\cdot & \\cdot\\\\\n\\cdot & \\cdot & \\cdot & \\cdot\\\\\n\\cdot & 3   & \\cdot & \\cdot\n\\end{bmatrix}\n\\]\nまだ値のない残りについても同様に計算できる。 その計算は省略して、どのように実際の計算が行われるか見ることにする。\n\n\n16.4.2 実装\n二次元の畳み込みの実装にもToeplitz行列を使う。 すでに述べたように、なぜすべての手順がそのなるか 厳密な形 では示さない。 ここでの意図は扱っている例を示すことであり、その例を踏まえて、必要ならいろいろと探究してほしい。\n\n16.4.2.1 手順1: フィルタ行列の準備\nフィルタを出力の大きさに合わせる。\n0  0 0 0\n1  1 0 0\n1 -1 0 0\n次にフィルタの全ての行について、下からToeplitz行列を作成する。\n# H0\n 1  0  0  \n-1  1  0  \n 0 -1  1  \n 0  0 -1  \n \n# H1\n 1  0  0  \n 1  1  0  \n 0  1  1  \n 0  0  1  \n \n# H2\n 0  0  0  \n 0  0  0  \n 0  0  0  \nコードは次のように書ける。\n\nH0 &lt;- torch_tensor(\n  cbind(\n    c(1, -1, 0, 0),\n    c(0, 1, -1, 0),\n    c(0, 0, 1, -1)\n  )\n)\n\nH1 &lt;- torch_tensor(\n  cbind(\n    c(1, 1, 0, 0),\n    c(0, 1, 1, 0),\n    c(0, 0, 1, 1)\n  )\n)\n\nH2 &lt;- torch_tensor(0)$unsqueeze(1)\n\n次に、これら三つの行列を組み合わせて、次のように 二重ブロックToeplitz 行列 を作る。\nH0   0\nH1  H0\nH2  H1\nこれをコードにする一つの方法は torch_block_diag() を二回使って二つの非0ブロックを作り、 で結合することだ。\n\nH &lt;- torch_cat(\n  list(\n    torch_block_diag(list(H0, H0)), torch_zeros(4, 6)\n  )\n) +\n  torch_cat(\n    list(\n      torch_zeros(4, 6),\n      torch_block_diag(list(H1, H1))\n    )\n  )\n\nH\n\ntorch_tensor\n 1  0  0  0  0  0\n-1  1  0  0  0  0\n 0 -1  1  0  0  0\n 0  0 -1  0  0  0\n 1  0  0  1  0  0\n 1  1  0 -1  1  0\n 0  1  1  0 -1  1\n 0  0  1  0  0 -1\n 0  0  0  1  0  0\n 0  0  0  1  1  0\n 0  0  0  0  1  1\n 0  0  0  0  0  1\n[ CPUFloatType{12,6} ]\n\n\ntorch_tensor\n 1  0  0  0  0  0\n-1  1  0  0  0  0\n 0 -1  1  0  0  0\n 0  0 -1  0  0  0\n 1  0  0  1  0  0\n 1  1  0 -1  1  0\n 0  1  1  0 -1  1\n 0  0  1  0  0 -1\n 0  0  0  1  0  0\n 0  0  0  1  1  0\n 0  0  0  0  1  1\n 0  0  0  0  0  1\n[ CPUFloatType{12,6} ]\n最終的な行列は二つの非0の「バンド」を持ち、二つの全てが0の対角線で分けられている。 この行列は、行列積のために必要なフィルタの最終的な形である。\n\n\n16.4.2.2 手順2: 入力の準備\nこの 12 x 6 行列と掛け算できるように、入力をベクトルに平坦化する必要がある。 再び、行を下から順に進める。\n\nx0 &lt;- torch_tensor(c(2, 5, 3)) \nx1 &lt;- torch_tensor(c(1, 4, 1))\n\nx &lt;- torch_cat(list(x0, x1))\nx\n\ntorch_tensor\n 2\n 5\n 3\n 1\n 4\n 1\n[ CPUFloatType{6} ]\n\n\n\n\n16.4.2.3 手順3: 行列積\nここまでで、畳み込みは単純な行列積に変わった。\n\ny &lt;- H$matmul(x)\ny\n\ntorch_tensor\n  2\n  3\n -2\n -3\n  3\n 10\n  5\n  2\n  1\n  5\n  5\n  1\n[ CPUFloatType{12} ]\n\n\n残るは、出力を正しい二次元の構造に変形することだけだ。 行を（下から）順に組み立てると、次のようになる。\n\\[\n\\begin{bmatrix}\n  1 & 5 & 5 & 1\\\\\n  3 & 10 & 5 & 2\\\\\n  2 & 3 & -2 & -3\\\\\n\\end{bmatrix}\n\\]\n(0, 1) の要素に着目する、手計算の結果と一致していることがわかる。\nここで、 torch の行列計算の話題を終える。 しかし、次の話題であるフーリエ変換は、行列計算から余り離れていない。 フーリエ変換は、時間領域の畳み込みが振動数領域の掛け算に対応することを覚えているだろうか。\nこの対応は、計算の高速化によく利用される。 入力データをフーリエ変換し、結果をフィルタで掛け算し、フィルタされた振動数領域の表現を再び逆変換する。 Rの colvolve() の説明を見てみよう。 出だしにはっきりと書いてある。\n\nフーリエ変換を使って、二つの数列の畳み込みを計算する。\n\nそれでは、フーリエ変換に進もう。",
    "crumbs": [
      "他にも`torch` でできること: 行列計算・フーリエ変換・ウェーブレット",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>行列計算: 畳み込み</span>"
    ]
  },
  {
    "objectID": "convolution.html#footnotes",
    "href": "convolution.html#footnotes",
    "title": "16  行列計算: 畳み込み",
    "section": "",
    "text": "引数 type = \"open\" を渡すことにより、単調で周期的でない畳み込みにすることを指定できる。↩︎\nクロネッカーのデルタは、 \\(i=j\\) のときに1、そうでないときに0となる。↩︎\n\\(n\\) を \\(t\\) の代わりに異なる位置として使っている。 信号その他離散化された値は、時間（あるいは空間）の離散点においてのみ「存在」するからである。 文脈によって、少しぎこちなく見えるかもしれないが少なくとも一貫している。↩︎",
    "crumbs": [
      "他にも`torch` でできること: 行列計算・フーリエ変換・ウェーブレット",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>行列計算: 畳み込み</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "参考文献",
    "section": "",
    "text": "Bronstein, M. M., J. Bruna, T. Cohen, and P. Velickovic, 2021: Geometric deep learning: Grids,\ngroups, graphs, geodesics, and gauges. CoRR,\nabs/2104.13478.\n\n\nCho, D., C. Yoo, J. Im, and D.-H. Cha, 2020: Comparative assessment\nofvarious machine learning-based bias correction methods for numerical\nweather prediction model forecasts of extreme air temperatures in urban\nareas. Earth and Space Science, 7,\ne2019EA000740, https://doi.org/10.1029/2019EA000740.\n\n\nTrefethen, L. N., and D. Bau, 1997: Numerical linear algebra.\nSIAM,.",
    "crumbs": [
      "参考文献"
    ]
  }
]