[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R torchによる深層学習と科学計算",
    "section": "",
    "text": "Welcome!\n\nThis is the on-line edition of Deep Learning and Scientific Computing with R torch, written by Sigrid Keydana. Visit the GitHub repository for this site, or buy a physical copy from the publisher, CRC Press. You’ll also find the book at the usual outlets, e.g., Amazon.\nThis on-line work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n\nPreface\nThis is a book about torch, the R interface to PyTorch. PyTorch, as of this writing, is one of the major deep-learning and scientific-computing frameworks, widely used across industries and areas of research. With torch, you get to access its rich functionality directly from R, with no need to install, let alone learn, Python. Though still “young” as a project, torch already has a vibrant community of users and developers; the latter not just extending the core framework, but also, building on it in their own packages.\nIn this text, I’m attempting to attain three goals, corresponding to the book’s three major sections.\nThe first is a thorough introduction to core torch: the basic structures without whom nothing would work. Even though, in future work, you’ll likely go with higher-level syntactic constructs when possible, it is important to know what it is they take care of, and to have understood the core concepts. What’s more, from a practical point of view, you just need to be “fluent” in torch to some degree, so you don’t have to resort to “trial-and-error-programming” too often.\nIn the second section, basics explained, we proceed to explore various applications of deep learning, ranging from image recognition over time series and tabular data to audio classification. Here, too, the focus is on conceptual explanation. In addition, each chapter presents an approach you can use as a “template” for your own applications. Whenever adequate, I also try to point out the importance of incorporating domain knowledge, as opposed to the not-uncommon “big data, big models, big compute” approach.\nThe third section is special in that it highlights some of the non-deep-learning things you can do with torch: matrix computations (e.g., various ways of solving linear-regression problems), calculating the Discrete Fourier Transform, and wavelet analysis. Here, more than anywhere else, the conceptual approach is very important to me. Let me explain.\nFor one, I expect that in terms of educational background, my readers will vary quite a bit. With R being increasingly taught, and used, in the natural sciences, as well as other areas close to applied mathematics, there will be those who feel they can’t benefit much from a conceptual (though formula-guided!) explanation of how, say, the Discrete Fourier Transform works. To others, however, much of this may be uncharted territory, never to be entered if all goes its normal way. This may hold, for example, for people with a humanist, not-traditionally-empirically-oriented background, such as literature, cultural studies, or the philologies. Of course, chances are that if you’re among the latter, you may find my explanations, though concept-focused, still highly (or: too) mathematical. In that case, please rest assured that, to the understanding of these things (like many others worthwhile of understanding), it is a long way; but we have a life’s time.\nSecondly, even though deep learning has been “the” paradigm of the last decade, recent developments seem to indicate that interest in mathematical/domain-based foundations is (again – this being a recurring phenomenon) on the rise (Consider, for example, the Geometric Deep Learning approach, systematically explained in Bronstein et al. (2021), and conceptually introduced in Beyond alchemy: A first look at geometric deep learning.) In the future, I assume that we’ll likely see more and more “hybrid” approaches that integrate deep-learning techniques and domain knowledge. The Fourier Transform is not going away.\nLast but not least, on this topic, let me make clear that, of course, all chapters have torch code. In case of the Fourier Transform, for example, you’ll see not just the official way of doing this, using dedicated functionality, but also, various ways of coding the algorithm yourself – in a surprisingly small number of lines, and with highly impressive performance.\nThis, in a nutshell, is what to expect from the book. Before I close, there is one thing I absolutely need to say, all the more since even though I’d have liked to, I did not find occasion to address it much in the book, given the technicality of the content. In our societies, as adoption of machine/deep learning (“AI”) is growing, so are opportunities for misuse, by governments as well as private organizations. Often, harm may not even be intended; but still, outcomes can be catastrophic, especially for people belonging to minorities, or groups already at a disadvantage. Like that, even the inevitable, in most of today’s political systems, drive to make profits results in, at the very least, societies imbued with highly questionable features (think: surveillance, and the “quantification of everything”); and most likely, in discrimination, unfairness, and severe harm. Here, I cannot do more than draw attention to this problem, point you to an introductory blog post that perhaps you’ll find useful: Starting to think about AI Fairness, and just ask you to, please, be actively aware of this problem in public life as well as your own work and applications.\nFinally, let me end with saying thank you. There are far too many people to thank that I could ever be sure I haven’t left anyone out; so instead I’ll keep this short. I’m extremely grateful to my publisher, CRC Press (first and foremost, David Grubbs and Curtis Hill) for the extraordinarily pleasant interactions during all of the writing and editing phases. And very special thanks, for their support related to this book as well as their respective roles in the process, go to Daniel Falbel, the creator and maintainer of torch, who in-depth reviewed this book and helped me with many technical issues; Tracy Teal, my manager, who supported and encouraged me in every possible way; and Posit (formerly, RStudio), my employer, who lets me do things like this for a living.\nSigrid Keydana\n\n\n\n\nBronstein, Michael M., Joan Bruna, Taco Cohen, and Petar Velickovic. 2021. “Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges.” CoRR abs/2104.13478. https://arxiv.org/abs/2104.13478.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "tensors.html",
    "href": "tensors.html",
    "title": "1  テンソル",
    "section": "",
    "text": "1.1 テンソルとは何か\ntorchで何か役に立つことをするには、テンソルについて知る必要がある。 数学や物理の意味のテンソルではない。 TensorFlowや (Py-)Torchのような深層学習フレームワークでは、 テンソル は「単なる」多次元配列で、CPUだけでなく、GPUやTPUのような専用の装置上での高速計算に最適化されたものだ。\n実際、torchのtensorは、Rのarrayに同様に任意の次元を取れる。 Rのarrayとは異なり、高速かつ大規模に計算を実行するために、GPUに移すことができる（おまけに、自動微分ができるので、大変有用だ）。\ntensorはR6オブジェクトに類似していて、$によりフィールドやメソッドを利用できる。\nlibrary(torch)\n\nt1 &lt;- torch_tensor(1)\nt1\nこれは単一の値1だけを格納したテンソルだ。 CPUに「生息」しており、その型はFloat。 次に波括弧の中の1{1}に着目する。 これはテンソルの値を改めて示したものではない。 これはテンソルの形状、つまりそれが生息する空間と次元の長さである。 Base Rと同様にベクトルは単一の要素だけでもよい （base Rは1とc(1)を区別しないことを思い出してほしい）。\n前述の$記法を使って、一つ一つ関連するフィールドを参照することで、個別に以上の属性を確認できる。\nt1$dtype\nt1$device\nt1$shape\nテンソルの $to() を使うと、メソッドいくつかの属性は直接変更できる。\nt2 &lt;- t1$to(dtype = torch_int())\nt2$dtype\n# GPUがある場合\n#t2 &lt;- t1$to(device = \"GPU\")\n# Apple Siliconの場合\nt2 &lt;- t1$to(device = \"mps\")\nt2$device\n形状の変更はどのようにするのか。 これは別途扱うに値する話題だが、手始めにいじってみることにする。 値の変更なしに、この1次元の「ベクトルテンソル」を2次元の「行列テンソル」にできる。\nt3 &lt;- t1$view(c(1, 1))\nt3$shape\n概念的には、Rで1要素のベクトルや行列を作るのに似ている。\nc(1)\nmatrix(1)\nテンソルがどのようなものか分かったところで、いくつかのテンソルを作る方法について考えてみる。",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>テンソル</span>"
    ]
  },
  {
    "objectID": "tensors.html#テンソルの作成",
    "href": "tensors.html#テンソルの作成",
    "title": "1  テンソル",
    "section": "1.2 テンソルの作成",
    "text": "1.2 テンソルの作成\n既に見たテンソルを作る一つの方法はtorch_tensor()を呼び出し、Rの値を渡すというものだった。 この方法は多次元オブジェクトに適用でき、以下にいくつかの例を示す。\nしかし、多くの異なる値を渡す必要があるときは効率が悪くなる。 ありがたいことに、値が全て同一であるべき場合や、明示的なパターンに従うときに適用できる別の方法がある。 この節ではこの技についても説明する。\n\n1.2.1 値からテンソル\n前の例では単一要素のベクトルをtorch_tensor()に渡したが、より長いベクトルを同様に渡すことができる。\n\ntorch_tensor(1:5)\n\n同様に規定のデバイスはCPUだが、最初からGPU/MPSに配置するテンソルを作成することもできる。\n\n#torch_tensor(1:5, device = \"cuda\")\ntorch_tensor(1:5, device = \"mps\")\n\nこれまで作ってきたのはベクトル。 行列、つまり2次元テンソルはどうやって作るのか。\nRの行列を同様に渡せばよい。\n\ntorch_tensor(matrix(1:9, ncol = 9))\n\n結果を見てほしい。 1から9までの数字は、列ごとに表示されている。 これは意図通りかもしれないし、そうではないかもしれない。 意図と異なる場合はmatrix()にbyrow = TRUEを渡せばよい。\n\ntorch_tensor(matrix(1:9, ncol = 3, byrow = TRUE))\n\n高次元のデータはどうするか。 同様の方針に従って、配列を渡すことができる。\n\ntorch_tensor(array(1:24, dim = c(4, 3, 2)))\n\nこの場合でも、結果はRの埋め方に沿ったものとなる。 これが求めるものではないなら、テンソルを構築するプログラムを書いた方が簡単かもしれない。\n慌てる前に、その必要が非常に稀であることを考えてみてほしい。 実際は、Rのデータセットからテンソルを作ることがほとんどだ。 「データセットからテンソル」の最後の小節で詳しく確認する。 その前に、少し時間をとって最後の出力を少し吟味しよう。\n {#fig-tensor-432} 私たテンソルは以下のように印字される。\n\narray(1:24, dim = c(4, 3, 2))\n\n上のテンソルの印字と比較しよう。 Arrayとtensorは異なる方向にオブジェクトを切っている。 テンソルは値を3x2の上向きと奥に向かう広がる長方形に切り、4つの\\(x\\)のそれぞれの値に対して一つ示している。 一方、配列はzの値で分割し、二つの奥向きと右向きに進む大きな4x3の部分を示す。\n言い換えれば、テンソルは左/「外側」から、配列は右/「内側」から思考を始めているとも言えるだろう。\n\n\n1.2.2 指定からテンソル\ntorchの大口生成函数が便利な状況は、おおまかに二つある。 一つは、テンソルの個々の値は気にせず、分布のみに興味がある場合だ。 もう一つは、ある一定のパターンに従う場合だ。\n要素の値の代わりに、大口生成函数を使うときは、取るべき形状を指定する。 例えば、3x3のテンソルを生成し、標準正規分部の値で埋める場合は次のようにする。\n\ntorch_randn(3, 3)\n\n次に示すのは、0と1の間の一様分布に対する同様なもの。\n\ntorch_rand(3, 3)\n\n全て1や0からなるテンソルが必要となることがよくある。\n\ntorch_zeros(2, 5)\n\n\ntorch_ones(2, 2)\n\n他にも多くの大口生成函数がある。 最後に線型代数で一般的ないくつかの行列を作る方法を見ておく。 これは単位行列。\n\ntorch_eye(n = 5)\n\nそしてこれは対角行列。\n\ntorch_diag(c(1, 2, 3))\n\n\n\n1.2.3 データセットからテンソル\nさて、Rのデータセットからテンソルを作る方法を見ていこう。 データセットよっては、この過程は「自動」であったり、考慮や操作が必要になったりする。\nまず、base RについてくるJohnsonJohnsonを試してみる。 これは、Johnson & Johnsonの一株あたりの四半期利益の時系列である。\n\nJohnsonJohnson\n\ntorch_tensor()に渡すだけで、魔法のようにほしいものが手に入るだろうか。\n\ntorch_tensor(JohnsonJohnson)\n\nうまくいっているようだ。 値は希望通り四半期ごとに並んでいる。\n魔法？いや、そうではない。 torchができるのは与えられたものに対して動作することだ。 ここでは、与えられたのは実は四半期順に並んだdoubleのベクトル。 データはtsクラスなので、その通りに印字されただけだ。\n\nunclass(JohnsonJohnson)\n\nこれはうまくいった。 別なものを試そう。\n\ndim(Orange)\n\n\nhead(Orange)\n\n\ntorch_tensor(Orange)\n\nどの型が処理されないのか。 「元凶」は順序付き因子の列Treeに違いないのは明らかだ。 先にtorchが因子を扱えるか確認する。\n\nf &lt;- factor(c(\"a\", \"b\", \"c\"), ordered = TRUE)\ntorch_tensor(f)\n\nこれは問題なく動作した。 他に何がありうるか。 ここでの問題は含まれている構造data.structureである。 as.matrix()を先に作用させる必要がある。 でも、因子が存在するので、全て文字列の配列になってしまい、希望通りにならない。 したがって、基礎となるレベル（整数）を抽出してから、data.frameから行列に変換する。\n\n orange_ &lt;- Orange |&gt;\n  transform(Tree = as.numeric(Tree)) |&gt;\n  as.matrix()\n\ntorch_tensor(orange_) |&gt; print(n = 7)\n\n同じことを別のdata.frame、modeldataのokcでしてみよう。\n\n\n\n\n\n\nCaution\n\n\n\nokcはmodeldataの0.1.1で廃止となり、0.1.2以降は削除された。\n\n\n\nload(\"data/okc.RData\")\n\nhead(okc)\n\n\ndim(okc)\n\n二つある整数の列は問題なく、一つある因子の列の扱い方は学んだ。 characterとdateの列はどうだろう。 個別にdateの列からテンソルを作ってみる。\n\nprint(torch_tensor(okc$date), n = 7)\n\nこれはエラーを投げなかったが、何を意味するのか。 こられはRのDateに格納されている実際の値、つまり1970年1月1日からの日数である。 すなわち、技術的には動作する変換だ。 結果が実際に意味をなすかは、どのようにそれを使うつもりかという問題だ。 言い換えれば、おそらく計算に使う前に、これらのデータを追加の処理する必要がある。 どのようにするかは文脈次第。\n次にlocationを見る。 これは、character型の列のうちの一つだ。 そのままtorchに渡すとどうなるか。\n\ntorch_tensor(okc$location)\n\n実際torchには文字列を格納するテンソルはない。 これらをnumeric型に本管する何らかの方法を適用する必要がある。 この例のような場合、個々の観測が単一の実体（例えば文やパラグラフではなく）を含む場合、最も簡単な方法はRでfactorに変換し、numeric、そしてtensorにすることだ。\n\nokc$location |&gt;\n  factor() |&gt;\n  as.numeric() |&gt;\n  torch_tensor() |&gt;\n  print(n = 7)\n\n確かに、技術的にはこれはうまく動作する。 しかしながら、情報が失われる。 例えば、最初と3番目の場所はそれぞれ”south san francisco”と”san francisco”だ。 一度因子に変換されると、これらは意味の上で”san francisco”や他の場所と同じ距離になる。 繰り返しになるが、これが重要かはデータの詳細と目的次第だ。 これが重要なら、例えば、観測をある基準でまとめたり、緯度/経度に変換したりすることを含めさまざまな対応がありうる。 これらの考慮は全くtorchに特有ではないが、ここで述べたのはtorchの「データ統合フロー」に影響するからだ\n最後に実際のデータ科学の世界に挑むには、NAを無視するわけにはいかない。 確認しよう。\n\ntorch_tensor(c(1, NA, 3))\n\nRのNAはNaNに変換された。 これを扱えるだろうか。 いくつかのtorchのかんすうでは可能だ。 例えば、torch_nanquantil()は単にNaNを無視する。\n\ntorch_nanquantile(torch_tensor(c(1, NA, 3)), q = 0.5)\n\nただし、ニューラルネットワークを訓練するなら、欠損値を意味のあるように置き換える方法を考える必要があるが、この話題は後回しにする。",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>テンソル</span>"
    ]
  },
  {
    "objectID": "tensors.html#テンソルに対する操作",
    "href": "tensors.html#テンソルに対する操作",
    "title": "1  テンソル",
    "section": "1.3 テンソルに対する操作",
    "text": "1.3 テンソルに対する操作\nテンソルに対する数学的操作は全て可能だ。和、差、積など。 これらの操作は（torch_で始まる）函数や（$記法で呼ぶ）オブジェクトに対するメソッドとして利用可能だ。 次の二つは同じだ。\n\nt1 &lt;- torch_tensor(c(1, 2))\nt2 &lt;- torch_tensor(c(3, 4))\n\ntorch_add(t1, t2)\n\n\nt1$add(t2)\n\nどちらも新しいオブジェクトが生成され、t1もt2も変更されない。 オブジェクトをその場で変更する別のメソッドもある。\n\nt1$add_(t2)\n\n\nt1\n\n実は、同じパターンは他の演算にも適用される。 アンダスコアが後についているのを見たら、オブジェクトはその場で変号とされる。\n当然、科学計算の場面では行列演算は特に重要だ。 二つの一次元構造、つまりベクトルの内積から始める。\n\nt1 &lt;- torch_tensor(1:3)\nt2 &lt;- torch_tensor(4:6)\nt1$dot(t2)\n\nこれは動かないはずだと考えただろうか。 テンソルの一つを転置（torch_t()）する必要があっただろうか。 これも動作する。\n\nt1$t()$dot(t2)\n\n最初の呼び出しも動いたのは、torchが行ベクトルと列ベクトルを区別しないからだ。 結果として、torch_matmul()を使ってベクトルを行列にかけるときも、ベクトルの向きを心配する必要はない。\n\nt3 &lt;- torch_tensor(matrix(1:12, ncol = 3, byrow = TRUE))\nt3$matmul(t1)\n\n同じ函数torch_matmul()は二つの行列をかけるときにも使う。 これがtorch_multiply()が行う、引数のスカラとどのように異なるかよく見てほしい。\n\ntorch_multiply(t1, t2)\n\nテンソル演算は他にも多数あり、勉強の途中、いくつかに出会うことになるか、特に述べておく必要な集まりが一つある。\n\n1.3.1 集計\nR行列に対して和を計算する場合、それは次の三つのうちの一つを意味する。 総和、行の和、もしくは列の和。 これら三つを見てみよう（訳あってapply()を使う）。\n\nm &lt;- outer(1:3, 1:6)\n\nsum(m)\napply(m, 1, sum)\napply(m, 2, sum)\n\nそれではtorchで同じことをする。 総和から始める。\n\nt &lt;- torch_outer(torch_tensor(1:3), torch_tensor(1:6))\nt$sum()\n\n行と列の和は面白くなる。 dim引数はtorchにどの次元の和をとるか伝える。 dim = 1を渡すと次のようになる。\n\nt$sum(dim = 1)\n\n予想外にも列の和になった。 結論を導く前に、dim = 2だとどうなるか。\n\nt$sum(dim = 2)\n\n今度は行の和である。 torchの次元の順序を誤解したのだろうか。そうではない。 torchでは、二つの次元があれば行が第一で列が第二である （すぐに示すように、添え字はRで一般的なのと同じで1から始まる）。\nむしろ、概念の違いは集計にある。 Rにおける集計は、頭の中にあるものをよく特徴づけている。 行（次元1）ごとに集計して行のまとめを得て、列（次元2）ごとに集計した列のまとめを得る。 torchでは考え方が異なる。 列（次元2）を圧縮して行のまとめを計算し、行（次元1）で列のまとめを得る。\n同じ考え方がより高い次元に対しても適用される。 例えば、4人の時系列データを記録しているとする。 二つの特徴量を3回計測する。 再帰型ニューラルネットワーク（詳しくは後ほど）を訓練する場合、測定を次のように並べる。\n\n次元1: 個人に亙る。\n次元2: 時刻に亙る。\n次元3: 特徴に亙る。\n\nテンソルは次のようになる。\n\nt &lt;- torch_randn(4, 3, 2)\nt\n\n二つの特徴量についての平均は、対象と時刻に独立で、次元1と2を圧縮する。\n\nt$mean(dim = c(1, 2))\n\n一方、特徴量について平均を求めるが、各個人に対するものは次のように計算する。\n\nt$mean(dim = 2)\n\nここで圧縮されたは時刻である。",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>テンソル</span>"
    ]
  },
  {
    "objectID": "tensors.html#テンソルの部分参照",
    "href": "tensors.html#テンソルの部分参照",
    "title": "1  テンソル",
    "section": "1.4 テンソルの部分参照",
    "text": "1.4 テンソルの部分参照\nテンソルを使っていると、計算のある部分が入力テンソルの一部にのみに対する演算であることはよくある。 その部分が単一の実体（値、行、列など）なら添字参照、このような実体の範囲なら切り出しと呼ばれる。\n\n1.4.1 「R思考」\n添字参照も切り出しも基本的にはRと同じように働く。 いくつかの拡張された記法を続く節で示すが、総じてふるまいは直感に反しない。\nなぜならRと同じように、torchでも添字は1から始まるし、1要素になった次元は落とされるからだ。\n下の例では、2次元テンソルの最初の行を求め、その結果1次元つまりベクトルを得る。\n\nt &lt;- torch_tensor(matrix(1:9, ncol = 3, byrow = TRUE))\nt[1, ]\n\nただし、drop = FALSEを指定すると次元は保持される。\n\nt[1, , drop = FALSE]\n\n切り出しの時は、1要素となる次元はないので、他に考慮すべきことはない。\n\nt &lt;- torch_rand(3, 3, 3)\nt[1:2, 2:3, c(1, 3)]\n\nまとめると、添字参照と切り出しはほぼRと同じように働く。 次に、前に述べた、さらに使いやすくする拡張について見る。\n\n\n1.4.2 Rを越える\n拡張の一つはテンソルの最後の要素の参照だ。 利便性のため、torchでは-1を使ってそれができる。\n\nt &lt;- torch_tensor(matrix(1:4, ncol = 2, byrow = TRUE))\nt[-1, -1]\n\n注意すべきは、Rでは負の添字はかなり異なった効果を持ち、対応する位置の要素は取り除かれることだ。\nもう一つの便利な機能は、切り出しの記法で刻み幅を二つ目のコロンの後に指定できることだ。 ここでは、一つ目から八つ目の列を一つおきに取り出している。\n\nt &lt;- torch_tensor(matrix(1:20, ncol = 10, byrow = TRUE))\nt[ , 1:8:2]\n\n最後に示すのは、同じコードを異なる次元のテンソルに対して動作させる方法だ。 この場合、..を使って明示的に参照されていない、存在する次元全てをまとめて指定できる。\n例えば、行列、配列、もしくは高次元の構造など、どんなテンソルが渡されても最初の次元の添字参照をしたいとする。 次の\nt[1, ..]\nは全てに対して機能する。\n\nt1 &lt;- torch_randn(2, 2)\nt2 &lt;- torch_randn(2, 2, 2)\nt3 &lt;- torch_randn(2, 2, 2, 2)\nt1[1, ..]\nt2[1, ..]\nt3[1, ..]\n\n最後の次元の添字参照がしたければ、代わりにt[.., 1]と書けばよい。 両方を組み合わせることもできる。\n\nt3[1, .., 2]\n\n次の話題は、添字参照や切り出しと同じくらい重要なテンソルの変形だ。",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>テンソル</span>"
    ]
  },
  {
    "objectID": "tensors.html#テンソルの変形",
    "href": "tensors.html#テンソルの変形",
    "title": "1  テンソル",
    "section": "1.5 テンソルの変形",
    "text": "1.5 テンソルの変形\n24要素のテンソルがあるとする。 形状はどうなっているか。 次の可能性がある。\n\n長さ24のベクトル\n24 x 1、12 x 2、6 x 4などの行列\n24 x 1 x 1, 12 x 2 x 1などの3次元配列\nその他（24 x 1 x 1 x 1 x 1という可能性もありうる）\n\n値をお手玉しなくても、view()メソッドでテンソルの形状を変更できる。 最初のテンソルは長さ24のベクトルとする。\n\nt &lt;- torch_zeros(24)\nprint(t, n = 3)\n\n同じベクトルを横長の行列に変形する。\n\nt2 &lt;- t$view(c(2, 12))\n\n新しいテンソルt2を得たが、興味深いことに（そして性能の上で重要なことに）、torchはその値に対して新たに記憶域を割り付ける必要がなかったことだ。 自分で確認することができる。 二つのテンソルはデータほ同じ場所に格納している。\n\nt$storage()$data_ptr()\nt2$storage()$data_ptr()\n\nどのように実現されているか少し議論する。\n\n1.5.1 複製なし変形と複製あり変形\ntorchにテンソルの変形をさせると、テンソルの中身に対して新たな記憶域を割り付けずに要求を達成しようとする。 これが実現可能なのは、同じデータ、究極的には同じバイト列は異なる方法で読み出すことができるからだ。 必要なのはメタデータの記憶域だけだ。\ntorchはどのようにしているか。 具体的な例を見てみる。 3 x 5行列から始める。\n\nt &lt;- torch_tensor(matrix(1:15, nrow = 3, byrow = TRUE))\nt\n\nテンソルにはstride()メソッドがあり、各次元に対して次の要素にたどり着くまでにいくつの要素を越えたか追跡する。 上記のテンソルtに対して、次の行に進むには5要素飛ばす必要があるが、次の列には一つだけ飛ばせばよい。\n\nt$stride()\n\nここで、テンソルを変形して、今度は5行3列にする。 データ自体は変化しないことを思い出してほしい。\n\nt2 &lt;- t$view(c(5, 3))\nt2\n\n今回は次の行には、5要素ではなく3要素だけ飛ばせば次の行に到達する。 次の列に進むのは、ここでも1要素だけ「跳べ」ばよい。\n\nt2$stride()\n\nここで、要素の順序を変えることができるか考えてみよう。 例えば、行列の転置はメタデータの方法で可能だろうか。\n\nt3 &lt;- t$t()\nt3\n\n元のテンソルとその転置はメモリ上の同じ場所を指しているので、実際に可能であるはずだ。\n\nt$storage()$data_ptr()\nt3$storage()$data_ptr()\n\nこれは道理にかなっている。 次の行に到達するのに、1要素だけ跳び、次の列には5要素跳べばよいから、うまくいくだろう。 確認する。\n\nt3$stride()\n\nその通りだ。\n可能な限り、torchは形状を変更する演算をこの方法で扱おうとする。\nこのような（今後多数見ることになる）複製なし演算の一つはsqueeze()とその対義語unsqueeeze()だ。 後者は指定位置に単一要素の次元を付け加え、前者は取り除く。 例を挙げる。\n\nt &lt;- torch_randn(3)\nt\n\nt$unsqueeze(1)\n\nここでは単一要素の次元を前につけた。 代わりに、t$unsqueeze(2)を使えば末尾につけることもできた。\nさて、複製なしの技法は失敗することがあるか。 そのような例を示す。\n\nt &lt;- torch_randn(3, 3)\nt$t()$view(9)\n\nストライドを変える演算を連続して行うと、二つ目は失敗する可能性が高い。 失敗するかどうか決める方法はあるが、簡単な方法はview()の代わりにreshape()を使うことだ。 後者は魔法のように機能し、可能であればメタデータで、そうでなければ複製する。\n\nt &lt;- torch_randn(3, 3)\nt2 &lt;- t$t()$reshape(9)\n\nt$storage()$data_ptr()\nt2$storage()$data_ptr()\n\n想像通り、二つのテンソルは今度は異なる場所に格納されている。\nこの長い章の終わりに取り上げる内容は、一見手に余るように見える機能だが、性能の上で極めて重要なものである。 多くのもののように、慣れるには時間が掛かるが、安心してほしい。 この本やtorchを使った多くのプロジェクトでたびたび目にすることになる。 この機能 伝播 と呼ばれている。",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>テンソル</span>"
    ]
  },
  {
    "objectID": "tensors.html#伝播",
    "href": "tensors.html#伝播",
    "title": "1  テンソル",
    "section": "1.6 伝播",
    "text": "1.6 伝播\n形状が厳密に一致しないテンソルに対する演算をすることが多い。\nもちろん、長さ2のベクトルに長さ5のベクトルを足すようなことはしないかもしれない。 でもやってみたいこともありうる。 例えば、全ての要素にスカラを掛けることがあるが、これはできる。\n\nt1 &lt;- torch_randn(3, 5)\nt1 * 0.5\n\nこれはおそらく大したことではなかっただろう。 Rで慣れている。 しかし、次はRでは動作しない。 同じベクトルを行列の全ての行に加えようとしている。\n\nm &lt;- matrix(1:15, ncol = 5, byrow = TRUE)\nm2 &lt;- matrix(1:5, ncol = 5, byrown = TRUE)\n\nm2をベクトルに代えてもうまくいかない。\n\nm3 &lt;- 1:5\n\nm + m3\n\n文法としては動いたが、意味の上では意図した通りではない。\nここで、上の二つをtorchで試してみる。 まず二つのテンソルが2次元の場合（概念的には一つは行ベクトルだが）から。\n\nt &lt;- torch_tensor(m)\nt2 &lt;- torch_tensor(m2)\n\nt$shape\nt2$shape\n\nt$add(t2)\n\n次に足すものが1次元テンソルの場合。\n\nt3 &lt;- torch_tensor(m3)\n\nt3$shape\n\nt$add(t3)\n\ntorchではどちらも意図通りにうまくいった。 理由を考えてみよう。 上の例でテンソルの形状をあえて印字した。 3 x 5のテンソルには、形状3のテンソルも形状1 x 5のテンソルも足すことができた。 これらは、伝播がどのようにされるか示している。 簡単にいうと、起きたのは次の通りだ。\n\n1 x 5テンソルが加数として使われると、実際は拡張される。 つまり、同じ3行あるかのように扱われる。 このような拡張は一致しない次元が単一で一番左にある場合にのみ実行される。\n形状3のテンソルも同様だが、先に手順が追加される。 大きさが1の主要な次元が実質上左に追加される。 これにより1と同様になり、そこから手順が続く。\n\n重要なのは、物理的な拡張はされないことだ。\nルールを系統だてることにする。\n\n1.6.1 伝播のルール\nルールは次の通り。 まず、一つ目は目を引くものではないか、全ての基礎になる。\n\nテンソルの形状を右に揃える。\n\n二つのテンソル、一つのサイズは3 x 7 x 1、もう一つは1 x 5、があるとする。 これらを右に揃える。\nt1, 形状:     3 7 1\nt2, 形状:       1 5\n\n右から始めて、揃えた軸に沿う大きさが厳密に一致するか、一つが1でなければならない。 後者の場合、単一要素次元のテンソルは単一でないものに対して 伝播 される。\n\n上の例では、伝播は各テンソルに1回ずつ2回発生する。 結果は実質的に以下のようになる。\nt1, 形状:     3 7 5\nt2, 形状:       7 5\n\nもしテンソルの一つが一つ（もしくは1以上）余分な軸があれば、実質的に拡張される。\nt1, 形状: 3 7 5 t2, 形状: 1 7 5\n\nそして伝播が発生する。\nt1, 形状:     3 7 5\nt2, 形状:     3 7 5\nこの例では、伝播が両方のテンソルに同時に発生していることを見た。 覚えておくべきことは、常に右から見るということだ。 次の例は、どんな伝播をしてもうまくいかない。\ntorch_zeros(4, 3, 2, 1)$add(torch_ones(4, 3, 2)) # error\n\nおそらく、この本の中で、この章は最も長く、最も応用から離れたように見えるものだった。 しかし、テンソルに慣れることは、torchをすらすら書くための前提であると言っておく。 同様のことは次の章で扱う話題、自動微分についても言える。 違いは、torchが大変な仕事を我々の代わりにしてくれるということだ。 我々は何をしているか理解すればよいだけだ。",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>テンソル</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "参考文献",
    "section": "",
    "text": "Bronstein, Michael M., Joan Bruna, Taco Cohen, and Petar Velickovic.\n2021. “Geometric Deep Learning: Grids, Groups, Graphs, Geodesics,\nand Gauges.” CoRR abs/2104.13478. https://arxiv.org/abs/2104.13478.",
    "crumbs": [
      "参考文献"
    ]
  }
]