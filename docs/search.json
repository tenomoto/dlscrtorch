[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R torchによる深層学習と科学計算",
    "section": "",
    "text": "Welcome!\n\nThis is the on-line edition of Deep Learning and Scientific Computing with R torch, written by Sigrid Keydana. Visit the GitHub repository for this site, or buy a physical copy from the publisher, CRC Press. You’ll also find the book at the usual outlets, e.g., Amazon.\nThis on-line work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n\nPreface\nThis is a book about torch, the R interface to PyTorch. PyTorch, as of this writing, is one of the major deep-learning and scientific-computing frameworks, widely used across industries and areas of research. With torch, you get to access its rich functionality directly from R, with no need to install, let alone learn, Python. Though still “young” as a project, torch already has a vibrant community of users and developers; the latter not just extending the core framework, but also, building on it in their own packages.\nIn this text, I’m attempting to attain three goals, corresponding to the book’s three major sections.\nThe first is a thorough introduction to core torch: the basic structures without whom nothing would work. Even though, in future work, you’ll likely go with higher-level syntactic constructs when possible, it is important to know what it is they take care of, and to have understood the core concepts. What’s more, from a practical point of view, you just need to be “fluent” in torch to some degree, so you don’t have to resort to “trial-and-error-programming” too often.\nIn the second section, basics explained, we proceed to explore various applications of deep learning, ranging from image recognition over time series and tabular data to audio classification. Here, too, the focus is on conceptual explanation. In addition, each chapter presents an approach you can use as a “template” for your own applications. Whenever adequate, I also try to point out the importance of incorporating domain knowledge, as opposed to the not-uncommon “big data, big models, big compute” approach.\nThe third section is special in that it highlights some of the non-deep-learning things you can do with torch: matrix computations (e.g., various ways of solving linear-regression problems), calculating the Discrete Fourier Transform, and wavelet analysis. Here, more than anywhere else, the conceptual approach is very important to me. Let me explain.\nFor one, I expect that in terms of educational background, my readers will vary quite a bit. With R being increasingly taught, and used, in the natural sciences, as well as other areas close to applied mathematics, there will be those who feel they can’t benefit much from a conceptual (though formula-guided!) explanation of how, say, the Discrete Fourier Transform works. To others, however, much of this may be uncharted territory, never to be entered if all goes its normal way. This may hold, for example, for people with a humanist, not-traditionally-empirically-oriented background, such as literature, cultural studies, or the philologies. Of course, chances are that if you’re among the latter, you may find my explanations, though concept-focused, still highly (or: too) mathematical. In that case, please rest assured that, to the understanding of these things (like many others worthwhile of understanding), it is a long way; but we have a life’s time.\nSecondly, even though deep learning has been “the” paradigm of the last decade, recent developments seem to indicate that interest in mathematical/domain-based foundations is (again – this being a recurring phenomenon) on the rise (Consider, for example, the Geometric Deep Learning approach, systematically explained in Bronstein et al. (2021), and conceptually introduced in Beyond alchemy: A first look at geometric deep learning.) In the future, I assume that we’ll likely see more and more “hybrid” approaches that integrate deep-learning techniques and domain knowledge. The Fourier Transform is not going away.\nLast but not least, on this topic, let me make clear that, of course, all chapters have torch code. In case of the Fourier Transform, for example, you’ll see not just the official way of doing this, using dedicated functionality, but also, various ways of coding the algorithm yourself – in a surprisingly small number of lines, and with highly impressive performance.\nThis, in a nutshell, is what to expect from the book. Before I close, there is one thing I absolutely need to say, all the more since even though I’d have liked to, I did not find occasion to address it much in the book, given the technicality of the content. In our societies, as adoption of machine/deep learning (“AI”) is growing, so are opportunities for misuse, by governments as well as private organizations. Often, harm may not even be intended; but still, outcomes can be catastrophic, especially for people belonging to minorities, or groups already at a disadvantage. Like that, even the inevitable, in most of today’s political systems, drive to make profits results in, at the very least, societies imbued with highly questionable features (think: surveillance, and the “quantification of everything”); and most likely, in discrimination, unfairness, and severe harm. Here, I cannot do more than draw attention to this problem, point you to an introductory blog post that perhaps you’ll find useful: Starting to think about AI Fairness, and just ask you to, please, be actively aware of this problem in public life as well as your own work and applications.\nFinally, let me end with saying thank you. There are far too many people to thank that I could ever be sure I haven’t left anyone out; so instead I’ll keep this short. I’m extremely grateful to my publisher, CRC Press (first and foremost, David Grubbs and Curtis Hill) for the extraordinarily pleasant interactions during all of the writing and editing phases. And very special thanks, for their support related to this book as well as their respective roles in the process, go to Daniel Falbel, the creator and maintainer of torch, who in-depth reviewed this book and helped me with many technical issues; Tracy Teal, my manager, who supported and encouraged me in every possible way; and Posit (formerly, RStudio), my employer, who lets me do things like this for a living.\nSigrid Keydana\n\n\n\n\nBronstein, Michael M., Joan Bruna, Taco Cohen, and Petar Velickovic. 2021. “Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges.” CoRR abs/2104.13478. https://arxiv.org/abs/2104.13478.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "tensors.html",
    "href": "tensors.html",
    "title": "1  テンソル",
    "section": "",
    "text": "1.1 テンソルとは何か\ntorchのtensorは、Rのarrayに同様に任意の次元を取れる。 Rのarrayとは異なり、高速かつ大規模に計算を実行するために、GPUに移すことができる（おまけに、自動微分ができるので、大変有用だ）。\ntensorはR6オブジェクトに類似していて、$によりフィールドやメソッドを利用できる。\nlibrary(torch)\n\nt1 &lt;- torch_tensor(1)\nt1\nこれは単一の値1だけを格納したテンソルだ。 CPUに「生息」しており、その型はFloat。 次に波括弧の中の1{1}に着目する。 これはテンソルの値を改めて示したものではない。 これはテンソルの形状、つまりそれが生息する空間と次元の長さである。 Base Rと同様にベクトルは単一の要素だけでもよい （base Rは1とc(1)を区別しないことを思い出してほしい）。\n前述の$記法を使って、一つ一つ関連するフィールドを参照することで、個別に以上の属性を確認できる。\nt1$dtype\nt1$device\nt1$shape\nテンソルの $to() を使うと、メソッドいくつかの属性は直接変更できる。\nt2 &lt;- t1$to(dtype = torch_int())\nt2$dtype\n# GPUがある場合\n#t2 &lt;- t1$to(device = \"GPU\")\n# Apple Siliconの場合\nt2 &lt;- t1$to(device = \"mps\")\nt2$device\n形状の変更はどのようにするのか。 これは別途扱うに値する話題だが、手始めにいじってみることにする。 値の変更なしに、この1次元の「ベクトルテンソル」を2次元の「行列テンソル」にできる。\nt3 &lt;- t1$view(c(1, 1))\nt3$shape\n概念的には、Rで1要素のベクトルや行列を作るのに似ている。\nc(1)\nmatrix(1)\nテンソルがどのようなものか分かったところで、いくつかのテンソルを作る方法について考えてみる。",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>テンソル</span>"
    ]
  },
  {
    "objectID": "tensors.html#テンソルの作成",
    "href": "tensors.html#テンソルの作成",
    "title": "1  テンソル",
    "section": "1.2 テンソルの作成",
    "text": "1.2 テンソルの作成\n既に見たテンソルを作る一つの方法はtorch_tensor()を呼び出し、Rの値を渡すというものだった。 この方法は多次元オブジェクトに適用でき、以下にいくつかの例を示す。\nしかし、多くの異なる値を渡す必要があるときは効率が悪くなる。 ありがたいことに、値が全て同一であるべき場合や、明示的なパターンに従うときに適用できる別の方法がある。 この節ではこの技についても説明する。\n\n値からテンソル\n前の例では単一要素のベクトルをtorch_tensor()に渡したが、より長いベクトルを同様に渡すことができる。\n\ntorch_tensor(1:5)\n\n同様に規定のデバイスはCPUだが、最初からGPU/MPSに配置するテンソルを作成することもできる。\n\n#torch_tensor(1:5, device = \"cuda\")\ntorch_tensor(1:5, device = \"mps\")\n\nこれまで作ってきたのはベクトル。 行列、つまり2次元テンソルはどうやって作るのか。\nRの行列を同様に渡せばよい。\n\ntorch_tensor(matrix(1:9, ncol = 9))\n\n結果を見てほしい。 1から9までの数字は、列ごとに表示されている。 これは意図通りかもしれないし、そうではないかもしれない。 意図と異なる場合はmatrix()にbyrow = TRUEを渡せばよい。\n\ntorch_tensor(matrix(1:9, ncol = 3, byrow = TRUE))\n\n高次元のデータはどうするか。 同様の方針に従って、配列を渡すことができる。\n\ntorch_tensor(array(1:24, dim = c(4, 3, 2)))\n\nこの場合でも、結果はRの埋め方に沿ったものとなる。 これが求めるものではないなら、テンソルを構築するプログラムを書いた方が簡単かもしれない。\n慌てる前に、その必要が非常に稀であることを考えてみてほしい。 実際は、Rのデータセットからテンソルを作ることがほとんどだ。 「データセットからテンソル」の最後の小節で詳しく確認する。 その前に、少し時間をとって最後の出力を少し吟味しよう。\n {#fig-tensor-432} 私たテンソルは以下のように印字される。\n\narray(1:24, dim = c(4, 3, 2))\n\n上のテンソルの印字と比較しよう。 Arrayとtensorは異なる方向にオブジェクトを切っている。 テンソルは値を3x2の上向きと奥に向かう広がる長方形に切り、4つの\\(x\\)のそれぞれの値に対して一つ示している。 一方、配列はzの値で分割し、二つの奥向きと右向きに進む大きな4x3の部分を示す。\n言い換えれば、テンソルは左/「外側」から、配列は右/「内側」から思考を始めているとも言えるだろう。\n\n\n指定からテンソル\ntorchの大口生成函数が便利な状況は、おおまかに二つある。 一つは、テンソルの個々の値は気にせず、分布のみに興味がある場合だ。 もう一つは、ある一定のパターンに従う場合だ。\n要素の値の代わりに、大口生成函数を使うときは、取るべき形状を指定する。 例えば、3x3のテンソルを生成し、標準正規分部の値で埋める場合は次のようにする。\n\ntorch_randn(3, 3)\n\n次に示すのは、0と1の間の一様分布に対する同様なもの。\n\ntorch_rand(3, 3)\n\n全て1や0からなるテンソルが必要となることがよくある。\n\ntorch_zeros(2, 5)\n\n\ntorch_ones(2, 2)\n\n他にも多くの大口生成函数がある。 最後に線型代数で一般的ないくつかの行列を作る方法を見ておく。 これは単位行列。\n\ntorch_eye(n = 5)\n\nそしてこれは対角行列。\n\ntorch_diag(c(1, 2, 3))\n\n\n\nデータセットからテンソル\nさて、Rのデータセットからテンソルを作る方法を見ていこう。 データセットよっては、この過程は「自動」であったり、考慮や操作が必要になったりする。\nまず、base RについてくるJohnsonJohnsonを試してみる。 これは、Johnson & Johnsonの一株あたりの四半期利益の時系列である。\n\nJohnsonJohnson\n\ntorch_tensor()に渡すだけで、魔法のようにほしいものが手に入るだろうか。\n\ntorch_tensor(JohnsonJohnson)\n\nうまくいっているようだ。 値は希望通り四半期ごとに並んでいる。\n魔法？いや、そうではない。 torchができるのは与えられたものに対して動作することだ。 ここでは、与えられたのは実は四半期順に並んだdoubleのベクトル。 データはtsクラスなので、その通りに印字されただけだ。\n\nunclass(JohnsonJohnson)\n\nこれはうまくいった。 別なものを試そう。\n\ndim(Orange)\n\n\nhead(Orange)\n\n\ntorch_tensor(Orange)\n\nどの型が処理されないのか。 「元凶」は順序付き因子の列Treeに違いないのは明らかだ。 先にtorchが因子を扱えるか確認する。\n\nf &lt;- factor(c(\"a\", \"b\", \"c\"), ordered = TRUE)\ntorch_tensor(f)\n\nこれは問題なく動作した。 他に何がありうるか。 ここでの問題は含まれている構造data.structureである。 as.matrix()を先に作用させる必要がある。 でも、因子が存在するので、全て文字列の配列になってしまい、希望通りにならない。 したがって、基礎となるレベル（整数）を抽出してから、data.frameから行列に変換する。\n\n orange_ &lt;- Orange |&gt;\n  transform(Tree = as.numeric(Tree)) |&gt;\n  as.matrix()\n\ntorch_tensor(orange_) |&gt; print(n = 7)\n\n同じことを別のdata.frame、modeldataのokcでしてみよう。\n\n\n\n\n\n\nCaution\n\n\n\nokcはmodeldataの0.1.1で廃止となり、0.1.2以降は削除された。\n\n\n\nload(\"data/okc.RData\")\n\nhead(okc)\n\n\ndim(okc)\n\n二つある整数の列は問題なく、一つある因子の列の扱い方は学んだ。 characterとdateの列はどうだろう。 個別にdateの列からテンソルを作ってみる。\n\nprint(torch_tensor(okc$date), n = 7)\n\nこれはエラーを投げなかったが、何を意味するのか。 こられはRのDateに格納されている実際の値、つまり1970年1月1日からの日数である。 すなわち、技術的には動作する変換だ。 結果が実際に意味をなすかは、どのようにそれを使うつもりかという問題だ。 言い換えれば、おそらく計算に使う前に、これらのデータを追加の処理する必要がある。 どのようにするかは文脈次第。\n次にlocationを見る。 これは、character型の列のうちの一つだ。 そのままtorchに渡すとどうなるか。\n\ntorch_tensor(okc$location)\n\n実際torchには文字列を格納するテンソルはない。 これらをnumeric型に本管する何らかの方法を適用する必要がある。 この例のような場合、個々の観測が単一の実体（例えば文やパラグラフではなく）を含む場合、最も簡単な方法はRでfactorに変換し、numeric、そしてtensorにすることだ。\n\nokc$location |&gt;\n  factor() |&gt;\n  as.numeric() |&gt;\n  torch_tensor() |&gt;\n  print(n = 7)\n\n確かに、技術的にはこれはうまく動作する。 しかしながら、情報が失われる。 例えば、最初と3番目の場所はそれぞれ”south san francisco”と”san francisco”だ。 一度因子に変換されると、これらは意味の上で”san francisco”や他の場所と同じ距離になる。 繰り返しになるが、これが重要かはデータの詳細と目的次第だ。 これが重要なら、例えば、観測をある基準でまとめたり、緯度/経度に変換したりすることを含めさまざまな対応がありうる。 これらの考慮は全くtorchに特有ではないが、ここで述べたのはtorchの「データ統合フロー」に影響するからだ\n最後に実際のデータ科学の世界に挑むには、NAを無視するわけにはいかない。 確認しよう。\n\ntorch_tensor(c(1, NA, 3))\n\nRのNAはNaNに変換された。 これを扱えるだろうか。 いくつかのtorchのかんすうでは可能だ。 例えば、torch_nanquantil()は単にNaNを無視する。\n\ntorch_nanquantile(torch_tensor(c(1, NA, 3)), q = 0.5)\n\nただし、ニューラルネットワークを訓練するなら、欠損値を意味のあるように置き換える方法を考える必要があるが、この話題は後回しにする。",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>テンソル</span>"
    ]
  },
  {
    "objectID": "tensors.html#テンソルに対する操作",
    "href": "tensors.html#テンソルに対する操作",
    "title": "1  テンソル",
    "section": "1.3 テンソルに対する操作",
    "text": "1.3 テンソルに対する操作\nテンソルに対する数学的操作は全て可能だ。和、差、積など。 これらの操作は（torch_で始まる）函数や（$記法で呼ぶ）オブジェクトに対するメソッドとして利用可能だ。 次の二つは同じだ。\n\nt1 &lt;- torch_tensor(c(1, 2))\nt2 &lt;- torch_tensor(c(3, 4))\n\ntorch_add(t1, t2)\n\n\nt1$add(t2)\n\nどちらも新しいオブジェクトが生成され、t1もt2も変更されない。 オブジェクトをその場で変更する別のメソッドもある。\n\nt1$add_(t2)\n\n\nt1\n\n実は、同じパターンは他の演算にも適用される。 アンダスコアが後についているのを見たら、オブジェクトはその場で変号とされる。\n当然、科学計算の場面では行列演算は特に重要だ。 二つの一次元構造、つまりベクトルの内積から始める。\n\nt1 &lt;- torch_tensor(1:3)\nt2 &lt;- torch_tensor(4:6)\nt1$dot(t2)\n\nこれは動かないはずだと考えただろうか。 テンソルの一つを転置（torch_t()）する必要があっただろうか。 これも動作する。\n\nt1$t()$dot(t2)\n\n最初の呼び出しも動いたのは、torchが行ベクトルと列ベクトルを区別しないからだ。 結果として、torch_matmul()を使ってベクトルを行列にかけるときも、ベクトルの向きを心配する必要はない。\n\nt3 &lt;- torch_tensor(matrix(1:12, ncol = 3, byrow = TRUE))\nt3$matmul(t1)\n\n同じ函数torch_matmul()は二つの行列をかけるときにも使う。 これがtorch_multiply()が行う、引数のスカラとどのように異なるかよく見てほしい。\n\ntorch_multiply(t1, t2)\n\nテンソル演算は他にも多数あり、勉強の途中、いくつかに出会うことになるか、特に述べておく必要な集まりが一つある。\n\n集計\nR行列に対して和を計算する場合、それは次の三つのうちの一つを意味する。 総和、行の和、もしくは列の和。 これら三つを見てみよう（訳あってapply()を使う）。\n\nm &lt;- outer(1:3, 1:6)\n\nsum(m)\napply(m, 1, sum)\napply(m, 2, sum)\n\nそれではtorchで同じことをする。 総和から始める。\n\nt &lt;- torch_outer(torch_tensor(1:3), torch_tensor(1:6))\nt$sum()\n\n行と列の和は面白くなる。 dim引数はtorchにどの次元の和をとるか伝える。 dim = 1を渡すと次のようになる。\n\nt$sum(dim = 1)\n\n予想外にも列の和になった。 結論を導く前に、dim = 2だとどうなるか。\n\nt$sum(dim = 2)\n\n今度は行の和である。 torchの次元の順序を誤解したのだろうか。そうではない。 torchでは、二つの次元があれば行が第一で列が第二である （すぐに示すように、添え字はRで一般的なのと同じで1から始まる）。\nむしろ、概念の違いは集計にある。 Rにおける集計は、頭の中にあるものをよく特徴づけている。 行（次元1）ごとに集計して行のまとめを得て、列（次元2）ごとに集計した列のまとめを得る。 torchでは考え方が異なる。 列（次元2）を圧縮して行のまとめを計算し、行（次元1）で列のまとめを得る。\n同じ考え方がより高い次元に対しても適用される。 例えば、4人の時系列データを記録しているとする。 二つの特徴量を3回計測する。 再帰型ニューラルネットワーク（詳しくは後ほど）を訓練する場合、測定を次のように並べる。\n\n次元1: 個人に亙る。\n次元2: 時刻に亙る。\n次元3: 特徴に亙る。\n\nテンソルは次のようになる。\n\nt &lt;- torch_randn(4, 3, 2)\nt\n\n二つの特徴量についての平均は、対象と時刻に独立で、次元1と2を圧縮する。\n\nt$mean(dim = c(1, 2))\n\n一方、特徴量について平均を求めるが、各個人に対するものは次のように計算する。\n\nt$mean(dim = 2)\n\nここで圧縮されたは時刻である。",
    "crumbs": [
      "torchに慣れる",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>テンソル</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "参考文献",
    "section": "",
    "text": "Bronstein, Michael M., Joan Bruna, Taco Cohen, and Petar Velickovic.\n2021. “Geometric Deep Learning: Grids, Groups, Graphs, Geodesics,\nand Gauges.” CoRR abs/2104.13478. https://arxiv.org/abs/2104.13478.",
    "crumbs": [
      "参考文献"
    ]
  }
]