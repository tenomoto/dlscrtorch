# 損失函数

損失函数の概念は機械学習に重要なものだ。
どの反復においても、現在の損失値は推定値が目的からどれくらい離れているかを示す。
そして、その値は損失が減少する方向にパラメタを更新するのに用いられる。

これまでの応用例でも、損失函数を使ってきた。
それは平均二乗誤差で手作業で次のように計算した。

```r
loss <- (y_pred - y)$pow(2)$sum()
```

想像通り、ここでもこのような手作業は不要だ。

この概念に関する最後の章で、現在の例を見直す前に、二つのことを述べたい。
一つは、`torch`組込の損失函数の作り方である。
もう一つは、どの損失函数を選択するかについてだ。

## `torch`の損失函数

`torch`では、損失函数は`nn_`または`nnf_`で始まる。

`nnf_`を使う場合、直接函数呼び出しを行う。
これに対応して、その（推定と目的の）引数はどちらもテンソルだ。
例えば、`nnf_mse_loss()`という組込函数は手作業でコードを書いたものに類似している。

```{r}
library(torch)
nnf_mse_loss(torch_ones(2, 2), torch_zeros(2, 2) + 0.1)
```

一方`nn_`では、まずオブジェクトを作成する。

```{r}
l <- nn_mse_loss()
```

このオブジェクトはテンソルに対して呼び出すと、求める損失が得られる。

```{r}
l(torch_ones(2, 2), torch_zeros(2, 2) + 0.1)
```

オブジェクトまたは函数のどちらを選択するかは、主に好みと文脈次第だ。
大きなモデルでは、いくつかの損失函数を合わせてつかうことになる。
その際、損失オブジェクトを作る方が部品として扱いやすく、維持しやすいコードになる。
本書では、最初の方法を主に用いるが、特別な理由があれば別の方法を取ることにする。

## どの損失函数を選択すべきか。

深層学習や機械学習全般では、ほとんどの応用は数値の予測または確率の推定のどちらか一つ、または両方を行う。
現在の例の回帰問題は前者を行なっている。
実際の応用では、気温や従業員の離職率を推定したり、売り上げを予測したりする。
後者では、典型的な問題は分類だ。
例えば画像を最も顕著な内容に基づいて分類するには、実際にはそれぞれの確率を計算する。
そして「犬」の確率が0.7で、「猫」の確率が0.3なら犬と判定する。

### 最尤法

分類も回帰も最も使われている損失函数は最尤原理に基づいている。
最尤とは、モデルのパラメタの選択がデータ、つまり観測したものや観測できたかもしれないことが最大限起こりやすいようにする。
この原理は基本的である「だけ」でなく、直感に訴えるものだ。
簡単な例を考えよう。

例えば、7.1, 22.14, 11.3という値があり、生じさせる過程が正規分布に則っているものとする。
その場合、これらのデータは平均14、標準偏差7の分布により生じたものである可能性が、平均20、標準偏差
1よりもはるかに可能性が高い。

### 回帰

回帰（目的の分布が正規分布であるという暗黙の仮定をおいたもの^1）では、尤度を最大化するには、これまで計算してきた損失である、平均二乗誤差を引き続き用いればよい。
最尤推定値は望まれる全ての統計的な性質を持つ。
しかしながら、特定の用途では他の損失を使う理由があるかもしれない。
^1: 仮定があり得ない場合、分布適合損失函数が提供されている（例: Poisson負対数は`nnf_poisson_nll_loss()）。

例えば、データセットに外れ値があり、何らかの理由で予測と目的がかなりずれていることがある。
平均二乗誤差は外れ値に大きな重みを置く。
そのような場合、代替となりうるのは平均絶対誤差（`nnf_l1_loss()`）と滑らかなL1損失（`nn_smooth_l1_loss()）である。
後者は混合した型で、既定では絶対（L1）誤差を計算するが二乗（L2）に絶対誤差が非常に小さいところで切り替える。