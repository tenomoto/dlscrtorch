# 行列計算: 畳み込み

機械学習では、畳み込みや畳み込み層、畳み込みニューラルネットワークが話題となる。しかしながら、画像処理の章で説明したように、そのように呼んで行っているものは実際は少し異なり、相互相関である。

形式的には、差は重要ではなく、符号が逆になるだけである。
意味の上では、これらは全く異なる。
既に見たように、相互相関は類似性を見つけ、特徴抽出器として働く。
畳み込みは、抽象的に特徴づけるのは難しい。
信号処理において果たしている傑出した役割やその数学的な意義については、本が何冊も書けるほどである。
ここでは、深く基礎を掘り下げることはしない。
その代わりに、その作用について何らかの洞察を得ることにする。
まず、関わる手順について考察し示すこと、そしてコードとして実装することによりこれを行う。
前の章と同様に、読者が興味を持てるよう、焦点は理解と更なる探究のための基礎作りに合わせる。

## 畳み込みの意義

信号処理では、 *フィルタ* を用いることにより、信号を変えて目的の形、例えば高振動数が除去されたものにする。
手元に、時系列をフーリエ変換した表現、つまり振動数毎に大きさと位相があるとする。
ある閾値より高い振動数をを除去したいとしよう。
最も簡単な方法は、振動数のデータに1と0の列を掛けることだ。
このようにすると、フィルタの作用は振動数領域で生じ、これが断然最も便利な方法だ。

もし、同様の結果を時間領域で得たい、つまり生の時系列データを扱うとしたらどうするか。
その場合、フィルタの時間領域での表現（逆フーリエ変換から得られる）を見つけなくてはならない。
そして、この表現を時系列に *畳み込む* 。
言い換えれば、時間領域における畳み込みは振動数領域における掛け算に対応する。
この基本的な事実は常に利用される。

それでは、畳み込みは何にをするものかや、どのように実装されているかについて、より詳しく理解しよう。
単一の次元から始めて、二次元の場合にどのようになるか、少し探究する。

## 一次元の畳み込み

簡単な信号 `x` と簡単なフィルタ `h` を作るところから取り掛かる。
変数の名前は思いつきで決めたのではない、信号処理で $h$ は、この後すぐに現れる用語 *インパルス* 応答を表すのに通常使われる記号だ。

```{r}
library(torch)

x <- torch_arange(start = 1, end = 4)
h <- torch_tensor(c(-1, 0, 1))
```

さて、 `torch_conv1d()` があるので、これを呼んで何が起こるか見てみよう。
畳み込みの定義によると、出力の長さは入力の長さとフィルタの長さとの和より一つ少ない。
`torch_conv1d()` を使って、長さ6の出力を得るには、フィルタの長さが3なので、両側に二つずつ広げる必要がある。

次のコードで、 `view()` の呼び出しに気を取られてはいけない。
これはただ `torch` が3次元の入力を必要としているためだ。
最初と次の次元は、いつも通りそれぞれバッチとチャンネルを表す。

```{r}
torch_conv1d(
  x$view(c(1, 1, 4)),
  h$view(c(1, 1, 3)),
  padding = 2
)
```

`torchconv1d()` は畳み込みではなく相互相関を計算するのだったのではないか、と考えているかもしれない。
Rには `convolve()` があるので確認しよう[^1]

[^1]: 引数 `type = "open"` を渡すことにより、単調で周期的でない畳み込みにすることを指定できる。

```{r}
x_ <- as.numeric(x)
h_ <- as.numeric(h)

convolve(x_, h_, type = "open")
```
結果は同じである。
しかしながら、 `convolve()` の説明を読むと

> 二つの列 `x` と `y` の畳み込みに対する通常の定義は次で与えられることに注意する。
> `convolve(x, rev(y), type="o")`

明らかに、フィルタの要素の順序を逆転させる必要がある。

```{r}
convolve(x_, rev(h_), type = "open")
```

確かに、今度の結果は異なっている。
同様なことを `torch_conv1d()` でやってみよう。

```{r}
torch_conv1d(
  x$view(c(1, 1, 4)),
  h$flip(1)$view(c(1, 1, 3)),
  padding = 2
)
```

ここでも結果は `torch` とRで同じである。
`convolve()` の説明の中の "Details"部にある、あの簡潔な記述は相互総観と畳み込みの違いを完璧に捉えている。
畳み込みでは、二つ目の引数は逆順になっている。
信号処理の言葉では、 *反転* である。
（「反転」の方がかなり良い用語だ。高次元にも一般化できるからだ。）

技術的にはさはわずかだ。
単に符号が逆転するだけである。
しかし、数学的には、フィルタが何であり、何をするかに関わるという意味で重要である。
これは、この後説明する。

畳み込みの基礎となる演算は二つの方法で示すことができる。

## 畳み込みの二つの考え方

一つは、単一の出力に着目しどのように得られたか定めるものだ。
つまり、どの入力要素がその値に寄与したか、どのように組み合わされたかを問う。
これは「出力からの見方」と呼ぶことができ、相互相関で馴染みのあるものである。

相互相関と同様に、次のようにも述べることもできる。
フィルタが画像を「移動」し、個々の画像中の位置（画素）で、周囲の入力画素と対応する「重ねられた」フィルタ値とを掛け算し、全ての画素を足し合わせる。
つまり、個々の出力画素は入力とフィルタ値の組の間の内積の計算から得られる。

二つ目の考え方は、入力の観点から見たもの（「入力からの見方」と呼ぶ）だ。
問いは、どのように各入力が出力に寄与するかである。
この見方は、最初の見方よりも慣れを要するが、単に馴染み、ニューラルネットワークの文脈でのよくある説明のされ方の問題かもしれない。
どちらにしても、入力からの見方は、畳み込みの数学的な意味について学ぶという点で非常に有益である。

両方の見方を学ぶ。
まず、馴染みのある、出力からの見方から始める。

#### 出力からの見方

出力からの見方では、`torch_conv1d()` で `padding = 2` を指定したときのように、まず入力信号の両側を延長する。
指定通りに、インパルス応答を反転し、`1, 0, -1` とする。
次に、「移動」を示す。

以下は、表形式でこれを示したものだ（@tbl-convolution_output_view）。

|    信号    | 反転 IR |      |      |      |      |      |
|-----------:|--------:|-----:|-----:|-----:|-----:|-----:|
|        `0` |     `1` |      |      |      |      |      |
|        `0` |     `0` |  `1` |      |      |      |      |
|        `1` |    `-1` |  `0` |  `1` |      |      |      |
|        `2` |         | `-1` |  `0` |  `1` |      |      |
|        `3` |         |      | `-1` |  `0` |  `1` |      |
|        `4` |         |      |      | `-1` |  `0` |  `1` |
|        `0` |         |      |      |      | `-1` |  `0` |
|        `0` |         |      |      |      |      | `-1` |
| **結果**   |    `-1` | `-2` | `-2` | `-2` |  `3` |  `4` |

: 畳み込み: 出力からの見方 {#tbl-convolution_output_view}

結局この話題について既に述べたように、書き出してみた結果に驚くことは何もない。
次に入力からの見方を考える。

#### 入力からの見方

入力からの考え方で重要なことは、入力信号をどのように考えるかである。
個々の要素は、拡大縮小され、移動されたインパルスと見なされる。

*インパルス* は単位標本（またはインパルス）函数、デルタ（$\delta$）で与えられる。
この函数は値が1となる0を除いたすべての位置において0である。

$$
\delta[n] = \begin{cases}1,\;n = 0\\0,\;n \ne 0\end{cases}
$$

これは、クロネッカーのデルタ $\delta_{ij}$[^2] に似ており、一つの添字を 0 に固定したものである。

[^2]: クロネッカーのデルタは、 $i=j$ のときに1、そうでないときに0となる。

$$
\delta[n] = \delta_{n0} = \delta_{0n}
$$

つまり、この函数 $\delta[n]$ だけがあれば、 $n$ が、例えば離散時間を表すとすると、厳密に一つの信号値、時刻 $n=0$[^3] における値を表すことができ、その唯一とりうる値は `1` である。
今、この操作に拡大縮小と移動を加える。

[^3]: $n$ を $t$ の代わりに異なる位置として使っている。
信号その他離散化された値は、時間（あるいは空間）の離散点においてのみ「存在」するからである。
文脈によって、少しぎこちなく見えるかもしれないが少なくとも一貫している。

- 拡大縮小により、$n=0$ でのいかなる値も作れる。例えば $x_0 = 0 * \delta[n]$。
- 移動により、他の時刻での値に影響を与えることができる。例えば、時刻 $n = 3$は $n - 3 = 0$ なので$\delta[n-3] = \delta_{n3}$ で与えられる。
- 両方を組み合わせることにより、どんな時刻のどんな値でも表すことができる。例えば $x_5 = 1.11 * \delta[n-5]$。

これまで、信号についてだけ考えてきた。
フィルタはどうなるか。
インパルスを特徴づけたのと同様に、フィルタも * インパルス応答* で完全に記述できる[^4]。
インパルス応答は、定義により、入力がインパルスである（つまり、時刻 $n=0$ で生じる）ときに現れるものである。
信号に対して用いた表記にならい、$h$ をインパルス応答とすると次のように書ける。

$$
h[n] = h[n - 0] \equiv h(\delta[n-0])
$$
[^4]: この章ではどこでも、フィルタについて話題にするときは、線型時間不変系だけを考えている。
時間不変系への制約は、畳み込み操作に内在している。

扱っている例では、数列 `-1, 0, 1` となるだろう。
でも信号は0だけでなく、他の時刻においても表すことが必要であるように、フィルタも他の位置でも適用できなければならない。
その目的のためにも、移動操作が用いられ、似た方法で定式化される。
例えば、$h[n-1]$ はフィルタが時刻1、$n-1$が0となる時刻に適用されることを意味する。
このような移動は、俗に「ずらし」と呼んでいるものに相当する。

さて、残されている仕事は部品を組み合わせることだ。
時刻 $n=0$ において、移動なしのインパルス応答を受け取り、信号の振幅で拡大縮小する。
例においては、その値は1なので、$1 * h[n- 0] = 1 * [-1, 0, 1] = [-1, 0, 1]$となる。
他の時刻においては、インパルスオス等を考えている入力位置に移動し掛け算する。
最後に、すべての入力位置からすべての寄与を得たら、足し合わせて畳み込まれた出力を得る。

次の表は、それを説明するものである[@tbl-convolution_input_view]。

|  信号   |  インパルス入力  |                  積 |
|--------:|-----------------:|--------------------:|
|     `1` |       `h[n - 0]` | `-1  0  1  0  0  0` |
|     `2` |       `h[n - 1]` |  `0 -2  0  2  0  0` |
|     `3` |       `h[n - 2]` |  `0  0 -3  0  3  0` |
|     `4` |       `h[n - 3]` |  `0  0  0 -4  0  4` |
| **和**  |                  | `-1 -2 -2 -2  3  4` |

: 畳み込み: 入力からの見方 {#tbl-convolution_input_view}

個人的には、出力からの見方の方が理解しやすいものの、入力からの見方から得られる洞察が多いように感じる。
特に、避けられない問い、なぜインパルス応答を反転させるのかに答えている。

謎の力のようなものからは程遠く、負号は信号の表現のされ方から生じた機械的な結果にすぎない。
時刻 $n=2$ において測定された信号は $\delta[n-2]$ （2から2を引くと0）と表され、その信号に適用されるフィルタも $h[n-2]$ となる。

### 実装

出力からの見方についての説明から、どのようにコードにするか述べることはあまりないと思うかもしれない。
入力ベクトルに亙ってループし、期待される出力位置において内積を計算するのは容易に見える。
しかし、これは多数のベクトル積を計算することを意味し、さらに入力が長ければ長いほどその計算が増加する。

幸運にも、もっと良い方法がある。
単一次元（線型）畳み込みは、対角成分がいくつかの定数で、他の要素が0であるToeplitz行列により計算される。
フィルタがToeplitz行列で定式化れれば、行われる積はToeplitz行列と入力との間の一つだけである。
行列の列数は入力の長さと同じだけ必要である（さもなくば積が計算できない）といっても、行列は「ほぼ空」なので計算量は少ない。

扱っている例に対して、Toeplitz行列を作ったものを示す。

```{r}
h <- torch_tensor(
  rbind(c(-1, 0, 0, 0),
        c(0, -1, 0, 0),
        c(1, 0, -1, 0),
        c(0, 1, 0, -1),
        c(0, 0, 1, 0),
        c(0, 0, 0, 1)
  ))
h
```

例の入力との積が期待通りの結果になるか確認しよう。

```{r}
h$matmul(x)
```

期待通りになった。
次に二次元の場合を考えよう。
概念的には、差はないが、実際の計算（「自作」と行列の両方）はもっと複雑になる。
つまり、手計算の（一般化可能な）一部を示すことで満足し、計算の部分ではいちいち詳細を示さないことにする。

## 二次元での畳み込み

一次元と二次元の畳み込みがどのように類似しているか外見的に示すために、出力からの見方を考える。

### 仕組み（出力からの見方）

今回の例では、入力は二次元で、次のようなものである。

$$
\begin{bmatrix}
1 & 4 & 1\\
2 & 5 & 3
\end{bmatrix}
$$

フィルタについても同様である。
ありうる例を示す。

$$
\begin{bmatrix}
1 & 1\\
1 & -1
\end{bmatrix}
$$

出力からの見方では、フィルタを入力に対して「ずらす」。
しかし、見やすくするために、一つの出力値（「画素」）に着目する。
入力が `m1 x n1` で出力が `m2 x n2` なら出力の大きさは `(m + m2 -1) x (n1 + n2 -1)` である。
つまり、例では `3 x 4` となる。
`(0, 1)` での値を選ぶ。
画像処理では、通常行を下から数える。

$$
\begin{bmatrix}
\cdot & \cdot & \cdot & \cdot\\
\cdot & \cdot & \cdot & \cdot\\
\cdot & y_{01} & \cdot & \cdot
\end{bmatrix}
$$

存在しない（負）位置を含めた表で入力を示す。

|   位置 (`x`/`y`)   | `-1`  | `0`   | `1`   | `2`   |
|----------------|-----|-----|-----|-----|
| **`1`**          |     | `1`   | `4`   | `1`   |
| **`0`**          |     | `2`   | `5`   | `3`   |
| **`-1`**         |     |     |     |     |

対応する位置に値を置いたフィルタは次のように表される。

|   位置 (`x`/`y`)   | `-1`  | `0`   | `1`   | `2`   |
|----------------|-----|-----|-----|-----|
| **`1`**          |     | `1`   | `1`   |     |
| **`0`**          |     | `1`   | `-1`  |     |
| **`-1`**         |     |     |     |     |

一次元の場合のように、まずはフィルタを反転させる。
反転は180°の回転である。

|   位置 (`x`/`y`)   | `-1`  | `0`   | `1`   | `2`   |
|----------------|-----|-----|-----|-----|
| **`1`**          |     |     |     |     |
| **`0`**          | `-1`  | `1`   |     |     |
| **`-1`**         | `1`   | `1`   |     |     |

次にフィルタを必要ない出力位置に移動する。
ここでは右に一つ移動して、上下には移動しない。

|   位置 (`x`/`y`)   | `-1`  | `0`   | `1`   | `2`   |
|----------------|-----|-----|-----|-----|
| **`1`**          |     |     |     |     |
| **`0`**          |     | `-1`  | `1`   |     |
| **`-1`**         |     | `1`   | `1`   |     |

`(0, 1)`での出力を計算する準備が完了した。
計算は重なっている画像とフィルタ値の内積である。

|  位置 (`x`/`y`)    | `-1`  |    `0`     |   `1`    |  `2`  |
|:--------------:|:---:|:--------:|:------:|:---:|
|     **`1`**      |     |          |        |     |
|     **`0`**      |     | `-1*2=-2` | `1*5=5` |     |
|     **`-1`**     |     |          |        |     |

最終結果は `-2 + 5 = 3` である。

$$
\begin{bmatrix}
\cdot & \cdot & \cdot & \cdot\\
\cdot & \cdot & \cdot & \cdot\\
\cdot & 3   & \cdot & \cdot
\end{bmatrix}
$$

まだ値のない残りについても同様に計算できる。
その計算は省略して、どのように実際の計算が行われるか見ることにする。

### 実装

二次元の畳み込みの実装にもToeplitz行列を使う。
すでに述べたように、なぜすべての手順がそのなるか *厳密な形* では示さない。
ここでの意図は扱っている例を示すことであり、その例を踏まえて、必要ならいろいろと探究してほしい。

#### 手順1: フィルタ行列の準備

フィルタを出力の大きさに合わせる。

    0  0 0 0
    1  1 0 0
    1 -1 0 0

次にフィルタの全ての行について、下からToeplitz行列を作成する。

    # H0
     1  0  0  
    -1  1  0  
     0 -1  1  
     0  0 -1  
     
    # H1
     1  0  0  
     1  1  0  
     0  1  1  
     0  0  1  
     
    # H2
     0  0  0  
     0  0  0  
     0  0  0  

コードは次のように書ける。

```{r}
H0 <- torch_tensor(
  cbind(
    c(1, -1, 0, 0),
    c(0, 1, -1, 0),
    c(0, 0, 1, -1)
  )
)

H1 <- torch_tensor(
  cbind(
    c(1, 1, 0, 0),
    c(0, 1, 1, 0),
    c(0, 0, 1, 1)
  )
)

H2 <- torch_tensor(0)$unsqueeze(1)
```

次に、これら三つの行列を組み合わせて、次のように *二重ブロックToeplitz* *行列* を作る。

    H0   0
    H1  H0
    H2  H1

これをコードにする一つの方法は `torch_block_diag()` を二回使って二つの非0ブロックを作り、 で結合することだ。

```{r}
H <- torch_cat(
  list(
    torch_block_diag(list(H0, H0)), torch_zeros(4, 6)
  )
) +
  torch_cat(
    list(
      torch_zeros(4, 6),
      torch_block_diag(list(H1, H1))
    )
  )

H
```

    torch_tensor
     1  0  0  0  0  0
    -1  1  0  0  0  0
     0 -1  1  0  0  0
     0  0 -1  0  0  0
     1  0  0  1  0  0
     1  1  0 -1  1  0
     0  1  1  0 -1  1
     0  0  1  0  0 -1
     0  0  0  1  0  0
     0  0  0  1  1  0
     0  0  0  0  1  1
     0  0  0  0  0  1
    [ CPUFloatType{12,6} ]

最終的な行列は二つの非0の「バンド」を持ち、二つの全てが0の対角線で分けられている。
この行列は、行列積のために必要なフィルタの最終的な形である。

#### 手順2: 入力の準備

この `12 x 6` 行列と掛け算できるように、入力をベクトルに平坦化する必要がある。
再び、行を下から順に進める。

```{r}
x0 <- torch_tensor(c(2, 5, 3)) 
x1 <- torch_tensor(c(1, 4, 1))

x <- torch_cat(list(x0, x1))
x
```

#### 手順3: 行列積

ここまでで、畳み込みは単純な行列積に変わった。

```{r}
y <- H$matmul(x)
y
```

残るは、出力を正しい二次元の構造に変形することだけだ。
行を（下から）順に組み立てると、次のようになる。

$$
\begin{bmatrix}
  1 & 5 & 5 & 1\\
  3 & 10 & 5 & 2\\
  2 & 3 & -2 & -3\\
\end{bmatrix}
$$

`(0, 1)` の要素に着目する、手計算の結果と一致していることがわかる。

ここで、 `torch` の行列計算の話題を終える。
しかし、次の話題であるフーリエ変換は、行列計算から余り離れていない。
フーリエ変換は、時間領域の畳み込みが振動数領域の掛け算に対応することを覚えているだろうか。

この対応は、計算の高速化によく利用される。
入力データをフーリエ変換し、結果をフィルタで掛け算し、フィルタされた振動数領域の表現を再び逆変換する。
Rの `colvolve()` の説明を見てみよう。
出だしにはっきりと書いてある。

> フーリエ変換を使って、二つの数列の畳み込みを計算する。

それでは、フーリエ変換に進もう。
